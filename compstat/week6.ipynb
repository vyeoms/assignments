{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Mathematics and Statistics  \n",
    "MAST32001 Computational Statistics, Autumn 2022  \n",
    "Luigi Acerbi  \n",
    "\n",
    "# Week 6 exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Leapfrog integration in HMC (4 points)\n",
    "\n",
    "In this exercise we will experiment with the leapfrog integrator used in HMC. As a target, we will use the 2D correlated normal \n",
    "$$ \\pi(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} ;\\; 0, \\boldsymbol{\\Sigma}_2), $$\n",
    "where $ \\boldsymbol{\\Sigma}_2 = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $\\rho = 0.998$.\n",
    "\n",
    "1. Simulate the system for $L=10$ leapfrog steps starting from the origin $\\boldsymbol{\\theta}_0 = (0, 0)$ with momentum $r = (1, 1/3)$ with several values of $\\epsilon$ and plot the resulting Metropolis-Hastings acceptance probabilities $a$ at the end of the leapfrog trajectory. Considering values of $\\epsilon \\in [0.001, 0.1]$:\n",
    "  - What is the smallest $\\epsilon$ that will yield acceptance probability below 60%? \n",
    "  - What is the largest $\\epsilon$ that will yield an acceptance probability above 10%?\n",
    "  - For this exercise, the required tolerance on the answer is 0.001.\n",
    "2. Simulate the Hamiltonian system for 500 leapfrog steps with $\\epsilon=0.05$ starting from the origin $\\boldsymbol{\\theta}_0 = (0, 0)$ with momentum $r = (1, 1/3)$. Plot the trajectory and the Euclidean distance from the starting point at each point. \n",
    "  - How many leapfrog steps do you need to take until the trajectory reaches the first local maximum distance from the origin?\n",
    "  - How many leapfrog steps (since the start of the trajectory) will lead you to the first local minimum?\n",
    "\n",
    "*Note*: This exercise tests only the deterministic leapfrog integration used within the HMC algorithm to produce a single proposal. **This procedure is completely deterministic.** You will not need any random number generators in your solution. In particular, you will not need to run the full HMC algorithm or make any actual accept/reject decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Efficiency of \"random walk\" MCMC and HMC (4 points)\n",
    "\n",
    "In this exercise we will test the efficiency of Metropolis-Hastings (also known as \"random walk\" MCMC) and HMC in terms of the distance covered in various situations. \n",
    "\n",
    "All the experiments are done using the same correlated normal target as in Exercise 1.\n",
    "For Metropolis-Hastings MCMC, we will use as proposal $q(\\boldsymbol{\\theta}'; \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta}'; \\boldsymbol{\\theta}, \\sigma^2 \\mathbf{I}_2)$ with $\\sigma = 0.01$. For HMC, set $\\epsilon=0.01$ and $L$ as specified below.\n",
    "\n",
    "Distance covered here means the distance between the final point and the starting point of the chain, ignoring any intermediate steps.\n",
    "\n",
    "1. Always starting at $\\boldsymbol{\\theta}_0 = (0, 0)$, run Metropolis-Hastings MCMC 100 times for 1 step and 100 times for 100 steps. Compute the average Euclidean distance covered from the starting point in each case. Compare the 100 step distance with the theoretical random walk scaling of $\\sqrt{n}$ over $n$ steps.\n",
    "2. Always starting at $\\boldsymbol{\\theta}_0 = (0, 0)$, run HMC 100 times for 1 step with $L=1$ and 100 times for 1 step with $L=100$. Compute the average Euclidean distance covered from the starting point in each case. Compare the 100 step distance with the theoretical random walk scaling of $\\sqrt{n}$ over $n$ steps.\n",
    "3. Repeat task 1 starting at $\\boldsymbol{\\theta}_0 = (-2, 2)$.\n",
    "4. Repeat task 2 starting at $\\boldsymbol{\\theta}_0 = (-2, 2)$.\n",
    "5. In each case, report the two average Euclidean distances in Moodle. In which cases do the algorithms move more quickly than would be expected for a random walk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NUTS simulation (6 points)\n",
    "\n",
    "In this exercise we will experiment with the build-tree operation of the NUTS algorithm. We will use a simplified version of the algorithm with the following differences from the original:\n",
    "* We will only extend the path forward instead of choosing the direction randomly.\n",
    "* We will check for the termination condition only from the entire newly added path segment, not all subpaths as NUTS really would.\n",
    "\n",
    "Your tasks:\n",
    "1. Implement a simplified version of NUTS that works as follows:\n",
    "  - Start at iteration $j = 1$.\n",
    "  - In each iteration $j$, with $j = 1,2,\\ldots$, set $L = 2^{j-1}$ and run $L$ leapfrog steps forward. Deterministically move to the new position (i.e., there is no acceptance check). \n",
    "  - At the end of each iteration, a \"termination check\" is run. The algorithm stops when the first newly simulated position $(\\boldsymbol{\\theta}^-, \\mathbf{r}^-)$ and the last newly simulated position $(\\boldsymbol{\\theta}^+, \\mathbf{r}^+)$ of the trajectory simulated in the current iteration segment satisfy\n",
    "$$ (\\boldsymbol{\\theta}^+ - \\boldsymbol{\\theta}^-) \\cdot \\mathbf{r}^- < 0 \\quad \\text{or} \\quad (\\boldsymbol{\\theta}^+ - \\boldsymbol{\\theta}^-) \\cdot \\mathbf{r}^+ < 0. $$\n",
    "2. Run your algorithm on the 2D correlated normal with $\\rho = 0.998$, starting from the origin $\\boldsymbol{\\theta}_0 = (0, 0)$ with momentum $\\mathbf{r} = (1, 1/3)$ with $\\epsilon=0.05$. At which iteration $j$ of the algorithm will the termination criterion be triggered for the first time? Report the iteration number in Moodle.\n",
    "3. Compute the distance from the origin of the point at which the termination criterion is triggered, and report it in Moodle.\n",
    "4. NUTS includes a special condition to simulate $u \\sim \\mathrm{Uniform}(0, \\exp(-H(\\boldsymbol{\\theta}_0, \\mathbf{r}_0))$ and ignoring all points where $u > \\exp(-H(\\boldsymbol{\\theta}, \\mathbf{r}))$. Assuming $u = 0.98\\exp(-H(\\boldsymbol{\\theta}_0, \\mathbf{r}_0))$ (a pretty extreme value), how many points from the trajectory of $2^j$ points generated during the first $j$ iterations (including the initial state and the one during which the termination condition was triggered first) would be included in the next step of the algorithm when choosing the next point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HMC sampling for posterior inference (6 points)\n",
    "\n",
    "In this task we will apply HMC sampling for posterior inference of a linear regression model.\n",
    "We will use data from the Framingham Heart Study that studies the association between heart disease and its causes.\n",
    "A description of the data and its fields can be found at http://www.maths.utas.edu.au/DHStat/Data/Flow.html (see also problem 10.4 in the computer tasks done in class).\n",
    "\n",
    "We will use a linear regression model\n",
    "$$ y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, $$\n",
    "where $\\mathbf{x}_i$ is a row vector of input variables and $\\boldsymbol{\\beta}$ is a column vector of regression coefficients, and the noise term $\\epsilon_i$ follows the normal $\\mathcal{N}(0, \\sigma_y^2)$. We will not need a constant intercept term in the model because we will standardise the data.\n",
    "\n",
    "Assuming the residuals $\\epsilon_i, i=1,\\dots,n$ are independent, the log-likelihood of the model is\n",
    "$$ \\log p(y | x, \\boldsymbol{\\beta}, \\sigma_y) = \\sum_{i=1}^n \\log p(y_i | \\boldsymbol{\\beta}, x_i, \\sigma_y) = \\sum_{i=1}^n \\log \\mathcal{N}(y_i; \\; \\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma_y^2). $$\n",
    "\n",
    "1. Load the data using the below code. Standardise the data (both $x$ and $y$) by subtracting the mean and dividing each variable by **two** standard deviations (see http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf). Fit all the models using the standardised data as this makes sampling a lot easier.\n",
    "2. Using the above log-likelihood and priors\n",
    "$$ p(\\boldsymbol{\\beta}) = \\prod_j p(\\beta_j) = \\prod_j \\mathcal{N}(\\beta_j;\\; 0, 2^2), \\\\ p(\\sigma_y) = \\mathrm{Gamma}(\\sigma_y;\\; k=10, \\theta=0.1),$$\n",
    "use HMC or NUTS to draw samples from the posterior distribution $p(\\boldsymbol{\\beta}, \\sigma_y | x, y)$, when the prediction target variable $y$ is 'SBP' and the input variables $x$ are 'FRW', 'AGE' and 'CHOL'.\n",
    "\n",
    "To ensure the standard deviation $\\sigma_y$ is positive, express it as $\\sigma_y = \\exp(s_y)$, where $s_y$ is unbounded. Remember to apply the density transformation to transform the prior over $\\sigma_y$ to that over $s_y$!\n",
    "\n",
    "Make sure your sampler is properly tuned so that results will be reliable.\n",
    "\n",
    "Report your estimates of the posterior means and standard deviations of the coefficient in $\\beta$ for 'FRW', and of $s_y$. (*Note*: This should be $s_y$, not $\\sigma_y$!)\n",
    "\n",
    "Implementations of the gamma log-pdf needed for the prior that are compatible with Autograd and PyTorch are provided below for your convenience. (Autograd does not support `scipy.stats.gamma.logpdf` to the level we need. The PyTorch library implementation uses a different parametrisation, so the below implementation is easier for consistency.)\n",
    "\n",
    "The required tolerance is $\\pm 0.01$.\n",
    "\n",
    "*Note*: The NUTS implementation provided in the course materials sometimes gets stuck and produces a sample with really small variance. If you encounter such behaviour, you should disregard such results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autograd\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.special as scs\n",
    "import autograd\n",
    "\n",
    "def standardize(x):\n",
    "    raise(NotImplementedError(\"Not implemented yet\"))\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('https://www2.helsinki.fi/sites/default/files/atoms/files/fram.txt', sep='\\t')\n",
    "# convert the variables of interest to numpy arrays for autograd compatibility\n",
    "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group,\n",
    "#   age and cholesterole level\n",
    "x = fram[['FRW', 'AGE', 'CHOL']].values.astype(float)\n",
    "# target: Systolic blood pressure, examination 1\n",
    "y = fram['SBP'].values.squeeze().astype(float)\n",
    "\n",
    "xs = standardize(x)\n",
    "ys = standardize(y)\n",
    "\n",
    "def gamma_logpdf(x, k, theta):\n",
    "    \"\"\"Gamma distribution log pdf.\"\"\"\n",
    "    return -scs.gammaln(k) - k*np.log(theta) + (k-1)*np.log(x) - x / theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def standardize(x):\n",
    "    raise(NotImplementedError(\"Not implemented yet\"))\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('https://www2.helsinki.fi/sites/default/files/atoms/files/fram.txt', sep='\\t')\n",
    "# convert the variables of interest to numpy arrays for autograd compatibility\n",
    "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group,\n",
    "#   age and cholesterole level\n",
    "x = fram[['FRW', 'AGE', 'CHOL']].values.astype(float)\n",
    "# target: Systolic blood pressure, examination 1\n",
    "y = fram['SBP'].values.squeeze().astype(float)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.double)\n",
    "y = torch.tensor(y, dtype=torch.double)\n",
    "\n",
    "xs = standardize(x)\n",
    "ys = standardize(y)\n",
    "\n",
    "def gamma_logpdf(x, k, theta):\n",
    "    \"\"\"Gamma distribution log pdf (PyTorch).\"\"\"    \n",
    "    return -torch.lgamma(k) - k*torch.log(theta) + (k-1)*torch.log(x) - x / theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
