{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Mathematics and Statistics  \n",
    "MAST32001 Computational Statistics, Autumn 2022  \n",
    "Luigi Acerbi\n",
    "\n",
    "# Week 3 exercises\n",
    "\n",
    "**General hints**:  \n",
    "- **In order to estimate the accuracy of your answer**, it is recommended to run the sampler a few times and compute the standard deviation of the values you obtain. If the standard deviation of the values you obtain is smaller than the tolerance, the average of the values you obtain should be within the tolerance, assuming everything was done correctly.\n",
    "- Monte Carlo error scales as $1/\\sqrt{n}$ with the number of iterations $n$, i.e. you need to increase the number of iterations by a factor of 4 to cut the error in half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sampling a 1D distribution (4 points)\n",
    "\n",
    "Write a Metropolis-Hastings sampler to sample from the (unnormalised) distribution\n",
    "$$ P^*(\\theta) = \\cos^2(\\theta) \\exp(-|\\theta|^3) $$\n",
    "using a normal distribution with standard deviation $\\sigma$ as the proposal\n",
    "$q(\\theta' ; \\theta) = \\mathcal{N}(\\theta';\\; \\theta, \\sigma^2)$.\n",
    "\n",
    "1. Initialising the sampler at $\\theta = 0$, draw 10000 samples with $\\sigma=0.5$. Report the acceptance rate of the samples in Moodle.\n",
    "2. Find a value of $\\sigma$ that gives an acceptance rate close to the theoretically optimal value of $0.5$.\n",
    "3. Evaluate $\\mathbb{E}[\\cos(\\theta)]$, using samples drawn with the optimally tuned $\\sigma$. Remember to throw out the warm-up samples! Report the value you obtain in Moodle.\n",
    "\n",
    "The required tolerance for the answer is $\\pm 0.03$.\n",
    "\n",
    "*Hint*: It is useful to check your result by plotting a histogram of your samples and the target density (remembering it is unscaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling a 2D distribution (4 points)\n",
    "\n",
    "Write a Metropolis-Hastings sampler to evaluate the following expectations over the given (unnormalised) distributions using a suitably scaled multivariate normal distribution with diagonal covariance as the proposal.\n",
    "\n",
    "1. $E[\\theta_1^2 + \\theta_2^2]$, when $P^*(\\boldsymbol{\\theta}) = \\exp\\left(-3(\\sqrt{(\\theta_1-\\mu_1)^2 + (\\theta_2-\\mu_2)^2} - r^2)^2 \\right)$.\n",
    "2. $E[\\theta_1^2 + \\theta_2^2]$, when $P^*(\\boldsymbol{\\theta}) = \\exp\\left(-3(\\sqrt{(\\theta_1-\\mu_1)^2 + (\\theta_2-\\mu_2)^2} - r^2)^2 + |2\\theta_1 - \\theta_2|\\right)$.\n",
    "\n",
    "Here $\\mu_1 = 1, \\mu_2 = 2$ and $r = \\sqrt{2}$.\n",
    "\n",
    "The required tolerance for the answer is $\\pm 0.3$.\n",
    "\n",
    "*Hint*: It is useful to check your result by plotting a scatter plot of your samples and a contour plot of the logarithm of the target function. Remember to throw out the warm-up samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCMC sampling for the posterior of the mean of the Student-t distribution (6 points)\n",
    "\n",
    "In this exercise we will use MCMC to sample the posterior distribution over the location (or mean) parameter $\\mu$ of [Student's $t$-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) for a set of data points $X = (x_i)_{i=1}^n$ that are assumed to be conditionally independent given $\\mu$, i.e.\n",
    "$$ p(X | \\mu) = \\prod_{i=1}^n p(x_i | \\mu), $$\n",
    "where $p(x_i | \\mu)$ is the Student's $t$-distribution centred at $\\mu$ with $\\nu = 5$ degrees of freedom.\n",
    "\n",
    "Using the data loaded in the below code as $X$,\n",
    "write a Metropolis-Hastings sampler to draw samples from $p(\\mu | X)$ when the prior $p(\\mu)$ is as follows, and return the required posterior statistics to Moodle.\n",
    "\n",
    "1. $p(\\mu) = \\mathcal{N}(\\mu;\\; 0, 1^2)$. Report the posterior mean and standard deviation of $\\mu$, i.e. mean and standard deviation of $p(\\mu | X)$ estimated from the Monte Carlo samples to Moodle.\n",
    "2. $p(\\mu) = \\mathrm{Uniform}(\\mu;\\; -5, 5)$. Report the posterior mean and standard deviation of $\\mu$, i.e. mean and standard deviation of $p(\\mu | X)$ estimated from the Monte Carlo samples to Moodle.\n",
    "\n",
    "The require tolerance is $\\pm 0.03$.\n",
    "\n",
    "*Hints*: \n",
    "- Please see Chapter 7 of the course notes for more background. \n",
    "- You can evaluate the log-pdf of the Student's $t$-distribution using `scipy.stats.t.logpdf()`.\n",
    "- You can compute posterior mean and standard deviation directly as the mean and standard deviation of your samples. \n",
    "- A suitably scaled normal distribution centred at the current point should be a good proposal.\n",
    "- Remember to throw out the warm-up samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://www2.helsinki.fi/sites/default/files/atoms/files/toydata2.txt', sep='\\t', header=None)\n",
    "data = data.values\n",
    "data = np.array(data[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MCMC sampling for the posterior of gamma distribution parameters (6 points)\n",
    "\n",
    "In this exercise we test Bayesian estimation of the parameters $\\alpha, \\beta$ of the [gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) for a given data set. We will use the shape/rate parametrisation of the gamma distribution. Under this parametrisation, the probability density function of the gamma distribution is\n",
    "$$ p(x) = \\mathrm{Gamma}(x;\\; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x), \\quad \\alpha,\\beta > 0,$$\n",
    "where $\\Gamma(x)$ is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function).\n",
    "\n",
    "The prior distributions of the parameters $\\alpha$ and $\\beta$ are\n",
    "$$ p(\\alpha) = \\mathrm{Gamma}\\left(\\frac{1}{2}, \\frac{1}{2}\\right), \\quad p(\\beta) = \\mathrm{Gamma}\\left(\\frac{1}{2}, \\frac{1}{2}\\right), $$\n",
    "and we assume a factorized prior, i.e. $$p(\\alpha, \\beta) = p(\\alpha) p(\\beta).$$\n",
    "\n",
    "Please remember that the Gamma distribution has the additional constraint that its parameters $\\alpha$ and $\\beta$ need to be positive. This can be accomplised by parametrising using new unconstrained variables $a,b$ with $\\alpha = \\exp(a)$ and $\\beta = \\exp(b)$.\n",
    "\n",
    "1. Load the data set $\\mathcal{D} = \\{ x_i | i = 1, \\dots, n \\}$ using the function below.\n",
    "2. Implement the log-probability density function of the gamma distribution. Report the value of the log-probability density $\\ln p(\\alpha)$ at $\\alpha = 1$.\n",
    "3. Transform the prior probability densities of $\\alpha$ and $\\beta$ to distributions over unbounded variables $a, b$. Report the log-density value $\\ln p(a)$ evaluated at $a=1$ in Moodle. (*Hint*: See Sec. 7.3.3. in the course notes!)\n",
    "4. Sample from the posterior $$p(a, b | \\mathcal{D}) \\propto p(\\mathcal{D}| a, b) p(a) p(b) = \\left[\\prod_{i=1}^n \\text{Gamma}(x_i; \\exp(a), \\exp(b))\\right] p(a) p(b)$$ using a Metropolis-Hastings sampler. *Note*: The symbol $\\propto$ means \"proportional to\", since we omitted the normalization constant $p(\\mathcal{D})$ in the denominator of Bayes' theorem. We can do this since Metropolis-Hastings (as most MCMC samplers) does not need the target density to be normalized.\n",
    "5. Transform the values of $a,b$ back to $\\alpha, \\beta$ and report the posterior means and standard deviations (`np.mean(alpha_samples)`, `np.std(alpha_samples)`, `np.mean(beta_samples)`, `np.std(beta_samples)`) for $\\alpha$ and $\\beta$ in Moodle.\n",
    "\n",
    "The required tolerance for the posterior statistics is $\\pm 0.1$.\n",
    "\n",
    "*Hints*: \n",
    "- You can check your implementations of the probability density functions by making sure they are properly normalised using e.g. `scipy.integrate.quad()`. \n",
    "- A multivariate normal distribution centred at the current point should be a good proposal. Remember to throw out the warm-up samples!\n",
    "- As a help, we already provide an implementation of the log pdf of the gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special as scs\n",
    "\n",
    "# 1 load dataset\n",
    "\n",
    "data1 = pd.read_csv('https://www2.helsinki.fi/sites/default/files/atoms/files/toydata.txt', sep='\\t', header=None)\n",
    "data1 = data1.values\n",
    "data1 = np.array(data1[:,0])\n",
    "\n",
    "# 2\n",
    "\n",
    "def gamma_logpdf(x, alpha, beta):\n",
    "    \"\"\"Log pdf of the gamma distribution with shape/rate parameters alpha and beta.\"\"\"\n",
    "    return (alpha*np.log(beta) - scs.gammaln(alpha) + (alpha-1) * np.log(x) - beta * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
