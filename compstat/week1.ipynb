{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Mathematics and Statistics  \n",
    "MAST32001 Computational Statistics, Autumn 2022  \n",
    "Luigi Acerbi  \n",
    "\n",
    "# Week 1 exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as slg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input ranges for overflow and underflow (4 pts)\n",
    "\n",
    "i. Write a program to find the largest integer for which `exp()` over double precision floating point numbers (`float64`) returns a finite value.\n",
    "\n",
    "ii. The logistic function $$ \\phi(x) = \\frac{1}{1 + \\exp(-x)} $$ is often used to map the real line to probabilities in the range $(0, 1)$, for example in logistic regression. For real numbers, the equation $ \\phi(x) = 1 $ has no solution, but the same is not true for floating point numbers on a computer. Write a program to determine the smallest integer $ x $ for which $ \\phi(x) =_F 1$ when using double precision floating point (`float64`) arithmetic.\n",
    "\n",
    "*Note*: \"Write a program\" means you should not try out all values manually, but program the computer to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21771/352962539.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  while not np.isinf(np.exp(large_integer)):\n"
     ]
    }
   ],
   "source": [
    "def findLargestInteger():\n",
    "    large_integer = 10\n",
    "    while not np.isinf(np.exp(large_integer)):\n",
    "        large_integer += 1\n",
    "    return large_integer - 1\n",
    "\n",
    "# The result is 709\n",
    "print(findLargestInteger())\n",
    "# print(np.exp(709))\n",
    "\n",
    "\n",
    "def findBigIntegerForOne():\n",
    "    large_integer = 10\n",
    "    while 1/(1 + np.exp(-1*large_integer)) != 1:\n",
    "        large_integer += 1\n",
    "    return large_integer\n",
    "\n",
    "# The result is 37\n",
    "print(findBigIntegerForOne())\n",
    "# print(1/(1+np.exp(-37)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numerical computation of binomial probabilities (4 pts)\n",
    "\n",
    "Write a computer program to compute sums of binomial probabilities:\n",
    "$$ f(l, u, n, p) = \\sum_{i=l}^u \\binom{n}{i} p^i (1-p)^{n-i}. $$\n",
    "Use it to compute:\n",
    "\n",
    "i. $ f(0, 5, 10, 0.25) = \\sum_{i=0}^{5} \\binom{10}{i} 0.25^i 0.75^{10-i} $\n",
    "\n",
    "ii. $ f(10, 20, 20, 0.25) = \\sum_{i=10}^{20} \\binom{20}{i} 0.25^i 0.75^{20-i} $\n",
    "\n",
    "iii. $ f(40, 60, 100, 0.25) = \\sum_{i=40}^{60} \\binom{100}{i} 0.25^i 0.75^{100-i} $\n",
    "\n",
    "iv. $ f(75, 100, 100, 0.25) = \\sum_{i=75}^{100} \\binom{100}{i} 0.25^i 0.75^{100-i} $\n",
    "\n",
    "*Hint*: Remember to use log probabilities and `logsumexp` as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0, 5, 10, 0.25) =  0.9802722930908243\n",
      "f(10, 20, 20, 0.25) =  0.01386441694376125\n",
      "f(40, 60, 100, 0.25) =  0.0006865922079344215\n",
      "f(75, 100, 100, 0.25) =  0.9802722930908243\n"
     ]
    }
   ],
   "source": [
    "def logBinomialCoeff(n, k):\n",
    "    if k > n:\n",
    "        raise Exception(\"k cannot be greater than n in binomial coefficient\")\n",
    "    res = 0\n",
    "    for i in range(1, n+1):\n",
    "        res += np.log(i)\n",
    "    for i in range(1, k+1):\n",
    "        res -= np.log(i)\n",
    "    for i in range(1, n-k+1):\n",
    "        res -= np.log(i)\n",
    "    return res\n",
    "    \n",
    "def calculateSumTerm(n, i, p):\n",
    "    log_ver = logBinomialCoeff(n, i)\n",
    "    log_ver += i*np.log(p) + (n-i)*np.log(1-p)\n",
    "    return np.exp(log_ver)\n",
    "\n",
    "def calculateSum(l, u, n, p):\n",
    "    res = 0\n",
    "    for i in range(l, u+1):\n",
    "        res += calculateSumTerm(n, i, p)\n",
    "    return res\n",
    "\n",
    "print(\"f(0, 5, 10, 0.25) = \", calculateSum(0, 5, 10, 0.25))\n",
    "print(\"f(10, 20, 20, 0.25) = \", calculateSum(10, 20, 20, 0.25))\n",
    "print(\"f(40, 60, 100, 0.25) = \", calculateSum(40, 60, 100, 0.25))\n",
    "print(\"f(75, 100, 100, 0.25) = \", calculateSum(0, 5, 10, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical evaluation of multivariate normal probabilities (4 pts)\n",
    "\n",
    "Consider the $d$-variate normal distribution $\\mathcal{N}(x;\\; \\mu, \\Sigma)$ with the log-probability density\n",
    "$$ \\ln p(x;\\; \\mu, \\Sigma) = -\\frac{d}{2} \\ln(2 \\pi) - \\frac{1}{2} \\ln |\\det \\Sigma| - \\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu). $$\n",
    "Let $\\mu = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, $\\Sigma = \\begin{pmatrix}2^2 & 2\\rho \\\\ 2\\rho & 1\\end{pmatrix}$.\n",
    "\n",
    "Evaluate the following log-probabilities:\n",
    "\n",
    "i. $ \\ln p( \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} ; \\; \\mu, \\Sigma), $ when $\\rho = 0.8$\n",
    "\n",
    "ii. $ \\ln p( \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} ; \\; \\mu, \\Sigma), $ when $\\rho = 0.999$\n",
    "\n",
    "iii. $ \\ln p( \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} ; \\; \\mu, \\Sigma), $ when $\\rho = 0.999$\n",
    "\n",
    "iv. $ \\ln p( \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} ; \\; \\mu, \\Sigma), $ when $\\rho = -0.999$\n",
    "\n",
    "*Note*: `ln` denotes the logarithm in natural base (for us it is the same as `log`, per our usual notational convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:  -3.224171427529236\n",
      "Case 2:  -3.224171427529236\n",
      "Case 3:  -66.00556212287773\n",
      "Case 4:  -565.7554370603533\n"
     ]
    }
   ],
   "source": [
    "def logDetSigma(sigma):\n",
    "    diag = sigma.diagonal()\n",
    "    diag = np.log(diag)\n",
    "    return 2*np.sum(diag)\n",
    "\n",
    "def quadFormEval(L, x, mu):\n",
    "    z = slg.solve_triangular(L, x-mu, lower=True)\n",
    "    return np.transpose(z) @ z\n",
    "\n",
    "def calcLnP(x, mu, sigma):\n",
    "    L = slg.cholesky(sigma, lower=True)\n",
    "    d = sigma.shape[0]\n",
    "    return -0.5*d*np.log(2*np.pi) - 0.5*logDetSigma(sigma) - 0.5*quadFormEval(L, x, mu)\n",
    "\n",
    "def createSigma(rho):\n",
    "    return np.matrix([ [4, 2*rho], [2*rho, 1] ])\n",
    "\n",
    "mu = np.array([[0], [0]])\n",
    "\n",
    "# 1\n",
    "x = np.array([[0], [0]])\n",
    "sigma = createSigma(0.8)\n",
    "print(\"Case 1: \", calcLnP(x, mu, sigma).item())\n",
    "\n",
    "# 2\n",
    "x = np.array([[0], [0]])\n",
    "sigma = createSigma(0.999)\n",
    "L = slg.cholesky(sigma, lower=True)\n",
    "print(\"Case 2: \", calcLnP(x, mu, sigma).item())\n",
    "\n",
    "# 3\n",
    "x = np.array([[1], [1]])\n",
    "sigma = createSigma(0.999)\n",
    "print(\"Case 3: \", calcLnP(x, mu, sigma).item())\n",
    "\n",
    "# 4\n",
    "x = np.array([[1], [1]])\n",
    "sigma = createSigma(-0.999)\n",
    "print(\"Case 4: \", calcLnP(x, mu, sigma).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rejection sampling of Beta(2, 3) (4 pts)\n",
    "\n",
    "Write a program to draw samples from the $\\text{Beta(2, 3)}$ distribution\n",
    "$ p(x) = 12 x (1-x)^2, \\quad 0 \\le x \\le 1 $  \n",
    "using the uniform distribution on the interval $(0, 1)$ as the proposal distribution $q(x)$.\n",
    "\n",
    "Remember that you need to find a constant $M$ such that $p(x) \\le M q(x)$ for all $x$. Plotting the function $p(x)/q(x)$ is a good way to start looking for one.\n",
    "\n",
    "Plot a normed histogram of your samples together with the density to check that they match.\n",
    "\n",
    "Draw at least 10000 samples from the distribution and report your estimate of $ \\mathbb{E}[x^5] $ for $ x \\sim \\mathrm{Beta}(2, 3)$.\n",
    "\n",
    "*Note*: Please report the actual value you obtain to Moodle, not your guess of the true value. Values within the expected Monte Carlo error will be accepted as correct.\n",
    "\n",
    "*Hint for evaluating the expectation* $\\mathbb{E}[x^5]$: assuming we have samples $x_i \\sim p(x), i = 1, \\dots, n$ following the distribution $p(x)$, we can compute a simple Monte Carlo approximation of the expectation of an arbitrary function $g(x)$ as\n",
    "$$ \\mathbb{E}_p[g(x)] \\approx \\frac{1}{n} \\sum_{i=1}^n g(x_i), \\qquad \\text{ for } x_i \\sim p(x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rejection sampling of a Gaussian with Laplace  (4 pts)\n",
    "\n",
    "Consider the standard $\\text{Laplace}(0, 1)$ distribution defined by:\n",
    "$$ q(x) = 1/2 \\exp(- | x | ),$$ \n",
    "and the standard normal distribution defined by\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( - \\frac{1}{2} x^2 \\right).$$\n",
    "For both the Laplace and normal distribution, $x \\in (-\\infty, \\infty)$.\n",
    "\n",
    "1. Write a program to draw samples from the $\\text{Laplace}(0, 1)$ distribution. (*Hint*: transformation considered on Lecture 2.) Test your program by comparing the normed histogram of the samples with the density.\n",
    "2. Design a method to draw samples from the standard normal using the standard Laplace distribution as the proposal. (*Hint*: you need to find a constant $M$ such that $p(x) \\le M q(x)$ for all $x$. Plotting the function $p(x)/q(x)$ is again useful for looking for one.)\n",
    "3. Use 10000 samples to compute the expectation $ E[x^4] $ for $ x $ following the standard normal distribution.\n",
    "4. Assume the underlying uniform random generator uses full 53 bits of precision of `float64` and produces random numbers in the range $[2^{-53}, 1]$ with increments of $2^{-53}$. Considering the same setup as above, what is the largest value the rejection sampling method can generate, if it were executed for long enough? \n",
    "\n",
    "Hints for part 5.4:\n",
    "- You need to check that the algorithm can both generate an extreme value, and accept it. What's the equation for accepting a value in rejection sampling?\n",
    "- You can use the same $M$ that you used in part 5.2 (as long as you chose a reasonable $M$, the answer will be valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
