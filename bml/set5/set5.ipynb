{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Gradient-based variational inference\n",
    "\n",
    "Let us consider the exact same model and data as in the previous exercise set:\n",
    "\\begin{align*}\n",
    "x_n &\\sim \\text{Poisson}(uv),\\\\\n",
    "u &\\sim \\text{Gamma}(6, 1),\\\\\n",
    "v &\\sim \\text{Gamma}(3, 3).\n",
    "\\end{align*}\n",
    "with the same data $\\mathbf{x}=[5, 3, 9, 13, 5, 3, 5, 1, 2, 7, 6, 5, 6, 7, 4]$.\n",
    "\n",
    "Now we use gradient-based methods to learn variational approximation for the posterior. \n",
    "\n",
    "Write code that:\n",
    "1. Specifies the approximation using suitable pytorch.distribution\n",
    "2. Evaluates the ELBO by using $M$ samples drawn from the approximation with .rsample()\n",
    "3. Optimizes ELBO wrt the parameters of the approximation\n",
    "4. Plots the approximation on top of Gibbs samples\n",
    "5. Plots convergence of the ELBO estimate (note that with small $M$ this may be somewhat noisy). Use this plot to check that your optimizer actually has converged to a good solution.\n",
    "\n",
    "Use the code to try out alternative models and approximations. For each of the three alternatives below, always show both the convergence curve and the resulting posterior approximation and **explain what you see**.\n",
    "1. Use the same approximation family as before, so that $q(u,v)$ is a product of two gamma distributions. Do you get the same result as with CAVI?\n",
    "2. Use multivariate normal approximation, so that $q(u,v)$ is a bivariate normal distribution. Explain what changed.\n",
    "3. Change the prior $p(u,v)$ from product of two gamma distributions to product of two half-normal distributions with scales of your own choosing. Explain what happens.\n",
    "\n",
    "HINTS:\n",
    "1. https://pytorch.org/docs/stable/distributions.html helps with the distribution syntax etc.\n",
    "2. You can use any $M$, but probably it is best to avoid very small ones. It is a good idea to quickly explore how the estimate behaves as a function of $M$.\n",
    "3. The easiest way to parameterize the multivariate normal is to use some 2x2 matrix $A$ as learnable parameters but then give \"L = torch.tril(A)\" as the Cholesky parameter for the multivariate normal distribution. We have one extra parameter in $A$ that is never optimized or used (the top right corner), but that does not matter.\n",
    "4. Be careful with bounds: You have positivity requirement for two things here, for the **parameters of the approximation terms** (for gamma approximation) and additionally for **the samples u and v drawn from the approximation** (for the normal approximation that could result in negative samples). You can, for example, use \"torch.nn.functional.softplus(alpha_unconstrained)\" as the alpha-parameter in a gamma distribution, and you can truncate the samples from the normal distribution to a small positive constant or push those also through softplus. The former is always valid but the latter is strictly speaking wrong as we then use truncated normal as approximation, but we can ignore this problem here for simplicity.\n",
    "5. It is possible to write general code that directly supports arbitrary distributions, but that is quite tedious. You can definitely have separate copies of your code for the different choices of $q(u,v)$ if that is easier -- that's what I will be doing in model solutions anyway.\n",
    "6. You already know the optimal solution for the gamma approximation based on Exercise 3. The values are quite large, so you might want to also initialize your approximation with numbers of similar magnitude to make the optimization problem a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below again has the model definition and Gibbs sampler for ease of result presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "u_alpha = 6.\n",
    "u_beta = 1.\n",
    "v_alpha = 3.\n",
    "v_beta = 3.\n",
    "\n",
    "x = np.array([5,3,9,13,5,3,5,1,2,7,6,5,6,7,4])\n",
    "N = len(x)\n",
    "\n",
    "# Priors\n",
    "u_prior = stats.gamma(u_alpha, scale=1./u_beta)\n",
    "v_prior = stats.gamma(v_alpha, scale=1./v_beta)\n",
    "\n",
    "def log_density(data, u, v):\n",
    "    likelihood = stats.poisson(u*v)\n",
    "    return np.sum(likelihood.logpmf(data)) + u_prior.logpdf(u) + v_prior.logpdf(v)\n",
    "\n",
    "def Gibbs(x, u, v, T):\n",
    "    # Storage for samples\n",
    "    samples = [u, v]\n",
    "    sumx = sum(x)\n",
    "    N = len(x)\n",
    "    for t in range(T):\n",
    "        # Sample u conditional on v and data\n",
    "        log_lambda_term = sumx + u_alpha\n",
    "        lambda_term = N*v + u_beta\n",
    "        u = np.random.gamma(log_lambda_term, scale = 1.0 / lambda_term)\n",
    "        \n",
    "        # Sample v conditional on u and data\n",
    "        log_lambda_term = sumx + v_alpha\n",
    "        lambda_term = N*u + v_beta\n",
    "        v = np.random.gamma(log_lambda_term, scale = 1.0 / lambda_term)\n",
    "\n",
    "        samples = np.vstack([samples, [u, v]])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softplus as sp\n",
    "\n",
    "# Define data and the optimization problem\n",
    "data = torch.tensor(x)\n",
    "parameters = ... # Set also some initial values here\n",
    "opt = optim.Adam(parameters, lr=...)\n",
    "M = ...\n",
    "nIter = ...\n",
    "\n",
    "# Priors\n",
    "u_prior = torch.distributions.Gamma(u_alpha, u_beta)\n",
    "v_prior = torch.distributions.Gamma(v_alpha, v_beta)\n",
    "\n",
    "    \n",
    "ELBOS = list()\n",
    "for i in range(nIter):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Define approximation\n",
    "    q_uv = ...\n",
    "\n",
    "    # Obtain samples\n",
    "    uv = ...\n",
    "    \n",
    "    # Compute ELBO\n",
    "    ELBO = ...\n",
    "    \n",
    "    # We need a minimization problem but ELBO is to be maximized\n",
    "    loss = -ELBO\n",
    "    loss.backward()\n",
    "    ELBOS.append(ELBO.item())\n",
    "    opt.step()\n",
    "    \n",
    "# Plotting goes here\n",
    "# 1. Convergence plot\n",
    "# 2. Illustration of the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Variational autoencoder\n",
    "\n",
    "The code below implements most parts of a **variational autoencoder for the MNIST digits data** and also downloads the data for you on the first run. We use two-dimensional representations (K=2) for ease of plotting, fully connected neural networks for all components of the model, and normal likelihood for simplicity. A better model using convolutional networks, higher-dimensional representations and more suitable likelihoods would follow the exact same general algorithm.\n",
    "\n",
    "You are free to consult external sources (including ones that provide code) to understand the model better, but remember to mention what you looked at.\n",
    "\n",
    "Complete the implementation by\n",
    "1. Specifying the prior distribution of the latent variables as a normal distribution with zero mean and unit covariance\n",
    "2. Compute the parameters of the approximation for the set of samples in the current mini-batch. Remember that the standard deviation has to be positive.\n",
    "3. Form the actual approximation and obtain samples from it using .rsample()\n",
    "4. Compute the ELBO, using normal likelihood with fixed standard deviation (obs_sigma). Since every data point is independent both in terms of the prior and the likelihood, it is probably easiest to write the expression for a single data point and then take the mean of those. By taking the mean instead of sum, you get numbers that are comparable over multiple batch sizes. Remember that our observations have D=784 dimensions and you need to sum over those in the log-likelihood part.\n",
    "\n",
    "Then run the code and inspect how it works. You should be seeing ELBO improve and the mean reconstructions for the images should look somehow reasonable. If this is not the case, try to guess what is wrong and debug your code. You can also change the parameters (number of hidden layers etc) if you want to further improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyperparameters of the model and optimization\n",
    "K = 2\n",
    "obs_sigma = 0.1\n",
    "batch_size = 50\n",
    "numEpoch = 20 \n",
    "lr = 0.001\n",
    "\n",
    "# MNIST data \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prior distribution for latent variables\n",
    "p_z = ...\n",
    "\n",
    "# Encoder and decoder specifications. Both are fully connected networks, so no CNN magic here\n",
    "D = 28*28\n",
    "H = 40\n",
    "encoder_mu = nn.Sequential(nn.Linear(D,H), nn.ReLU(),\n",
    "                           nn.Linear(H,H), nn.ReLU(),\n",
    "                           nn.Linear(H,K,bias=True))\n",
    "encoder_sigma = nn.Sequential(nn.Linear(D,H), nn.ReLU(),\n",
    "                              nn.Linear(H,H), nn.ReLU(),\n",
    "                              nn.Linear(H,K,bias=True))\n",
    "decoder = nn.Sequential(nn.Linear(K,H), nn.ReLU(),\n",
    "                        nn.Linear(H,H), nn.ReLU(),\n",
    "                        nn.Linear(H,D,bias=True))\n",
    "\n",
    "# Optimize over parameters of all networks\n",
    "params = list(encoder_mu.parameters()) + list(encoder_sigma.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "elbos = []\n",
    "for i in tqdm(range(numEpoch)):\n",
    "    batches = iter(train_loader)\n",
    "\n",
    "    epochloss = 0.\n",
    "    for j in range(len(batches)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Next batch of samples\n",
    "        batch_data, batch_targets = next(batches)\n",
    "        x = batch_data.reshape((batch_size,-1))\n",
    "    \n",
    "        # Form parameters of approximation\n",
    "        mu_approx = ...\n",
    "        sigma_approx = ...\n",
    "        \n",
    "        # Sample from approximation\n",
    "        q_z_x = ...\n",
    "        z = ...\n",
    "\n",
    "        # Find mean parameters of observed data\n",
    "        x_mean = ...\n",
    "    \n",
    "        # ELBO\n",
    "        ELBO_for_one_point = ...\n",
    "        loss = - torch.mean(ELBO_for_one_point)\n",
    "        epochloss += loss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    elbos.append(-epochloss/len(batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find code for making the plots you need. Feel free to improve the plots if you want to show something else, for example illustrate the variance of the representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence plot\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.plot(elbos)\n",
    "plt.title(\"Convergence plot\")\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Illustration of some samples, showing the mean of the reconstruction\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "# Note: Uses the values from the last iteration of the algorithm, so this shows some\n",
    "# examples in the last minibatch\n",
    "for sam in range(8):\n",
    "    plt.subplot(4,4,sam*2+1)\n",
    "    plt.imshow(x[sam,:].reshape(28,28))\n",
    "\n",
    "    plt.subplot(4,4,sam*2+2)\n",
    "    plt.imshow(x_mean[sam,:].detach().reshape(28,28))\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "# 2D visualization of the data in the latent space\n",
    "# Do this for 10,000 examples, forming a batch of those and computing q() for them\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])),\n",
    "  batch_size=10000, shuffle=True)\n",
    "\n",
    "batches = iter(train_loader)\n",
    "batch_data, batch_targets = next(batches)\n",
    "x = batch_data.reshape((10000,-1))\n",
    "\n",
    "# Find the parameters of the approximation in the same way as during optimization\n",
    "mu_approx = ...\n",
    "sigma_approx = ...\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "for c in range(10):\n",
    "    plt.plot(mu.detach()[batch_targets==c,0], mu.detach()[batch_targets==c,1], '.', alpha=0.8)\n",
    "    # Perhaps add here some way of illustrating the variance of the embedding\n",
    "    #plt.plot(...)\n",
    "\n",
    "plt.title(\"Latent representation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Normalizing flows as variational approximation\n",
    "\n",
    "Read through the paper \"Sylvester Normalizing Flows for Variational Inference\" by van den Berg et al. (UAI, 2018) available at http://auai.org/uai2018/proceedings/papers/156.pdf and watch the 15-minute conference precentation explaining the paper available at https://www.youtube.com/watch?v=VeYyUcIDVHI&ab_channel=UAI2018. Note that this is not the first paper tha proposed using normalising flows for variational inference, but I chose this because it is easier to read and understand.\n",
    "\n",
    "Answer to the following questions. If you use the notebook to write the answers, please use the 'Markdown' mode for the cell and write equations in latex notation inside dollar symbols. Hand-written answers are also fine, and illustrations that help understand the concepts are appreciated.\n",
    "1. Explain briefly **how the Sylvester flow works** -- explain it also using mathematical notation and tell the main characteristics. No need to go through any proofs or the details for different special cases, but focus on explaining the main principle.\n",
    "2. Tell how we can **use the flow as variational approximation**, explaning it in words while also providing the details. You can either describe the details mathematically or write python-like pseudocode where you explain how specific quantities are being transformed and what do we optimize for etc.\n",
    "3. What do you think the result would be if you applied this for our gamma-Poisson problem studied throughout the exercises? What would the resulting posterior approximation most likely be? Do you see challenges in using this for that problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
