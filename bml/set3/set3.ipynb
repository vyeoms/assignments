{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Laplace approximation for the Gamma-Poisson model\n",
    "\n",
    "Let us consider the exact same model and data as in the previous exercise set:\n",
    "\\begin{align*}\n",
    "x_n &\\sim \\text{Poisson}(uv),\\\\\n",
    "u &\\sim \\text{Gamma}(6, 1),\\\\\n",
    "v &\\sim \\text{Gamma}(3, 3).\n",
    "\\end{align*}\n",
    "where individual observations $\\mathbf{x}=[5, 3, 9, 13, 5, 3, 5, 1, 2, 7, 6, 5, 6, 7, 4]$ are conditionally independent of each other. \n",
    "\n",
    "Now we use distributional methods to approximate the posterior, instead of sampling from it. The first problem is about Laplace approximation.\n",
    "\n",
    "1. Find the **Laplace approximation** for the posterior. First find the maximum of $\\log p(\\mathbf{x},u,v)$ with respect to $u$ and $v$ with any algorithm (e.g. gradient-descent) and then compute the precision matrix as the negative of the Hessian of the same density, evaluated at $\\hat u, \\hat v$ that correspond to the optimum.\n",
    "2. **Compare the approximation with the results of Gibbs or Metropolis** (last week), paying particular attention into the tails of the distribution. Explain your main findings: How do the results differ and would you be happy to use the approximation here? \n",
    "\n",
    "For computing the Hessian you can either compute the second derivatives manually (recommended; remember to return also scans of your derivation) or use automatic differentiation in PyTorch (if familiar with that), or both -- then you can check whether your derivations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "# Define the model, using the log-density as before\n",
    "logdensity = ...\n",
    "\n",
    "# Step 2:\n",
    "# Find the best point estimate, used as the mean of the approximation\n",
    "# Hint: You can do this in the same way as in Exercise 1 where you optimized logistic regression in PyTorch\n",
    "uhat = ...\n",
    "vhat = ...\n",
    "\n",
    "# Step 3:\n",
    "# Determine the covariance of the approximation\n",
    "# - Compute all second derivatives\n",
    "# - Convert to the covariance\n",
    "Covhat = ...\n",
    "\n",
    "#\n",
    "# EXAMPLE PLOTTING CODE THAT MAY BE HELPFUL\n",
    "#\n",
    "ugrid = np.arange(2,17,0.05)\n",
    "vgrid = np.arange(0.2,3.5,0.05)\n",
    "uset, vset = np.meshgrid(ugrid, vgrid)\n",
    "uv = np.column_stack([uset.flat, vset.flat])\n",
    "\n",
    "# Density values at the grid points\n",
    "pdf_laplace = stats.multivariate_normal(np.array([uhat,vhat]), CovHat).pdf(uv).reshape(uset.shape)\n",
    "\n",
    "# Add some Gibbs samples on top to compare\n",
    "gibbs_samples = Gibbs(x, 3.0, 0.5, 2500)\n",
    "plt.plot(gibbs_samples[500:,0], gibbs_samples[500:,1], '.', alpha=0.2)\n",
    "\n",
    "# Then plot the countours \n",
    "_ = plt.contour(uset, vset, pdf_laplace, levels=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Variational approximation for the Gamma-Poisson model\n",
    "\n",
    "Continue still with the exact same model. Now we form a variational approximation for the posterior using the coordinate-ascent VI method (CAVI).\n",
    "\n",
    "As the approximation we use $q(u,v|\\lambda) = q(u|a,b) q(v|c,d)$ where $\\lambda=\\{a,b,c,d\\}$ are the parameters of the approximation. \n",
    "\n",
    "1. **Figure out the update rules** for the approximation parameters $\\lambda$, starting from the conditional distributions $p(u|\\mathbf{x},v)$ and $p(u|\\mathbf{x},v)$ derived in the previous exercise. The update rule for $q(u|a,b)$ builds on $p(u|\\mathbf{x},v)$ but rather than conditioning on a specific value of $v$ we need to integrate $\\log p(u|\\mathbf{x},v)$ over the current $q(v|c,d)$. Return your derivations.\n",
    "2. **Write code that implements CAVI**, using the update equations. Note that you need to initialize your algorithm with some vlaues for the $\\lambda$ -- think about how to do this, to avoid very bad initializations.\n",
    "3. **Compare the approximation with the results of Gibbs or Metropolis** (last week) and the Laplace approximation, again paying attention to the tails. Explain briefly your main findings. How does the variational approximation differ from Laplace? Which one would you prefer here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative optimization of q_u and q_v\n",
    "q_u = ...\n",
    "q_v = ...\n",
    "\n",
    "\n",
    "# For the plotting you can get the density with something like the following, assuming your q_u and q_v are common distributions\n",
    "pdf_VI = (q_u.pdf(uset)*q_v.pdf(vset)).reshape(uset.shape)\n",
    "\n",
    "# Then plot the countours\n",
    "_ = plt.contour(uset, vset, pdf_VI, levels=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Variational approximation for mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to derive and implement CAVI algorithm for a mixture of univariate Gaussians. Section 28.2.1 in Murphy's book explains the model briefly and Section 10.2.6 covers variational inference for a more general (multivariate) mixture model. Start by reading these sections.\n",
    "\n",
    "The model is defined as\n",
    "\\begin{align*}\n",
    "p(\\boldsymbol{\\pi}|\\alpha) &= \\text{Dirichlet}(\\alpha)\\\\\n",
    "p(\\mu_k|\\tau_0) &= \\mathcal{N}(0, \\tau_0)\\\\\n",
    "p(\\tau_{k}|\\alpha_0, \\beta_0) &= \\text{Gamma}(\\alpha_0, \\beta_0)\\\\\n",
    "p(z_n|\\boldsymbol{\\pi}) &= \\text{Categorical}(\\boldsymbol{\\pi})\\\\\n",
    "p(x_{n}|\\{\\mu\\},\\{\\tau\\}, z_n) &= \\mathcal{N}(\\mu_{z_n},\\tau_{z_n})\n",
    "\\end{align*}\n",
    "where $\\tau$ is precision. The notation $\\mu_{z_n}$ on the last row indicates we use the mean of the $z_n$:th cluster. The joint likelihood and the conditional distributions needed for a Gibbs sampler are provided below as a starting point to make derivation of variational updates easier. \n",
    "\n",
    "If we write the joint likelihood of cluster allocations and data as\n",
    "\\begin{align*}\n",
    "p(x_n,z_n|\\dots) = \\left (\\prod_k p(z_n|\\boldsymbol{\\pi})   p(x_n|\\mu_{k},\\tau_{k}) \\right )^{I[z_n=k]},\n",
    "\\end{align*}\n",
    "\n",
    "we get a nice expression for the log-density as\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \n",
    "& \\left [ \\prod_n \\prod_k \\left ( p(z_n|\\boldsymbol{\\pi}) p(x_{n}|\\mu_{z_n},\\tau_{z_n})\\right )^{I[z_n=k]} \\right ]\n",
    "\\left [ \\prod_k p(\\mu_k)p(\\tau_k) \\right ]\n",
    "p(\\boldsymbol{\\pi})\\\\\n",
    "= &\n",
    "\\sum_n \\sum_k  \\mathbb{I}[z_n=k] \\left [ \\log \\pi_k + \\left ( -\\frac{1}{2} \\log (2\\pi) + \\frac{1}{2} \\log \\tau_{d} - \\frac{1}{2} \\tau_{d} (x_{d} - \\mu_{d})^2 \\right ) \\right ]\\\\\n",
    "&+ \n",
    "\\sum_k \\left ( -\\frac{1}{2}\\log (2\\pi) + \\frac{1}{2}\\log \\tau_0 - \\frac{1}{2} \\tau_0 \\mu_{k}^2 +  \\alpha_0 \\log (\\beta_0) - \\Gamma(\\alpha_0) + (\\alpha_0-1) \\log (\\tau_{k}) - \\beta_0 \\tau_{k} \\right )\\\\\n",
    "&+\n",
    "\\sum_k (\\alpha-1) \\log \\pi_k - \\log B(\\alpha).\n",
    "\\end{align*}\n",
    "Note that $\\pi$ in $\\log (2\\pi)$ is the mathematical constant, not the parameter.\n",
    "\n",
    "Simple algebraic manipulation of the joint density gives the conditional distributions:\n",
    "\\begin{align*}\n",
    "p(\\pi|\\boldsymbol{z},\\alpha) &= \\text{Dirichlet}(\\alpha + \\sum_k I[z_n=k]) = \\text{Dirichlet}(\\alpha + N_k),\\\\\n",
    "p(\\tau_k|\\dots) &= \\text{Gamma}(\\alpha_0 + \\frac{1}{2}N_k, \\beta_0 + \\frac{1}{2} \\sum_n \\mathbb{I}(z_n=k) (x_{n} - \\mu_{k})^2),\\\\\n",
    "p(\\mu_{k}|\\dots) &= \\mathcal{N}(\\frac{\\tau_k \\sum_n I[z_n=k] \\mu_{k}}{t_0 + N_k \\tau}, t_0 + N_k\\tau),\\\\\n",
    "p(z_n=k|x_n,\\dots) &=  \\frac{e^{\\nu_{nk}}}{\\sum_j e^{\\nu_{nj}}}\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "\\nu_{nk} = \n",
    " \\log p(z_n=k|x_n,\\dots) \\propto \\log \\pi_k + \\left ( -\\frac{1}{2} \\log (2\\pi) + \\frac{1}{2} \\log \\tau_{k} - \\frac{1}{2} \\tau_{k} (x_{n} - \\mu_{k})^2 \\right ) \n",
    "\\end{align*}\n",
    "and $N_k$ is always the number of samples currently allocated for each cluster. In most cases these conditionals follow directly from the simple Gaussian model we covered in the lectures, but now we just need to consider only the samples that belong to this cluster. Note that each sample is always allocated to a single cluster determined by $z_n$, but the allocation naturally changes during the sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a snippet of code that samples data from this model and visualizes it with histograms. The left plot has separate histogram for the samples of each cluster, whereas the right one shows the full data histogram. In real use cases we naturally do not know the true $z_n$ so we could not draw the left plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(7879)\n",
    "\n",
    "# Define the model\n",
    "# This time we specify the model parameters directly rather than sampling from the prior,\n",
    "# to avoid e.g. two clusters with almost identical means.\n",
    "K = 3; N = 500\n",
    "pi_true = np.array([0.35, 0.2, 0.45])\n",
    "mu_true = np.array([0., -1.5, 1.5])\n",
    "tau_true = np.array([1., 5., 5.])\n",
    "\n",
    "# Sample data\n",
    "N = 500\n",
    "x = np.zeros((N,))\n",
    "z_true = stats.multinomial(1,pi_true).rvs(N) # one-hot-encoding for easier implementation\n",
    "colors = ['r','g','b']\n",
    "plt.subplot(1,2,1)\n",
    "for k in range(3):\n",
    "    x[z_true[:,k]==1] = stats.norm(mu_true[k], 1./np.sqrt(tau_true[k])).rvs(sum(z_true[:,k]))\n",
    "    plt.hist(x[z_true[:,k]==1], color=colors[k], bins=np.linspace(-4.,4.,50))\n",
    "plt.subplot(1,2,2)\n",
    "_ = plt.hist(x,bins=np.linspace(-4.,4.,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to derive and implement mean-field VI for this model, using the approximation\n",
    "\\begin{align*}\n",
    "q(\\boldsymbol z, \\boldsymbol{\\pi}, \\boldsymbol \\tau, \\boldsymbol \\mu) =  \\prod_n \\left [ q(z_n|\\boldsymbol \\xi_n) \\right ] q(\\boldsymbol{\\pi}|\\boldsymbol{\\eta}) \\prod_k \\left [ q(\\tau_k|a_k,b_k) q(\\mu_k|m_k, t_k) \\right ]\n",
    "\\end{align*}\n",
    "Here $q(z)$ are categorical, $q(\\pi)$ is Dirichlet, $q(\\tau)$ are gamma, and $q(\\mu)$ are normal. Note that we have $N$ copies of $q(z)$, one for each sample, and $K$ copies of $q(\\mu)$ and $q(\\tau)$, one for each cluster.\n",
    "\n",
    "1. **Derive the update rules for all approximation terms**. Start from the conditional distributions provided above and proceed in the same way as in the 2nd problem. Note that Murphy's book gives the correct updates for a more general mixture model in Section 10.2.6. You can follow those derivations but remember that our model is a bit simpler.\n",
    "\n",
    "2. **Implement the CAVI algorithm** and run it on the synthetic data generated above. The code snippet below already defines the approximations and provides one example of how to initialize the approximation terms.\n",
    "\n",
    "3. **Analyse the results**. How does the algorithm converge? What does the posterior approximation look like?\n",
    "\n",
    "HINTS:\n",
    "1. The lecture notebook already shows code for an individual normal distribution and the recorded lecture went through the derivations. You can build on those for the updates for $q(\\mu)$ and $q(\\tau)$.\n",
    "2. In Gibbs sampler the equations for $\\mu_k$ and $\\tau_k$ sum over the samples currently allocated into this cluster (that is, $z_n=k$). Now we have a distribution $q(z_n)$ instead, which can be interpreted as soft memberships. In practice, we always sum over all samples when updating the approximation term for any cluster, but the data points belong to each cluster only partially (and often $q(z_n==k) \\approx 0$ for many $k$).\n",
    "3. For updating $q(z)$ you again need the log-sum-exp trick to avoid underflows\n",
    "4. You will need some non-trivial expectations (in particular, expectation of log of a specific random variable). You should easily find an expression for that with google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (you can use these values or some others if you wish)\n",
    "nIter = 100\n",
    "K = 3\n",
    "alpha = 1.\n",
    "alpha0 = 0.5\n",
    "beta0 = 0.5\n",
    "tau0 = 0.5\n",
    "\n",
    "# Define and initialize approximation\n",
    "nu = np.full(K, N/K)              # Equal probability for all clusters in the beginning\n",
    "\n",
    "xi = stats.dirichlet(nu).rvs(N)   # Each row of xi needs to sum up to one\n",
    "\n",
    "a = np.full((K,), 1.)             # We don't care too much about initial precisions but avoid very large values\n",
    "b = np.full((K,), 5.)\n",
    "\n",
    "m = stats.norm(0., 1.).rvs((K,))  # Need to start with different ones to break symmetry\n",
    "t = np.full((K,), 5.)             # Some uncertainty in beginning\n",
    "\n",
    "# Now you can define the distributions with something like\n",
    "# q_tau = stats.gamma(a,b)\n",
    "# which enables querying most of the needed expectations with expressions like q_tau.mean()\n",
    "# Alternatively, you can just directly compute the expectations; mean(q_tau) = a/b etc.\n",
    "\n",
    "# Might be a good idea to store the parameters after each iteration, to monitor the convergence\n",
    "# However, you might not want to store the biggest matrices (xi).\n",
    "for iter in range(nIter):\n",
    "    # Update q(z)\n",
    "    for n in range(N):\n",
    "        xi[n,:] = ...\n",
    "    \n",
    "    # Update q(pi)\n",
    "    nu = ...\n",
    "        \n",
    "    # Update cluster parameters\n",
    "    for k in range(K):\n",
    "        # Precision\n",
    "        a[k] = ...\n",
    "        b[k] = ...\n",
    "        \n",
    "        # Mean\n",
    "        t[k] = ...\n",
    "        m[k] = ...\n",
    "        \n",
    "    history[iter,:] = np.concatenate([nu, m, t, a, b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
