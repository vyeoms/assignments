{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Gaussian processes\n",
    "\n",
    "## Problem 1: Moodle quiz\n",
    "\n",
    "Answer the quiz in Moodle.\n",
    "\n",
    "You can write below brief justifications for your answers for the yes/no questions, especially if you are unsure of your reasoning or have trouble understanding exactly what is being asked. I can then consider giving you points even if the automatic grading considered the answer wrong. You can also show the calculations you did to get the numerical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "When evaluating a new value $f(x^*)$ we know that the conditional is given by:\n",
    "\n",
    "$$p(f(x^*|\\mathbf{f})) = \\mathcal{N}(K_{x^*, \\mathbf{x}}K_{\\mathbf{x}, \\mathbf{x}}^{-1}\\mathbf{f}, K_{x^*, x^*} - K_{x^*, \\mathbf{x}}K_{\\mathbf{x}, \\mathbf{x}}^{-1}K_{\\mathbf{x}, x^*}$$\n",
    "\n",
    "where $K$ denotes the covariance matrix whose entries are defined by the kernel. In our case, we use an RBF kernel with $\\ell=1.5$, $\\lambda=2$, $x^*=1$, $x=2$ and $f(x=2)=3$. Note that when we use a symmetric kernel, like RBF, $K_{\\mathbf{x}, x^*} = K_{x^*, \\mathbf{x}}$ Using the formula above, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-covariance: 1.6014748058336161\n",
      "Covariance in data: 2.0\n",
      "Covariance in data to evaluate: 2.0\n",
      "Posterior mean: 2.402212208750424\n",
      "Posterior var: 0.7176392231400908\n"
     ]
    }
   ],
   "source": [
    "### Here be the justifications and calculations, if so desired\n",
    "\n",
    "def rbf(x, x_p, l, lam):\n",
    "    return lam*np.exp( -(x-x_p)**2 / (2*l**2) )\n",
    "\n",
    "cross_k = rbf(2, 1, 1.5, 2)\n",
    "x_k = rbf(2, 2, 1.5, 2)\n",
    "x_star_k = rbf(1, 1, 1.5, 2)\n",
    "\n",
    "print(f\"Cross-covariance: {cross_k}\")\n",
    "print(f\"Covariance in data: {x_k}\")\n",
    "print(f\"Covariance in data to evaluate: {x_star_k}\")\n",
    "\n",
    "post_mu = cross_k*3/x_k\n",
    "post_var = x_star_k - cross_k**2/x_k\n",
    "\n",
    "print(f\"Posterior mean: {post_mu}\")\n",
    "print(f\"Posterior var: {post_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "See section 18.4 of the book\n",
    "\n",
    "### Question 3\n",
    "\n",
    "The smaller the length-scale parameter, the more \"squiggly\" our approximated function. This means that then a single observation doesn't really give us much information about the rest of the function; the values of the function are very highly uncorrelated, so we don't really learn how observations affect our inferred function very well. On the other hand, by setting $\\ell \\to \\infty$, our covariance kernel returns pretty much a constant value for any pair of x's; the values are correlated over a long range. This is pretty much the basis for a linear model, for example in linear regression we are fitting a line using a training dataset, and we use this line to regress over unseen values.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "By setting the hyperparameter $\\sigma$ to 0, we are telling the model to assume that the observed $y$ was measured with no noise, so it will interpolate this datapoint perfectly.\n",
    "\n",
    "### Question 5\n",
    "\n",
    "The kernel we are using measures corellation as a function of distance, and it outputs very similar values for values that are close. They assume smoothness in the function, so if two points are close they will produce similar values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: GPs and marginal likelihood\n",
    "\n",
    "You are given a data set of $x$ and $y$ pairs as defined below. Your task is to learn a good **Gaussian Process regression** model for this.\n",
    "\n",
    "Use the RBF kernel\n",
    "$$\n",
    "k(x, x') = \\lambda e^{-\\frac{1}{2l^2} (x-x')^2}\n",
    "$$\n",
    "with normal likelihood\n",
    "$$\n",
    "p(y|f(x)) = \\mathcal{N}(f(x), \\sigma^2)\n",
    "$$\n",
    "where $\\sigma^2$ is the noise variance.\n",
    "\n",
    "The model has three hyperparameters, $l$, $\\lambda$ and $\\sigma$, and we will be inspecting their effect on the result.\n",
    "\n",
    "Write code that:\n",
    "1. Estimates the posterior distribution of the latent function $f(x)$ and evaluate it for the dense grid of test points x_test. You should do this so that it works for any choice of the hyperparameters. Note that much of this is already available in the lecture notebook.\n",
    "2. Optimizes the hyperparameters by maximizing the **marginal likelihood** $p(y|x,l,\\lambda,\\sigma)$ of the available data. You can use gradient descent in PyTorch or some other technique (e.g. gradient descent with analytic derivatives, or even some black-box optimization routine).\n",
    "3. Prints the optimal values and the resulting marginal likelihood.\n",
    "4. Plots the function. Make a plot that shows (a) The mean estimate of f() for the test points (the grid over x-axis), (b) the uncertainty of f(), and (c) the uncertainty of $y$ for the test points. You should also plot the training data in the same plot so that the plots are easier to interpret.\n",
    "\n",
    "Now use the code you wrote to study the effect of the hyperparameters, by showing the plots and marginal likelihoods for different choices of the hyperparameters. Start by showing the result with the optimal parameters and then proceed to show pairs of plots where you always make one parameter clearly smaller or larger. Label your plots clearly (with something like \"Large observation noise\" if using excessively large $\\sigma$ etc) and **explain** how each of the parameters influences the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 10 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     mean \u001b[38;5;241m=\u001b[39m k_pred_obs \u001b[38;5;241m@\u001b[39m inv_k_obs \u001b[38;5;241m@\u001b[39m y_obs\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mmultivariate_normal\u001b[38;5;241m.\u001b[39mMultivariateNormal(mean, cov_matrix)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_pred_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mposterior\u001b[0;34m(y_obs, k_obs, k_pred, k_pred_obs, var)\u001b[0m\n\u001b[1;32m     44\u001b[0m pred_len \u001b[38;5;241m=\u001b[39m k_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m var_mat \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39meye(obs_len)\n\u001b[0;32m---> 47\u001b[0m inv_chol_L \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve_triangular(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_obs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar_mat\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39meye(obs_len), upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m inv_k_obs \u001b[38;5;241m=\u001b[39m inv_chol_L\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m inv_chol_L\n\u001b[1;32m     50\u001b[0m cov_matrix \u001b[38;5;241m=\u001b[39m k_pred \u001b[38;5;241m-\u001b[39m k_pred_obs \u001b[38;5;241m@\u001b[39m inv_k_obs \u001b[38;5;241m@\u001b[39m k_pred_obs\u001b[38;5;241m.\u001b[39mt()\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 10 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "# THE DATA\n",
    "x = torch.tensor([ 0.0504, -1.9066, -0.5220, -1.7397,  0.5151,  1.7474,  0.2499, -1.6334,\n",
    "        -1.3278, -0.6832,  0.0248, -1.6533,  1.2656, -0.4293,  1.8606, -0.6216,\n",
    "         0.2714,  0.0983, -0.6295, -1.1215,  1.8418,  1.6155,  1.9598,  1.8651,\n",
    "         0.3304, -1.2396,  0.6092,  1.6508,  0.3112, -0.0261])\n",
    "y = torch.tensor([ 1.3866,  1.2162,  0.0608,  1.8141,  1.6938,  0.1754,  1.4721,  2.5264,\n",
    "         2.4079,  0.7910,  1.1510,  1.8651,  0.9920,  0.0425,  0.4240,  0.8889,\n",
    "         1.7757,  1.2457,  0.7961,  1.5359, -0.1856, -0.4273, -0.2238,  0.4841,\n",
    "         1.5591,  1.8768,  1.4800,  0.4432,  1.7465,  1.7640])\n",
    "N = 30\n",
    "\n",
    "# A REGULAR GRID OVER THE X-AXIS AS TEST POINTS\n",
    "x_test = torch.linspace(-2.5, 2.5, 100)\n",
    "\n",
    "# DEFINE THE KERNEL\n",
    "\n",
    "def rbf(x, x_p, l):\n",
    "    return np.exp( -0.5*(x-x_p)**2 / (l**2) )\n",
    "\n",
    "# Simple RBF kernel with l controlling the smoothness\n",
    "# and lam controlling the prior variance\n",
    "def covMatrix(a, b, l=1.0, lam=1.):\n",
    "    # You usually would not compute this with nested for loops, but\n",
    "    # I wanted to make it very clear what the kernel looks like\n",
    "    # k = torch.zeros((len(a), len(b)))\n",
    "    # for i in range(len(a)):\n",
    "    #     for j in range(len(b)):\n",
    "    #         k[i,j] = rbf(a[i], b[j], l=l)\n",
    "    k = -0.5 * torch.square( a.reshape((len(a), 1)) - b.reshape((1, len(b))) )\n",
    "    k /= l*l\n",
    "    k = torch.exp(k)\n",
    "    return lam*k\n",
    "\n",
    "def is_psd(mat):\n",
    "    return bool((mat == mat.T).all() and (torch.eig(mat)[0][:,0]>=0).all())\n",
    "\n",
    "cov_obs = covMatrix(x, x)\n",
    "cov_pred = covMatrix(x_test, x_test)\n",
    "cov_pred_obs = covMatrix(x_test, x)\n",
    "\n",
    "# Compute the posterior with analytic expressions\n",
    "def posterior(y_obs, k_obs, k_pred, k_pred_obs, var = 0.):\n",
    "    obs_len = k_obs.shape[0]\n",
    "    pred_len = k_pred.shape[0]\n",
    "    var_mat = var*torch.eye(obs_len)\n",
    "    \n",
    "    inv_chol_L = torch.linalg.solve_triangular(torch.cholesky(k_obs + var_mat), torch.eye(obs_len), upper=False)\n",
    "    inv_k_obs = inv_chol_L.t() @ inv_chol_L\n",
    "    \n",
    "    cov_matrix = k_pred - k_pred_obs @ inv_k_obs @ k_pred_obs.t()\n",
    "    mean = k_pred_obs @ inv_k_obs @ y_obs\n",
    "    return torch.distributions.multivariate_normal.MultivariateNormal(mean, cov_matrix)\n",
    "\n",
    "posterior(y, cov_obs, cov_pred, cov_pred_obs)\n",
    "\n",
    "# Marginal likelihood\n",
    "# def marginal_likelihood(...):\n",
    "    # ...\n",
    "    \n",
    "# Find optimal hyperparameters\n",
    "# Note that you might want to optimize over log of these if using gradient descent\n",
    "# l, lam, sigma = maximize_marginal_likelihood(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "# Plot the latent function, its uncertainty and the uncertainty of the predictions y\n",
    "# Do this for the optimal hyperparameters and also for bad configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: GPyTorch\n",
    "\n",
    "In this exercise we practice using GPs with proper tools, rather than implementing inference from scratch.\n",
    "\n",
    "We are given data that are pairs of univariate inputs $x$ and outputs $y$ that are constrained to be between zero and one. For instance, the input could be a dose of a drug (on some scale) and the output could be the probability that the drug has a positive effect in some population. We want to learn how $y$ depends on $x$ and realized we need a flexible model for this.\n",
    "\n",
    "1. Start by reading the regression tutorial in https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html. \n",
    "2. Implement the model for the data provided below (using some kernel of your choice), using **exact inference** and **normal likelihood** for the observations. Do this so that you optimize over the kernel hyperparameters and report the results with an appropriate plot.\n",
    "3. Change your code to use **variational approximation** but still use the normal likelihood. Check whether you get the same (or highly similar) result as with the exact inference. The documentation has clear pointers on how to do that, for instance in https://docs.gpytorch.ai/en/latest/examples/04_Variational_and_Approximate_GPs/Non_Gaussian_Likelihoods.html. Note that you now need to set both the model parameters and the likelihood parameters as trainable, since the likelihood parameters are no longer part of the model itself.\n",
    "4. Continue with the variational approximation but now change the likelihood to **Beta distribution** so that you assume $y$ to be generated from a Beta distribution with the parameters\n",
    "$$\n",
    "    p(y|f) = \\text{Beta}(s(f)b,(1-s(f))b)\n",
    "$$\n",
    "where $b$ is a scale parameter, $s()$ is the sigmoid function and $f$ is a latent function following a GP prior. Note that you do not need to implement this from scratch but the library has a direct implementation for the likelihood (see https://docs.gpytorch.ai/en/latest/likelihoods.html). Compare the result of modelling the data in this way with the previous results where you assumed normal likelihood for $y$ directly. What changed?\n",
    "\n",
    "HINTS: \n",
    "- Whenever you plot something, remember to show also the uncertainty appropriately.\n",
    "- Remember to write brief written explanation for what you see. How are the results different?\n",
    "- The hyperparameter optimizer may sometimes converge to a local optimum where the data is modelled purely as noise. You can try to initialize the optimization routines better to avoid this, or you can try setting priors for these parameters. See https://docs.gpytorch.ai/en/latest/examples/00_Basic_Usage/Hyperparameters.html for examples on how to do this.\n",
    "- For the Beta likelihood the parameter 's' is simply one extra hyperparameter. You can either leave it at some default value or give it a some prior and let the optimizer fit it. You should not use values that are very small since the latent function would then have very little effect -- for very small $s$ we just have uniform distribution over $y$ irrespective of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVED DATA\n",
    "train_x = torch.tensor([0.5126, 0.0233, 0.3695, 0.0651, 0.6288, 0.9368, 0.5625, 0.0917, 0.1680,\n",
    "        0.3292, 0.5062, 0.0867, 0.8164, 0.3927, 0.9651, 0.3446, 0.5678, 0.5246,\n",
    "        0.3426, 0.2196, 0.9605, 0.9039, 0.9900, 0.9663, 0.5826, 0.1901, 0.6523,\n",
    "        0.9127, 0.5778, 0.4935, 0.5012, 0.6410, 0.4883, 0.9442, 0.2780, 0.2305,\n",
    "        0.4002, 0.2337, 0.5585, 0.4633, 0.0507, 0.6100, 0.3050, 0.7710, 0.6987,\n",
    "        0.2694, 0.3509, 0.1588, 0.9979, 0.0690, 0.7652, 0.2706, 0.2667, 0.5845,\n",
    "        0.6084, 0.4865, 0.5080, 0.5669, 0.2378, 0.0224, 0.0600, 0.8233, 0.3634,\n",
    "        0.9148, 0.2383, 0.9255, 0.3103, 0.7075, 0.3455, 0.6943, 0.2243, 0.9852,\n",
    "        0.6694, 0.8797, 0.3302, 0.5267, 0.7142, 0.9162, 0.3036, 0.5128])\n",
    "train_y = torch.tensor([0.9473, 0.9803, 0.6330, 0.9250, 0.3311, 0.9499, 0.8616, 0.9144, 0.3090,\n",
    "        0.3811, 0.8733, 0.9124, 0.3459, 0.4235, 0.9813, 0.2945, 0.8940, 0.9254,\n",
    "        0.3932, 0.2734, 0.9696, 0.9025, 0.9856, 0.9716, 0.8138, 0.2813, 0.2329,\n",
    "        0.8965, 0.8364, 0.9311, 0.9089, 0.5626, 0.9586, 0.9882, 0.1046, 0.2527,\n",
    "        0.7312, 0.2641, 0.8285, 0.8893, 0.9742, 0.4065, 0.2098, 0.1219, 0.1360,\n",
    "        0.2983, 0.4822, 0.3533, 0.9649, 0.9614, 0.1990, 0.0884, 0.0490, 0.7147,\n",
    "        0.6823, 0.9583, 0.8776, 0.9316, 0.0573, 0.9834, 0.9565, 0.3792, 0.6348,\n",
    "        0.9224, 0.1689, 0.9594, 0.1974, 0.3019, 0.1805, 0.1727, 0.2434, 0.9776,\n",
    "        0.3741, 0.8168, 0.3170, 0.9061, 0.1242, 0.8877, 0.1660, 0.9657])\n",
    "N = 80\n",
    "\n",
    "# SET OF TEST POINTS (AGAIN A GRID OVER THE INPUT AXIS)\n",
    "test_x = torch.linspace(0.0, 1.0, 100)\n",
    "\n",
    "\n",
    "# FOR EACH OF THE THREE MODELS YOU NEED ROUGHLY THE FOLLOWING\n",
    "# 1. Specify kernel and likelihood, as well as the function that provides the marginal likelihood\n",
    "# 2. Define the optimization problem: Tell what parameters are to be learned etc. Initialize with reasonable values if having trouble with convergence.\n",
    "# 3. Maximize marginal likelihood\n",
    "# 4. Switch to evaluation mode and make predictions for the test samples\n",
    "# 5. Plot results, paying attention also to uncertainty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
