{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MCMC for Poisson-gamma model\n",
    "\n",
    "Consider a model \n",
    "\\begin{align*}\n",
    "x_n &\\sim \\text{Poisson}(uv),\\\\\n",
    "u &\\sim \\text{Gamma}(6, 1),\\\\\n",
    "v &\\sim \\text{Gamma}(3, 3).\n",
    "\\end{align*}\n",
    "where individual observations $x_n$ are conditionally independent of each other. Use MCMC to infer $p(u,v|\\mathbf{x})$ for the set of observations $\\mathbf{x}=[5, 3, 9, 13, 5, 3, 5, 1, 2, 7, 6, 5, 6, 7, 4]$\n",
    "\n",
    "Solve the following tasks related to this model:\n",
    "\n",
    "1. **Write down the joint log-probability** $\\log p(\\mathbf{x},u,v)$\n",
    "2. **Derive a Gibbs sampler** for this model by providing the conditional distributions $p(u|v,\\mathbf{x})$ and $p(v|u,\\mathbf{x})$\n",
    "3. **Implement the Gibbs sampler** you just derived\n",
    "4. **Run both the Gibbs sampler and Metropolis sampler** (using the implementation provided in the lecture notebook, or some other) and inspect the results. Plot the marginal distributions and trace plots of $u$ and $v$ as well as a cross-plot of the posterior samples. Finally, report the mean and variance of $u$ and $v$.\n",
    "5. **Compare Gibbs and Metropolis** in this task. Do you find the same posterior with both? Are there any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-c3ab3943ca0b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-c3ab3943ca0b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    log_density(data, param):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = np.array([5, 3, 9, 13, 5, 3, 5, 1, 2, 7, 6, 5, 6, 7, 4])\n",
    "\n",
    "log_density(data, param):\n",
    "    ...\n",
    "    return value\n",
    "\n",
    "# Before writing the Gibbs sampler you need pen-and-paper derivation of the conditionals\n",
    "\n",
    "gibbs_sampler(data, initial_param, iterations):\n",
    "    ...\n",
    "    return samples\n",
    "\n",
    "gibbs_samples = gibbs_sampler(x, ...)\n",
    "\n",
    "metropolis_samples = metropolis_sampler(x, ...)\n",
    "\n",
    "# Analyse results here\n",
    "# (Can re-use code from the lecture notebook if you think it is useful, but think what you are plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Latent Dirichlet Allocation\n",
    "    \n",
    "Start by getting familiar with the LDA model by reading the material provided in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below generates data from the model. Make sure you understand the dimensionalities and meanings of all terms, so that you can correctly manipulate them in the samplers. We will later use the data to try out the sammplers.\n",
    "\n",
    "The data is intentionally very small so that you do not need to worry about code efficiency. If your code is fast, feel free to try it with larger $N$, $V$ and/or $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(789789)\n",
    "\n",
    "# Define the data dimensionalities\n",
    "N = 40   # Number of documents\n",
    "V = 100  # Vocabulary size\n",
    "L = 50   # Length of each document (here identical for simplicity)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha_true = 0.1\n",
    "gamma_true = 0.1\n",
    "K_true = 10\n",
    "\n",
    "# Sample data from the model\n",
    "def create_data(N, V, K, L):\n",
    "    # Topic distributions of individual documents\n",
    "    pi = stats.dirichlet(alpha_true*np.ones(K)).rvs(N)\n",
    "    # Word distributions of individual topics\n",
    "    b = stats.dirichlet(gamma_true*np.ones(V)).rvs(K_true)\n",
    "    q = np.zeros((N, L), 'int')\n",
    "    y = np.zeros((N, L), 'int')\n",
    "    for n in range(N):\n",
    "        for w in range(L):\n",
    "            # Sample topic responsible for generating the word\n",
    "            topic = np.random.choice(K_true, 1, p=pi[n,:])[0]\n",
    "            q[n, w] = topic\n",
    "            # Sample the word itself\n",
    "            word = np.random.choice(V, 1, p=b[topic,:])[0]\n",
    "            y[n, w] = word\n",
    "    return y, pi, b, q\n",
    "\n",
    "y, pi_true, b_true, q_true = create_data(N, V, K_true, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Direct Gibbs\n",
    "\n",
    "Implement a direct Gibbs sampler, following Equations (27.30) - (27.32) that give the conditional distributions needed for sampling $\\pi$ and $q_{il}$ (the responsible topic for each word) and $b_k$ (the topics). Note that here the book suddenly uses $x_{il}$ instead of $y_{il}$ -- this is a mistake in the book. \n",
    "\n",
    "For initialization (of both samplers) you can sample $\\pi$ and $b$ from their priors and then use Equation (27.30) to get initial $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will probably want to write a separate function for each conditional distribution\n",
    "\n",
    "# You will also need a function that computes the $c_{ivk}$ based on $q_{il}$.\n",
    "# Pay attention to the indexing; in $c_{ivk}$ the index $v$ is the word identity,\n",
    "# which is *not* the same thing as $l$ that gives the running index of each word. \n",
    "\n",
    "# ...and then a simple loop for the main algorithm that uses the functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Collapsed Gibbs\n",
    "\n",
    "Implement a collapsed Gibbs sampler. For this you will need the Equation (27.37) that provides the probabilities for sampling $q_{il}$ for one word conditional on the allocations for all others, integrating over $\\pi$ and $b$. Note that you can still use equations (27.31) and (27.32) to obtain $\\pi$ and $b$ for interpretation after running the algorithm.\n",
    "\n",
    "Collapsed Gibbs is easiest to implement by storing and updating both $c_{ivk}$ and $q_{il}$. To allocate a new sample you first subtract one from $c_{ivk}$ based on current value $q_{il}$ before computing all the sums for equation (27.37). After sampling a new value you add one to the right element and update the corresponding element in $q_{il}$. This is inefficient compared to explicitly maintaining also the different marginal sums, but does not matter for our small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here the collapsed sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Evaluate the samplers\n",
    "\n",
    "Run both samplers on the toy data and study the results. You can use the same $\\alpha$, $\\gamma$ and $K$ that were used when creating the data.\n",
    "\n",
    "1. Plot the \"document-to-topic\" and \"topic-to-word\" distributions for the last posterior sample and compare them against the true values used when generating the data. Note that LDA is not permutation-invariant -- there is no reason to expect the topics would appear in the same order in your solution as in the generation process. To better compare them, you can sort the topics according to their overall popularity.\n",
    "\n",
    "2. Investigate a single document to understand how the model works. Check that the document is using topics that have high probability of generating the words that appear in the document. Try to be 'properly Bayesian' here, in the sense that you do not look only at the last posterior sample but attempt to characterise also the variation in some way.\n",
    "\n",
    "3. Would you prefer the direct or collapsed sampler? Was one of them easier to implement? Do you think one works better?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
