{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244f4576",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec0fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy as scp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3c026",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "## Part A\n",
    "\n",
    "The confusion matrix for the exercise looks like this:\n",
    "\n",
    "| Proportion  | H0 retained | H0 rejected   |\n",
    "| :---:       |    :----:   |          :---:|\n",
    "| H0 true     | TN          | FP (Type I)   |\n",
    "| H1 true     | FN (Type II)|       TP      |\n",
    "\n",
    "With the definitions given in the lectures for $\\alpha$ and $\\beta$, we have that each proportion may be determined by:\n",
    "\n",
    "$$\n",
    "TN = n_0 (1-\\alpha)\\\\\n",
    "FP = \\alpha n_0\\\\\n",
    "FN = n_1(1-\\beta)\\\\\n",
    "TP = \\beta n_1\n",
    "$$\n",
    "\n",
    "Where $n_0$ is the number of hypotheses that are true ($H_0$) and $n_1$ is the number of hypotheses that are false ($H_1$). Then the probability that $H_1$ is true given that $H_0$ is rejected is given by the proportion:\n",
    "\n",
    "$$ \\frac{TP}{TP + FP} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5247eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculations with alpha=0.05, beta=0.8, n0=100, n1=100\n",
      "P(H1|H0 rejected): 0.9411764705882353\n",
      "Calculations with alpha=0.05, beta=0.5, n0=100, n1=100\n",
      "P(H1|H0 rejected): 0.9090909090909091\n",
      "Calculations with alpha=0.05, beta=0.2, n0=100, n1=100\n",
      "P(H1|H0 rejected): 0.8\n",
      "Calculations with alpha=0.05, beta=0.8, n0=900, n1=100\n",
      "P(H1|H0 rejected): 0.64\n",
      "Calculations with alpha=0.05, beta=0.5, n0=900, n1=100\n",
      "P(H1|H0 rejected): 0.5263157894736842\n",
      "Calculations with alpha=0.05, beta=0.2, n0=900, n1=100\n",
      "P(H1|H0 rejected): 0.3076923076923077\n",
      "Calculations with alpha=0.05, beta=0.8, n0=99900, n1=100\n",
      "P(H1|H0 rejected): 0.015763546798029555\n"
     ]
    }
   ],
   "source": [
    "def calculateProps(alpha, beta, n0, n1):\n",
    "    print(f'Calculations with alpha={alpha}, beta={beta}, n0={n0}, n1={n1}')\n",
    "#     print(f'TN = {n0*(1-alpha)}')\n",
    "#     print(f'FP = {alpha*n0}')\n",
    "#     print(f'FN = {n1*(1-beta)}')\n",
    "#     print(f'TP = {beta*n1}')\n",
    "    TP = beta*n1\n",
    "    FP = alpha*n0\n",
    "    print(f'P(H1|H0 rejected): {TP/(TP+FP)}')\n",
    "\n",
    "props = [(100, 100), (900, 100)]\n",
    "betas = [0.8, 0.5, 0.2]\n",
    "\n",
    "for prop in props:\n",
    "    for beta in betas:\n",
    "        calculateProps(0.05, beta, prop[0], prop[1])\n",
    "calculateProps(0.05, 0.8, 99900, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63dba5a",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "The probabilities where the populations of true $H_0$ and $H_1$ are similar are high, which is what we would like to see when we reject the null hypothesis. These probabilities also become lower when the power of the test decreases. The definition of power is \"the probability that $H_0$ is rejected when $H_0$ is false\", so this result is also expected.\n",
    "\n",
    "However, when we skew the proportion of hypothesis results toward $H_0$ we can see that the probabilities drop drastically. This shows one of the weaknesses of NHST, where if we have underrepresented groups of data we will not get accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e7e89",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d09a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9446199162650353\n",
      "7.979997860951826e-05\n"
     ]
    }
   ],
   "source": [
    "def waldStat(x, y):\n",
    "    return ( np.mean(x) - np.mean(y) ) / np.sqrt( np.var(x)/x.size + np.var(y)/y.size )\n",
    "\n",
    "def waldP(x_lim):\n",
    "    return 2*(1 - scp.stats.norm.cdf(x_lim))\n",
    "\n",
    "twain = np.array([.225, .262, .217, .240, .230, .229, .235, .217])\n",
    "snodgrass = np.array( [.209, .205, .196, .210, .202, .207, .224, .223, .220, .201] )\n",
    "print( waldStat(twain, snodgrass) )\n",
    "print(waldP(3.945))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63dbd",
   "metadata": {},
   "source": [
    "The Wald statistic for this test is approximately 3.945. The rejection region is defined by $R = \\{|Z| > 3.945\\}$, where Z corresponds to the Wald statistic. Thus, the p-value is given by:\n",
    "\n",
    "$$ P\\left( |Z| > 3.945 \\right) = 2\\cdot P\\left(Z > 3.945 \\right) = 2 \\cdot \\left( 1 - P(Z \\leq 3.945) \\right) = 2\\cdot (1 - \\Phi(3.945) ) $$\n",
    "\n",
    "because the distribution of Z is approximately normal with mean 0 and variance 1 for a Wald test. For a normal distribution, this probability is given by:\n",
    "\n",
    "$$ 2\\cdot \\left(1 - \\int_{-\\infty}^{3.945} \\frac{1}{\\sqrt{2\\pi}} e^{ -\\frac{x^2}{2} } dx \\right) = 2\\cdot(1-\\Phi(3.945))  \\approx 0.0000798 $$\n",
    "\n",
    "The confidence interval is given by $\\overline{X} - \\overline{Y} \\pm 2\\sqrt{ \\frac{s_1^2}{8} + \\frac{s_2^2}{10} } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff2e0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mean - Y_mean, with X being Twain and Snodgrass Y: 0.022175\n",
      "Sample variance metric: 0.011243161810629607\n",
      "Confidence interval: [0.010931838189370394, 0.03341816181062961]\n"
     ]
    }
   ],
   "source": [
    "mean_diff = np.mean(twain) - np.mean(snodgrass)\n",
    "var_metric = 2*np.sqrt( np.var(twain)/twain.size + np.var(snodgrass)/snodgrass.size )\n",
    "conf_interval = [ mean_diff - var_metric, mean_diff + var_metric]\n",
    "\n",
    "print(f'X_mean - Y_mean, with X being Twain and Snodgrass Y: {mean_diff}')\n",
    "\n",
    "print(f'Sample variance metric: {var_metric}')\n",
    "\n",
    "print(f'Confidence interval: {conf_interval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb8263",
   "metadata": {},
   "source": [
    "The p-value indicates that the result is statistically significant, but the confidence interval $(0.1, 0.3)$ is small. Since the confidence interval is so small, you could argue that the authors aren't different people\n",
    "\n",
    "## Part B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02624b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008999100089991002\n"
     ]
    }
   ],
   "source": [
    "B = 10000\n",
    "p_val = 1/(B+1)\n",
    "permute_space = np.concatenate( (twain, snodgrass) )\n",
    "\n",
    "npr.seed(0)\n",
    "for i in range(B):\n",
    "    permute = npr.permutation(permute_space)\n",
    "    p_val += ( waldStat( permute[:8], permute[8:] ) > 3.945 ) / (B+1)\n",
    "\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd124e8b",
   "metadata": {},
   "source": [
    "The p-value using permutation testing is almost 10 times bigger than the one calculated in the previous section, so the evidence against the null is weaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577356fc",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Part A\n",
    "\n",
    "The distributions for the drug trials are modeled as Bernoulli, with a success rate $p$. The plug-in estimator for the expected value is $\\hat{p} = \\frac{successes}{n}$, with n trials. The variance is given by $\\hat{p}(1-\\hat{p})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3f1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug:Chlorpromazine, stat=-2.7643637780027186, odds ratio=1.4933333333333334,p-value=0.005703391555257786\n",
      "Drug:Dimenhydrinate, stat=0.6429872617820929, odds ratio=0.8873949579831933,p-value=0.5202323654229468\n",
      "Drug:Pentobarbital100mg, stat=-0.4864275028950293, odds ratio=1.091684434968017,p-value=0.6266640947313891\n",
      "Drug:Pentobarbital150mg, stat=-1.6466051393070562, odds ratio=1.2907563025210085,p-value=0.09963923341820236\n"
     ]
    }
   ],
   "source": [
    "def statisticsEx3(n, nausea):\n",
    "    drug_no_nausea = n - nausea\n",
    "    placebo_p = 35 / 80\n",
    "    drug_p = drug_no_nausea/n\n",
    "    \n",
    "    wald = (placebo_p - drug_p) / np.sqrt( placebo_p*(1-placebo_p)/80 + drug_p*(1-drug_p)/n )\n",
    "    odds_ratio = drug_p / placebo_p\n",
    "    p_val = waldP( np.abs(wald) )\n",
    "    return wald, odds_ratio, p_val\n",
    "\n",
    "drugs_dict = {\n",
    "    'Chlorpromazine': statisticsEx3(75, 26),\n",
    "    'Dimenhydrinate': statisticsEx3(85, 52),\n",
    "    'Pentobarbital100mg' : statisticsEx3(67, 35),\n",
    "    'Pentobarbital150mg' : statisticsEx3(85, 37),\n",
    "}\n",
    "\n",
    "for key in drugs_dict:\n",
    "    print(f'Drug:{key}, stat={drugs_dict[key][0]}, odds ratio={drugs_dict[key][1]},p-value={drugs_dict[key][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb7590",
   "metadata": {},
   "source": [
    "With a Wald statistic test, we see that for a 5 per cent level the only drug that has a statistically significant value is Chlorpromazine, with a p-value of ~0,006 and an odds ratio of ~1,49. Dimenhydrinate is the only drug that presents an odds ratio smaller than 1, so we can infer that it's the only drug that performed worse than a placebo during this trial.\n",
    "\n",
    "## Part B\n",
    "\n",
    "With Bonferroni, we only divide the p-value cutoff by the number of tests realized. In this case, we had 4 tests, so $\\frac{\\alpha}{4} = \\frac{0.05}{4} = 0.0125$. With this new cutoff, the p-value of Chlorpromazine is still statistically significant. The Bonferroni adjustment is also stricter than FDR, so this p-value must also be statistically significant under FDR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b20441",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "## Part A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368fc156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution with multinomial: [0.1  0.1  0.1  0.15 0.1  0.15 0.25 0.   0.05 0.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd725de4370>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 20\n",
    "seq = np.array([6,3,7,4,6,9,2,6,7,4,3,7,7,2,5,4,1,7,5,1])\n",
    "\n",
    "p = np.zeros(10)\n",
    "\n",
    "for j in range(10):\n",
    "    p[j] = np.sum(seq == j+1) / n\n",
    "\n",
    "print(f'Distribution with multinomial: {p}')\n",
    "plt.plot(p, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e156e",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa947bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution with Dirichlet using alpha_i = 0.1: [2.1 2.1 2.1 3.1 2.1 3.1 5.1 0.1 1.1 0.1]\n",
      "Mean estimates using alpha_i=0.1: [0.1        0.1        0.1        0.14761905 0.1        0.14761905\n",
      " 0.24285714 0.0047619  0.05238095 0.0047619 ]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_estimates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution with Dirichlet using alpha_i = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_prior\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean estimates using alpha_i=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_prior\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_alpha \u001b[38;5;241m/\u001b[39m alpha_0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmean_estimates\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_estimates' is not defined"
     ]
    }
   ],
   "source": [
    "alpha_priors = [0.1, 1, 10]\n",
    "\n",
    "for alpha_prior in alpha_priors:\n",
    "    alphas = np.full(10, alpha_prior)\n",
    "    estimated_alpha = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        estimated_alpha[i] = alphas[i] + np.sum( seq == i+1 )\n",
    "    alpha_0 = np.sum(estimated_alpha)\n",
    "    print(f'Distribution with Dirichlet using alpha_i = {alpha_prior}: {estimated_alpha}')\n",
    "    print(f'Mean estimates using alpha_i={alpha_prior}: {estimated_alpha / alpha_0}\\n')\n",
    "    plt.plot(mean_estimates, 'o')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
