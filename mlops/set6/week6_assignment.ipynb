{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6 Assignments\n",
    "You'll gain some hands-on experience with monitoring your model using Prometheus and Evidently in this week's assignments.\n",
    "\n",
    "**Guidelines of submitting assignments**:\n",
    "- In the first assignment, you'll need to write some configurations in a YAML file, so please include the YAML file in your submission.  \n",
    "- For every remaining assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to capture screenshots in order to earn points. Please put all your screenshots into a single PDF file. For each screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are up-to-date and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your screenshots in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# mlserver and simplejson are used to encode the requests to be sent to the inference service.\n",
    "# We downgrade mlserver (1.3.5 was used in week4 assignments) because 1.3.5 is not compatible with the evidently version we use.\n",
    "%pip install mlserver==1.2.1 simplejson~=3.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import requests\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlserver.codecs import PandasCodec\n",
    "from utils import send_requests, init_evidently_project\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple, Dict, Any, Callable, List\n",
    "from unittest.mock import create_autospec\n",
    "import webbrowser\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset\n",
    "from evidently.tests import TestValueMAE\n",
    "from evidently.ui.workspace import Workspace\n",
    "from evidently.ui.remote import RemoteWorkspace\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = Path.cwd()\n",
    "\n",
    "# mlflow configuration\n",
    "MLFLOW_S3_ENDPOINT_URL = \"http://mlflow-minio.local\"\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server.local\"\n",
    "AWS_ACCESS_KEY_ID = \"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY = \"minioadmin\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "# Directory where model inputs(features received on production) and outputs(predictions) are saved\n",
    "INPUTS_OUTPUTS_LOCAL_DIR_NAME = \"inputs_outputs\"\n",
    "\n",
    "# Directory where ground truth is saved\n",
    "GROUND_TRUTH_LOCAL_DIR_NAME = \"ground_truth\"\n",
    "\n",
    "# Directory where the feature-engineered data (data preprocessed by the etl function from the second week) is saved\n",
    "FEATURE_STORE_DIR_NAME = \"feature_store\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation before starting the assignments\n",
    "Let's begin by training a model and deploying it to KServe. This model is trained using the house price data that we used in Week2 assignments. The task of the model is to predict the price of a house given the house's information such as building year and living area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_name = \"week6-lgbm-house-price\"\n",
    "registered_model_name = \"Week6LgbmHousePrice\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(mlflow_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw training data can be found from \"raw_data/reference/train\" directory. The training data has been feature-engineered using the `etl` function from the second week and are split into a feature file (0_0_X.parquet) and a target file (0_0_y.csv) in the \"feature_store\" directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "train_x = pd.read_parquet(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_X.parquet\")\n",
    "train_y = pd.read_csv(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_y.csv\")\n",
    "\n",
    "# model hyperparameters (hyperparameter optimization was performed)\n",
    "params = {\n",
    "    \"colsample_bytree\" : 0.7,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    \"max_depth\" : 50, \n",
    "    \"min_child_samples\" : 5,\n",
    "    \"min_split_gain\" : 20.0,\n",
    "    \"n_estimators\" : 1000,\n",
    "    \"num_leaves\" : 100,\n",
    "    \"reg_lambda\" : 50.0,\n",
    "    \"subsample\" : 0.1,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Train a model and upload it to MLflow\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow_run_id = run.info.run_id\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    for key, val in params.items():\n",
    "        mlflow.log_param(key, val)\n",
    "\n",
    "    model_name = \"lgbm-house\"\n",
    "    mlflow.lightgbm.log_model(\n",
    "        lgb_model=model,\n",
    "        artifact_path=model_name,\n",
    "        registered_model_name=registered_model_name\n",
    "    )\n",
    "    model_s3_uri = mlflow.get_artifact_uri(artifact_path=model_name)\n",
    "    print(\"The S3 URI of the registered model:\", model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final output line, you should see the model version created from the MLflow Run:\n",
    "```text\n",
    "Created version <your-model-version> of model 'Week6LgbmHousePrice'\n",
    "```\n",
    "Let's assign the model version to the `model_version` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"13\" # Replace this with your own model version, notice that the model_version is a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also see the S3 URI of your uploaded model printed . Let't then deploy the model to KServe. Before running the next cell, replace the `storageUri` in [manifests/house-price.yaml](./manifests/house-price.yaml) with your own S3 URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy an inference service named \"house-price\"\n",
    "!kubectl apply -f manifests/house-price.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/house-price created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the \"house-price\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc house-price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "house-price   http://house-price.kserve-inference.example.com   True           100                              house-price-predictor-default-00001   55s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then make sure there's a running pod for the \"house-price\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=house-price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "house-price-predictor-default-00001-deployment-748bc8bc67-r7p46   2/2     Running   0          68s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow can be used to manage the lifecycle of models. We can transition a registered model to one of the stages: Staging, Production or Archived. Since we just deployed a model, let's change its stage to \"Production\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Transition the stage of other model versions whose stage is \"Production\" to None\n",
    "# This way we ensure there is only one model on the \"Production\" stage at any given time\n",
    "for mv in client.search_model_versions(f\"name='{registered_model_name}'\"):\n",
    "    if mv.current_stage != \"Production\":\n",
    "        continue\n",
    "    client.transition_model_version_stage(\n",
    "        name=registered_model_name, version=mv.version, stage=\"None\"\n",
    "    )\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=registered_model_name, version=model_version, stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to [http://mlflow-server.local](http://mlflow-server.local), you should also see the model stage is \"Production\" on the MLflow UI. Make sure the model version is the same one as your `model_version` variable. \n",
    "\n",
    "<img src=\"./images/mlflow-prod-model.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Monitoring 4xx responses of your inference service (2 points)\n",
    "In this assignment, your task is to add a Prometheus alerting rule so that Prometheus will trigger an alert when your inference service gives too many client error responses (i.e., responses whose HTTP status code is 4xx). \n",
    "\n",
    "(If HTTP status codes are new to you, more information can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).)\n",
    "\n",
    "You need to add an alerting rule to [manifests/prometheus-config-patch.yaml](./manifests/prometheus-config-patch.yaml) that **immediately** triggers an alert when your \"house-price\" inference service running in the \"kserve-inference\" namespace gives more than **ten** 4xx error responses in the **past one minute**. \n",
    "\n",
    "Please add your alerting rule between the commands \"### START ALERTING RULE\" and \"### END ALERTING RULE\". **Please include the file \"prometheus-config-patch.yaml\" in your submission.**\n",
    "\n",
    "Hints:\n",
    "- An essential part in configuring an alerting rule is to decide which PromQL query to use. It may be easier if you first test your query in the Prometheus UI [http://prometheus-server.local](http://prometheus-server.local) before writing your alerting rule to the configuration file. You can use the following code cell to send some invalid requests to your inference service, and then go to the Prometheus UI to test if your query can retrieve a reasonable value. \n",
    "- You may find the `revision_app_request_count` metric useful (introduced in the tutorial). You can then use the labels `namespace_name`, `isvc_name`, `response_code_class` to only include the responses you want to monitor. \n",
    "- You may also find the PromQL function [increase()](https://prometheus.io/docs/prometheus/latest/querying/functions/#increase) useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pretend your downstream application is somehow broken and starts sending invalid requests to the inference service. \n",
    "# Inside each request, the input data is in a wrong format so the inference service will return responses with\n",
    "# 422 (unprocessable entity) HTTP status code\n",
    "send_requests(model_name=\"house-price\", input=[None for _ in range(16)], count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Prometheus configuration by patching your alerting rule to the ConfigMap consumed by the Prometheus pod\n",
    "!kubectl -n monitoring patch configmap prometheus-server-conf --patch-file manifests/prometheus-config-patch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the old Prometheus pod so a new one that consumes the updated ConfigMap will be recreated automatically\n",
    "!kubectl -n monitoring delete pod -l app=prometheus-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the new Prometheus pod is ready\n",
    "!kubectl -n monitoring get pod -l app=prometheus-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                     READY   STATUS    RESTARTS   AGE\n",
    "prometheus-deployment-7b898cb9d8-g8wd2   1/1     Running   0          6s\n",
    "```\n",
    "The \"AGE\" should be relatively small since this pod should be created after you deleted the old one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send invalid requests again and you should see an alert of too many 4xx responses has been triggered\n",
    "send_requests(model_name=\"house-price\", input=[None for _ in range(16)], count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots for Assignment 1\n",
    "Now you can go to [http://prometheus-server.local/alerts](http://prometheus-server.local/alerts) and see if the alert of too many 4xx responses is triggered.\n",
    "\n",
    "**Please make a screenshot of the triggered alert and put it in your PDF file.** Please extend the alert field so your PromQL query is shown in the screenshot. Note the example is about another alert, it's just used to show you what should be included in the screenshot. \n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/alert-example.png\" width=1000/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Model-level monitoring with Evidently (4 points)\n",
    "In this assignment, you'll use Evidently to monitor your model that predicts houce price. The model will be monitored on a quarter basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Prepare reference data\n",
    "Let's start by preparing the reference data. Please complete the `prep_ref` function that performs the following tasks:\n",
    "1. Concatenating a given feature DataFrame (denoted as `x` in the function) and a target DataFrame (denoted as `y` in the function) into a single DataFrame. The concatenation should be performed horizontally (side by side). You can assume that the feature and target DataFrames are aligned (for $i^{th}$ row of the feature DataFrame, the corresponding target is also at $i^{th}$ row of the target DataFrame).\n",
    "1. Adding a new column to the concatenated DataFrame named \"prediction\". The values in this column are generated by calling the predict method (`model.predict()`) of the provided model on `x`. Simply speaking, the function adds a column of predicted values to the DataFrame.\n",
    "\n",
    "Finally, the function should return the concatenated DataFrame with the \"prediction\" column added. \n",
    "\n",
    "**Hint**: You may find the [pandas.DataFrame.concat](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.concat.html) function useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ref(x: pd.DataFrame, y: pd.DataFrame, model: LGBMRegressor|mlflow.pyfunc.PyFuncModel) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a reference data by combining features, targets, and the predictions made by the model\n",
    "    Args:\n",
    "        x: Feature DataFrame\n",
    "        y: Target DataFrame\n",
    "        model: Used to make predictions on x\n",
    "    Returns:\n",
    "        Reference data (features+targets+predictions)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if your `prep_ref` function works correctly by preparing a reference dataset used in this assignment. We will use model's testing data as the reference data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = \"s3://mlflow/11/4e08505e318f4f5690ceebe673878268/artifacts/lgbm-house\" # This should be the same as the modelUri in your manifests/house-price.yaml\n",
    "\n",
    "uploaded_model = mlflow.pyfunc.load_model(model_uri=model_s3_uri)\n",
    "\n",
    "# read features and targets of the testing data\n",
    "# We generated \"1_0_X.parquet\" and \"1_0_y.csv\" in advance by pre-processing the raw data in raw_data/reference/test using the etl function from the second week\n",
    "test_x = pd.read_parquet(WORKING_DIR/FEATURE_STORE_DIR_NAME/\"1_0_X.parquet\")\n",
    "test_y = pd.read_csv(WORKING_DIR/FEATURE_STORE_DIR_NAME/\"1_0_y.csv\", index_col=False)\n",
    "\n",
    "current_ref_df = prep_ref(x=test_x, y=test_y, model=uploaded_model)\n",
    "display(current_ref_df.head())\n",
    "print(f\"Reference DataFrame should have the following {current_ref_df.shape[1]} columns: {current_ref_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"./images/ref_data.png\" />\n",
    "\n",
    "Reference DataFrame should have the following 18 columns: Index(['yr_built', 'bedrooms', 'postcode', 'area', 'bathrooms', 'condition',\n",
    "       'grade', 'sqft_living', 'sqft_lot', 'sqft_basement', 'sqft_living15',\n",
    "       'sqft_lot15', 'waterfront', 'view', 'distance', 'year', 'price',\n",
    "       'prediction'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to complete the `upload_reference_data` function that uploads the reference data (a Parquet file) to MLflow. The file should be under the MLflow Run that trains your on-production model (for house price prediction), i.e., the model you deployed to KServe. \n",
    "\n",
    "**Hints**:\n",
    "- [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run)\n",
    "- [mlflow.log_artifact](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.log_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_reference_data(mlflow_run_id: str, model_s3_uri: str, x: pd.DataFrame, y: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepare a reference data and store it under an MLflow Run as an artifact (a Parquet file).\n",
    "    Args:\n",
    "        mlflow_run_id: The ID of the MLflow Run under which the reference data is stored\n",
    "        model_s3_uri: The S3 URI of the model used to make predictions on x\n",
    "        x: A feature DataFrame\n",
    "        y: A target DataFrame, x and y are used to generate the reference data\n",
    "    \"\"\"\n",
    "    # Prepare the reference data and save it locally first\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri=model_s3_uri)\n",
    "    reference_df = prep_ref(x=x, y=y, model=loaded_model)\n",
    "    reference_df.to_parquet(\"reference.parquet\")\n",
    "\n",
    "    artifact_path = \"reference_data\" # use this as the run-relative artifact path when uploading the Parquet file of the reference data\n",
    "\n",
    "    # Upload the local reference.parquet to MLflow under the Run with the given mlflow_run_id\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your `upload_reference_data` using the next code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = \"5c2e344a8aab483b8a269a7b22e58756\" # replace this with your own one, this should be the MLflow Run ID that trains your on-production model\n",
    "\n",
    "upload_reference_data(mlflow_run_id=mlflow_run_id, model_s3_uri=model_s3_uri, x=test_x, y=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the MLflow Run that trains your on-production model, there should be a \"reference.parquet\" file under the \"reference_data\" folder. E.g., \n",
    "\n",
    "<img src=\"./images/ref_data_mlflow_run.png\" width=1000 />\n",
    "\n",
    "(You may need to refresh the page to see the uploaded file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Collect model inputs and outputs and combine them with ground truth\n",
    "So far you've prepared the reference data for your model. The next step is to collect the production data so that we can monitor how the model performs on real-world data. Similar to reference data, the production data also contains: 1) model inputs (the features on which it makes predictions), 2) model outputs (the predictions), and 3) the ground truth of the inputs.\n",
    "\n",
    "The inference service running on KServe receives inputs (features) and returns outputs (predictions). Your task is to complete the `collect_input` function that performs the following tasks:\n",
    "1. Given an HTTP response returned by the inference service, extracting the predictions from the response and adding the predictions to the input feature DataFrame.\n",
    "1. Saving the DataFrame containing both inputs and predictions to a CSV file. The input DataFrame has an index named \"request_id\", please include the index when you save the DataFrame to a CSV file. If the file already exists, the function appends the data to the existing file; otherwise, it creates a new file.\n",
    "\n",
    "**Hints**:\n",
    "- [os.path.isfile](https://docs.python.org/3.10/library/os.path.html#os.path.isfile)\n",
    "- [pandas.DataFrame.to_csv](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.to_csv.html) (The `index_label`, `mode`, and `header` arguments might be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_inputs_outputs(input_df: pd.DataFrame, response: requests.Response, inputs_outputs_file_path: str):\n",
    "    \"\"\"\n",
    "    Adds the predictions returned from a web API response to an input DataFrame and saves the combined data (input features and predictions) to a Parquet file\n",
    "    Args:\n",
    "        input_df: Input (feature) DataFrame\n",
    "        response: The response (containing predictions) returned by an inference service\n",
    "        inputs_outputs_file: The CSV file of the combined data, i.e., inputs(features)+outputs(predictions)\n",
    "    \"\"\"\n",
    "    # Extract predictions and add them to the input DataFrame. We assume the predictions and inputs are aligned\n",
    "    y_pred_list = response.json()[\"outputs\"][0][\"data\"]\n",
    "    input_df[\"prediction\"] = y_pred_list\n",
    "    \n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the `collect_inputs_outputs` function by running the following two code cells. In the next code cell, we'll simulate requests using the data from 2016, 2017, and 2018 and send these requests to the inference service. Each request contains 100 inputs (one feature row is one input). The data used for generating requests is saved in the \"request_data\" folder, which was generated beforehand so you don't need to worry about it. \n",
    "\n",
    "The next code cell takes around 1min30s to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collected inputs and outputs for each quarter is saved in a separate file (inputs_outputs/<year>_<quarter>.csv)\n",
    "# We collect data for years 17, 18, and 19. This data will be used in some other tasks later\n",
    "for year in [2016, 2017, 2018]:\n",
    "    for quarter in range(1, 5):\n",
    "        # Delete possible existing inputs_outputs files for consistency\n",
    "        inputs_outputs_path = WORKING_DIR/INPUTS_OUTPUTS_LOCAL_DIR_NAME/f\"{year}_{quarter}.csv\"\n",
    "        if os.path.isfile(inputs_outputs_path):\n",
    "            os.remove(inputs_outputs_path)\n",
    "    \n",
    "        inputs_df = pd.read_parquet(WORKING_DIR/\"request_data\"/f\"{year}_{quarter}_X.parquet\")\n",
    "        \n",
    "        # Each request contains 100 inputs\n",
    "        chunk_size = 10\n",
    "    \n",
    "        for i in range(0, inputs_df.shape[0], chunk_size):\n",
    "            request_df = inputs_df.iloc[i:i+chunk_size]\n",
    "            encoded_request_data = PandasCodec.encode_request(request_df).dict()\n",
    "            response = send_requests(model_name=\"house-price\",\n",
    "                                     input=encoded_request_data[\"inputs\"], count=1)\n",
    "            collect_inputs_outputs(request_df, response, str(inputs_outputs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "for quarter in range(1, 5):\n",
    "    inputs_outputs_df = pd.read_csv(WORKING_DIR/INPUTS_OUTPUTS_LOCAL_DIR_NAME/f\"{year}_{quarter}.csv\", index_col=\"request_id\")\n",
    "    print(f\"Collected data of inputs and outputs ({year}-quarter{quarter}) has a shape of: {inputs_outputs_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epected output:\n",
    "\n",
    "```text\n",
    "Collected data of inputs and outputs (2016-quarter1) has a shape of: (718, 17)\n",
    "Collected data of inputs and outputs (2016-quarter2) has a shape of: (796, 17)\n",
    "Collected data of inputs and outputs (2016-quarter3) has a shape of: (727, 17)\n",
    "Collected data of inputs and outputs (2016-quarter4) has a shape of: (972, 17)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected model inputs and outputs, let's look at how the collected data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_outputs_sample_df = pd.read_parquet(WORKING_DIR/INPUTS_OUTPUTS_LOCAL_DIR_NAME/f\"2016_1.parquet\").head()\n",
    "inputs_outputs_sample_df = pd.read_csv(WORKING_DIR/INPUTS_OUTPUTS_LOCAL_DIR_NAME/f\"2016_1.csv\", index_col=\"request_id\").head()\n",
    "inputs_outputs_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"./images/inputs_outputs_sample.png\" />\n",
    "\n",
    "As we can see, the collected data includes the features that the model receives as inputs and the predictions the model returns as outputs. In addition, the DatFrame also has an index named \"request_id\". This index will be used to combine the ground truth (i.e., targets) with the inputs. \n",
    "\n",
    "Let's assume we've already collected the ground truth for each quarter of 2016 and saved them as CSV files in the \"ground_truth\" directory.The ground_truth data also has \"request_id\" index. Now, please complete the `combine_ground_truth` that performs the following tasks:\n",
    "1. Reading inputs+outputs data (in the \"inputs_outputs\" directory) and the ground truth data from their files as DataFrames.\n",
    "1. Using the \"request_id\" index to combine ground truth DataFrame with the inputs+outputs DataFrame. Please note that the ground truth may be collected in a different order than the corresponding inputs. \n",
    "1. Removing the \"request_id\" index from the combined DataFrame as the index may cause some issues with Evidently. \n",
    "\n",
    "The function should return a DataFrame the contains model inputs, outputs, and the ground truth. \n",
    "\n",
    "**Hint**:  The [pandas.DataFrame.merge](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.merge.html) function may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ground_truth(year: int, quarter: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine ground truth with the model inputs+outputs based on the \"request_id\" index\n",
    "    Args:\n",
    "        year and quarter: The time range of the data to be combined\n",
    "    \"\"\"\n",
    "    # The inputs+outputs DataFrame\n",
    "    inputs_outputs_df = pd.read_csv(WORKING_DIR/INPUTS_OUTPUTS_LOCAL_DIR_NAME/f\"{year}_{quarter}.csv\", index_col=\"request_id\")\n",
    "\n",
    "    # The ground truth DataFrame\n",
    "    ground_truth_df = pd.read_csv(WORKING_DIR/GROUND_TRUTH_LOCAL_DIR_NAME/f\"{year}_{quarter}_y.csv\", index_col=\"request_id\")\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quarter in range(1, 5):\n",
    "    prod_df = combine_ground_truth(year=2016, quarter=quarter)\n",
    "    print(f\"Production data of quarter {quarter} should have a shape of {prod_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "Production data of quarter 1 should have a shape of (718, 18)\n",
    "Production data of quarter 2 should have a shape of (796, 18)\n",
    "Production data of quarter 3 should have a shape of (727, 18)\n",
    "Production data of quarter 4 should have a shape of (972, 18)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Get on-production model version and the corresponding MLflow Running ID\n",
    "In reality, the on-production model is not fixed, new model version will be deployed to replace the old one. Please complete the `get_production_model_version` function that uses the name of the registered model to retrieves the on-production model version (model's stage should be \"Production\") and the corresponding MLflow Run ID. You can assume that only one model for predicting house price can be in the \"Production\" stage at any given time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_model_version(registered_model_name: str) -> NamedTuple(\"Output\", [(\"model_version\", str), (\"run_id\", str),]):\n",
    "    \"\"\"\n",
    "    Retrieve the on-production model version (model's stage should be \"Production\") and the corresponding MLflow Run ID\n",
    "    Args:\n",
    "        registered_model_name: The name of the registered model, it's the name passed as the \"registered_model_name\" argument to the mlflow.lightgbm.log_model function\n",
    "    Returns:\n",
    "        A namedtuple consisting of the on-production model version and the corresponding MLflow Run ID\n",
    "\n",
    "    \"\"\"\n",
    "    client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "    output = namedtuple(\"Output\", [\"model_version\", \"run_id\"])\n",
    "\n",
    "    model_version = None # on-production model version\n",
    "    mlflow_run_id = None # corresponding MLflow Run ID\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return output(f\"{registered_model_name}-{model_version}\", mlflow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_production_model_version(registered_model_name)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "Output(model_version='Week6LgbmHousePrice-<on-production-model-version>', run_id='<corresponding-mlflow-run-id>')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Monitor model performance and drift using Evidently\n",
    "First, complete the `prep_report` function that produces an Evidently Report using the following Metric Presets: `RegressionPreset`, `TargetDriftPreset` and `DataDriftPreset`. (You can use the default configurations of these Presets.) The Report should also have a list of tags and a timestamp. The tags and timestamp are specified as the function's arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_report(prod_df: pd.DataFrame, ref_df: pd.DataFrame, tags: List[str], timestamp: datetime, column_mapping_dict: Dict[str, Any]) -> Report:\n",
    "    \"\"\"\n",
    "    Generate a Evidently Report of regression performance, target drift and data drift\n",
    "    Args:\n",
    "        prod_df: Production DataFrame (model inputs+outputs+ground truth) to be monitored\n",
    "        ref_df: Reference DataFrame \n",
    "        tags: Tags of the Evidently Report\n",
    "        timestamp: Timestamp of the Evidently Report\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "    Returns:\n",
    "        Evidently Report\n",
    "    \"\"\"\n",
    "    # The column mapping that Evidently needs\n",
    "    column_mapping = ColumnMapping(**column_mapping_dict)\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if the `prep_report` function can generate the correct Evidently Report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Report for the third quarter of 2016\n",
    "prod_df = combine_ground_truth(year=2016, quarter=3)\n",
    "ref_df = pd.read_parquet(\"reference.parquet\") #\"reference.parquet\" was created when running the upload_reference_data function\n",
    "\n",
    "numerical_features = ['yr_built', 'bedrooms', 'postcode', 'area', 'bathrooms', 'condition',\n",
    "                     'grade', 'sqft_living', 'sqft_lot', 'sqft_basement', 'sqft_living15',\n",
    "                     'sqft_lot15', 'waterfront', 'view', 'distance']\n",
    "column_mapping_dict = {\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"target\": \"price\",\n",
    "    \"prediction\": \"prediction\",\n",
    "    \"datetime_features\": [\"year\"]\n",
    "}\n",
    "\n",
    "report = prep_report(prod_df=prod_df, \n",
    "            ref_df=ref_df, \n",
    "            tags=[\"2016-quarter3\"], \n",
    "            timestamp=datetime(year=2016, month=10, day=1),\n",
    "            column_mapping_dict=column_mapping_dict)\n",
    "\n",
    "report.save_html(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content in \"report.html\" should be the same as \"sample_monitoring_results/report.html\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the report.html you produced and the example one\n",
    "webbrowser.open(\"file:///\" + str(WORKING_DIR/\"report.html\"))\n",
    "webbrowser.open(\"file:///\" + str(WORKING_DIR/\"sample_monitoring_results\"/\"report.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, complete the `prep_regression_test` that produces an Evidently TestSuite. The TestSuite should include only one individual test for testing MAE (of the production data).  The test should fail if the MAE is not less than 40000. The TestSuite should also have a list of tags and a timestamp. The tags and timestamp are specified as the function's arguments. \n",
    "\n",
    "You may find the `TestValueMAE` test helpful. The use of `TestValueMAE` is the same as `TestValueR2Score` in the tutorial. You can use `lt<threshold` (lt means \"less than\") as the argument to specify the test criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_regression_test(prod_df: pd.DataFrame, ref_df: pd.DataFrame, tags: List[str], timestamp: datetime, column_mapping_dict: Dict[str, Any]) -> TestSuite:\n",
    "    \"\"\"\n",
    "    Generate a Evidently TestSuite of MAE\n",
    "    Args:\n",
    "        prod_df: Production DataFrame (model inputs+outputs+ground truth) to be monitored\n",
    "        ref_df: Reference DataFrame (the one uploaded to MLflow)\n",
    "        tags: Tags of the Evidently report\n",
    "        timestamp: Timestamp of the Evidently report\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "    Returns:\n",
    "        Evidently TestSuite\n",
    "    \"\"\"\n",
    "    # The column mapping that Evidently needs\n",
    "    column_mapping = ColumnMapping(**column_mapping_dict)\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if the `prep_regression_test` function can generate the correct Evidently Test Suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_test = prep_regression_test(prod_df=prod_df, \n",
    "                     ref_df=ref_df, \n",
    "                     tags=[\"2016-quarter3\"], \n",
    "                     timestamp=datetime(year=2016, month=10, day=1),\n",
    "                     column_mapping_dict=column_mapping_dict\n",
    "                    )\n",
    "regression_test.save_html(\"regression_test.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content in \"regression_test.html\" should be the same as \"sample_monitoring_results/regression_test.html\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the regression_test.html you produced and the example one\n",
    "webbrowser.open(\"file:///\" + str(WORKING_DIR/\"regression_test.html\"))\n",
    "webbrowser.open(\"file:///\" + str(WORKING_DIR/\"sample_monitoring_results\"/\"regression_test.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, complete the `monitor` function that performs the following tasks:\n",
    "1. Calling `prep_report` and `prep_regression_test` to generate an Evidently Report and TestSuite for a given production dataset.\n",
    "1. Pushing the Report and Test to a given Evidently Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(\n",
    "    evidently_workspace: Workspace,\n",
    "    evidently_project_id: str,\n",
    "    column_mapping_dict: Dict[str, Any], \n",
    "    prod_df: pd.DataFrame,\n",
    "    prod_model_version: str,\n",
    "    year: int,\n",
    "    quarter: int\n",
    "):  \n",
    "    \"\"\"\n",
    "    Generate an Evidently Report and TestSuite for a given production dataset and push them to a given Evidently Workspace.\n",
    "    Args:\n",
    "        evidently_workspace and evidently_project_id: The Evidently Workspace and the ID of the Evidently Project where the reports and test suites should be stored\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "        prod_df: The production DataFrame to be monitored\n",
    "        prod_model_version: The on-production model version to be monitored\n",
    "        year and quarter: The time range of the data to be monitored\n",
    "    \"\"\"\n",
    "    # Prepare tags for the Report and TestSuite\n",
    "    time_tag = f\"{year}-quarter{quarter}\"\n",
    "    tags = [time_tag, prod_model_version]\n",
    "\n",
    "    # Prepare timestamp for the Report and TestSuite, assuming the monitoring results are generated on the first day of next quarter\n",
    "    timestamp = None\n",
    "    if quarter < 4:\n",
    "        timestamp = datetime(year=year, month=quarter*3+1, day=1)\n",
    "    elif quarter == 4:\n",
    "        timestamp = datetime(year=year+1, month=1, day=1)\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local Evidently monitoring workspace and initialize a project\n",
    "local_workspace_name = \"evidently_workspace\"\n",
    "local_project_name = \"house-price-model-monitoring\"\n",
    "\n",
    "if os.path.isfile(WORKING_DIR/local_workspace_name):\n",
    "    os.remove(WORKING_DIR/local_workspace_name)\n",
    "\n",
    "local_workspace = Workspace.create(WORKING_DIR/local_workspace_name)\n",
    "\n",
    "local_house_price_project = init_evidently_project(workspace=local_workspace, project_name=local_project_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "for quarter in range(1, 5):\n",
    "    monitor(\n",
    "        evidently_workspace=local_workspace,\n",
    "        evidently_project_id=local_house_price_project.id,\n",
    "        column_mapping_dict=column_mapping_dict,\n",
    "        prod_df=combine_ground_truth(year=year, quarter=quarter),\n",
    "        prod_model_version=get_production_model_version(registered_model_name=registered_model_name).model_version,\n",
    "        year=year,\n",
    "        quarter=quarter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are should be eight snapshots under the directory evidently_workspace/<evidently_project_id>/snapshots. E.g., \n",
    "\n",
    "<img src=\"./images/snapshots.png\" width=500 />\n",
    "\n",
    "(The snapshot filenames vary.)\n",
    "\n",
    "If you open the Evidently monitor UI by running\n",
    "```bash\n",
    "# Under the same directory of this notebook\n",
    "evidently ui --workspace ./evidently_workspace/\n",
    "```\n",
    "and go to [http://localhost:8000](http://localhost:8000), you should see a dashboard (under the \"house-price-model-monitoring\" project) like\n",
    "\n",
    "<img src=\"./images/evidently_dashboard.png\" width=800 />\n",
    "\n",
    "In the \"REPORTS\" field, there should be four Reports, one for each quarter:\n",
    "\n",
    "<img src=\"./images/evidently_reports.png\" width=1000/>\n",
    "\n",
    "Similarly, in the \"TEST SUITES\" field, there should be four Test Suites, one for each quarter:\n",
    "\n",
    "<img src=\"./images/evidently_testsuites.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put everything together to create a monitoring pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_monitoring_pipeline(\n",
    "        year: int, \n",
    "        quarter: int, \n",
    "        registered_model_name: str, \n",
    "        evidently_workspace: Workspace,\n",
    "        evidently_project_id: str,\n",
    "        column_mapping_dict: Dict[str, Any]\n",
    "):\n",
    "    \"\"\" \n",
    "    A monitoring pipeline that 1) prepares the production data ot be monitored, 2) fetching the on-production model version to be monitored, and \n",
    "    3) stores the monitoring results to a local Evidently Workspace. \n",
    "    Args:\n",
    "        year and quarter: The time range of the data to be monitored\n",
    "        registered_model_name: Name of the model registered to MLflow\n",
    "        evidently_workspace and evidently_project_id: The Evidently Workspace and the ID of the Evidently Project where the reports and test suites should be stored\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "    \"\"\"\n",
    "    prod_df = combine_ground_truth(year=year, quarter=quarter)\n",
    "    model_version_output = get_production_model_version(registered_model_name=registered_model_name)\n",
    "    monitor(\n",
    "        evidently_workspace=evidently_workspace,\n",
    "        evidently_project_id=evidently_project_id,\n",
    "        column_mapping_dict=column_mapping_dict,\n",
    "        prod_df=prod_df,\n",
    "        prod_model_version=model_version_output.model_version,\n",
    "        year=year,\n",
    "        quarter=quarter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another Evidently project\n",
    "local_project_name2 = \"house-price-model-monitoring2\"\n",
    "\n",
    "if os.path.isfile(WORKING_DIR/local_workspace_name):\n",
    "    os.remove(WORKING_DIR/local_workspace_name)\n",
    "\n",
    "local_house_price_project = init_evidently_project(workspace=local_workspace, project_name=local_project_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column mapping configuration\n",
    "numerical_features = ['yr_built', 'bedrooms', 'postcode', 'area', 'bathrooms', 'condition',\n",
    "                     'grade', 'sqft_living', 'sqft_lot', 'sqft_basement', 'sqft_living15',\n",
    "                     'sqft_lot15', 'waterfront', 'view', 'distance']\n",
    "column_mapping_dict = {\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"target\": \"price\",\n",
    "    \"prediction\": \"prediction\",\n",
    "    \"datetime_features\": [\"year\"]\n",
    "}\n",
    "\n",
    "arguments = {\n",
    "    \"year\": 2016,\n",
    "    \"quarter\": 1,\n",
    "    \"registered_model_name\": registered_model_name,\n",
    "    \"evidently_workspace\": local_workspace,\n",
    "    \"evidently_project_id\": local_house_price_project.id,\n",
    "    \"column_mapping_dict\": column_mapping_dict\n",
    "}\n",
    "\n",
    "# monitor data for every quarter of 2016 and 2017\n",
    "for year in [2016, 2017]:\n",
    "    for quarter in range(1, 5):\n",
    "        arguments[\"year\"] = year\n",
    "        arguments[\"quarter\"] = quarter\n",
    "        local_monitoring_pipeline(**arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the Evidently monitor UI and you should see the similar dashboard under the \"house-price-model-monitoring2\" project (except that the monitoring results come from both 2016 and 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to delete a snapshot\n",
    "\n",
    "# local_house_price_project.delete_snapshot(\n",
    "#     snapshot_id=\"7dd70c32-2d95-4e9a-b79e-efa4b5a709f9\") # Assign the ID of the snapshot you want to delete to snapshot_id\n",
    "# requests.get(\n",
    "#     f\"http://localhost:8000/api/projects/{local_house_price_project.id}/reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots for Assignment2\n",
    "Please include the following screenshots in your PDF file:\n",
    "\n",
    "(The examples below are used to illustrate which parts should be included in the screenshot. They are not the correct screenshots required.)\n",
    "\n",
    "**The dashboard showing the MAE changes in 2016 and 2017**\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/evidently_dashboard.png\" width=800 />\n",
    "    <figcaption>The example only shows MAE changes in 2016</figcaption>\n",
    "</details>\n",
    "\n",
    "\n",
    "**The Evidently Report for quarter 3 of 2017**\n",
    "\n",
    "Please capture the following three parts: 1)\"Regression Model Performance, 2)\"Error Bias Table\"+\"Predicted vs Actual per Group\", and 3)\"Data Drift Summary\"\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/report_regression_performance.png\" width=1000 />\n",
    "    <br />\n",
    "    <img src=\"./images/report_error_bias.png\" width=1000 />\n",
    "    <br />\n",
    "    <img src=\"./images/report_data_drift.png\" width=1000 />\n",
    "    <figcaption>The examples are captured from the Report for quarter 3 of 2016</figcaption>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Monitoring pipeline (4 points)\n",
    "Now, let's migrate the monitoring pipeline to the MLOps platform using KFP. In this assignment, you need create three KFP components and then combine them into a KFP Pipeline, which works in a similar way as the previous local monitoring pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Let's first deploy a [remote Evidently Workspace](https://docs.evidentlyai.com/user-guide/monitoring/workspace_project#remote-workspace) to the MLOps platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a remote Evidently Workspace to the MLOps platform\n",
    "!kubectl apply -k evidently_monitor_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that a pod running the Evidently Workspace is running\n",
    "!kubectl -n monitoring get pods -l app=evidently-monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "NAME                                READY   STATUS    RESTARTS   AGE\n",
    "evidently-monitor-b8754f77b-2dq5t   1/1     Running   0          85s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following text to your `/etc/hosts` so that your machine knows how to resolve the host name of the remote Evidently Workspace service. (Remember to use sudo to open the file, otherwise you may not be able to save the modification).\n",
    "```text\n",
    "<the-floating-ip-of-your-cPouta-VM> evidently-monitor-ui.local\n",
    "```\n",
    "(You only need to do this once.)\n",
    "\n",
    "Go to [http://evidently-monitor-ui.local](http://evidently-monitor-ui.local) and you should see the Eviently monitoring UI as you saw in the previous assignment, expect there is not any project yet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently configuration\n",
    "EVIDENTLY_MONITOR_URL = \"http://evidently-monitor-ui.local\"\n",
    "\n",
    "INPUTS_OUTPUTS_BUCKET_NAME = \"inputs-outputs\"\n",
    "GROUND_TRUTH_BUCKET_NAME = \"ground-truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the files of model inputs+outputs and ground truth to a MinIO storage service (we use the one used by MLflow here). When running a monitoring pipeline on the MLOps plarform, we can assume that the model inputs+outputs and the ground truth are already available in MinIO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload production data and ground truth to minio\n",
    "\n",
    "minio_client = Minio(\n",
    "    endpoint=\"mlflow-minio.local\",\n",
    "    access_key=AWS_ACCESS_KEY_ID,\n",
    "    secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# MinIO doesn't accept _ in bucket names, so we replace _ with - in the directory names and use a dictionary to map a bucket name to the corresponding local directory name\n",
    "dirname_to_bucketname = {\n",
    "    INPUTS_OUTPUTS_LOCAL_DIR_NAME: INPUTS_OUTPUTS_BUCKET_NAME,\n",
    "    GROUND_TRUTH_LOCAL_DIR_NAME: GROUND_TRUTH_BUCKET_NAME\n",
    "}\n",
    "\n",
    "# Create two buckets\n",
    "for dir_name in [INPUTS_OUTPUTS_LOCAL_DIR_NAME, GROUND_TRUTH_LOCAL_DIR_NAME]:\n",
    "    bucket_name = dirname_to_bucketname[dir_name]\n",
    "    found = minio_client.bucket_exists(bucket_name)\n",
    "    if found:\n",
    "        print(f\"Bucket {bucket_name} already existing\")\n",
    "        continue\n",
    "    minio_client.make_bucket(bucket_name=bucket_name, object_lock=True)\n",
    "\n",
    "# Upload files containing model inputs + outputs, and the ground truth\n",
    "for dir_name in [INPUTS_OUTPUTS_LOCAL_DIR_NAME, GROUND_TRUTH_LOCAL_DIR_NAME]:\n",
    "    files = os.listdir(WORKING_DIR/dir_name)\n",
    "    bucket_name = dirname_to_bucketname[dir_name]\n",
    "    for filename in files:\n",
    "        minio_client.fput_object(bucket_name=bucket_name, object_name=filename, file_path=WORKING_DIR/dir_name/filename)\n",
    "        print(f\"Uploaded {filename} to {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project in the remote Evidently Workspace, this project will be used to test if your KFP components work correctly. \n",
    "remote_workspace = RemoteWorkspace(EVIDENTLY_MONITOR_URL)\n",
    "test_project_name = \"test_project\"\n",
    "test_project = init_evidently_project(workspace=remote_workspace, project_name=test_project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Component for combining the ground truth with model inputs and outputs\n",
    "Similar to the `combine_ground_truth` function in Assignment2, this component performs the following tasks:\n",
    "1. Reading model inputs+outputs data and the ground truth data from their files into DataFrames. Unlike the previous assignment where the files are saved locally, the component needs to read the data from files stored in MinIO. (This is already done for you.) \n",
    "1. Using the \"request_id\" index to combine the ground truth DataFrame with the inputs+outputs DataFrame.\n",
    "1. Removing the \"request_id\" index from the combined DataFrame.\n",
    "The component should then save the combined DataFrame as a Parquet file into an output of type Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\", \"minio~=7.1.3\", \"fastparquet~=2023.10.1\"],\n",
    ")\n",
    "def combine_ground_truth(\n",
    "    year: int, \n",
    "    quarter: int, \n",
    "    s3_endpoint_url: str, \n",
    "    inputs_outputs_bucket_name: str,\n",
    "    ground_truth_bucket_name: str,\n",
    "    prod_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine ground truth with the model inputs+outputs based on the \"request_id\" index\n",
    "    Args: \n",
    "        year and quarter: The time range of the data to be combined\n",
    "        s3_endpoint_url: The URL of the MinIO service where the data is stored\n",
    "        inputs_outputs_bucket_name: Name of the bucket where model inputs+outputs data is stored\n",
    "        ground_truth_bucket_name: Name of the bucket where ground truth is stored\n",
    "        prod_data: The output of type Dataset that the combined data (inputs+outputs+ground truth in a Parquet file) should be saved\n",
    "    \"\"\"\n",
    "    from minio import Minio\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import io\n",
    "\n",
    "    def read_df_from_s3(bucket_name: str, object_name: str, function: Callable, **kwargs) -> pd.DataFrame:\n",
    "        obj = minio_client.get_object(\n",
    "            bucket_name=bucket_name, object_name=object_name)\n",
    "        df = function(io.BytesIO(obj.data), **kwargs)\n",
    "        return df\n",
    "\n",
    "    minio_client = Minio(\n",
    "        endpoint=s3_endpoint_url.split(\"://\")[1],\n",
    "        access_key=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        secret_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    inputs_outputs_filename = f\"{year}_{quarter}.csv\"\n",
    "    ground_truth_filename = f\"{year}_{quarter}_y.csv\"\n",
    "\n",
    "    # inputs-outputs DataFrame\n",
    "    inputs_outputs_df = read_df_from_s3(\n",
    "        bucket_name=inputs_outputs_bucket_name, object_name=inputs_outputs_filename, function=pd.read_csv, index_col=\"request_id\")\n",
    "    # Ground truth DataFrame\n",
    "    ground_truth_df = read_df_from_s3(bucket_name=ground_truth_bucket_name,\n",
    "                                      object_name=ground_truth_filename, function=pd.read_csv, index_col=\"request_id\")\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the functionality of the `combine_ground_truth` component by combining the inputs+outputs and the ground truth data for 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "year = 2016\n",
    "for quarter in range(1, 5):\n",
    "    # Mock a Dataset output\n",
    "    prod_data = create_autospec(Dataset, metadata=dict(), path=f\"prod_data_{year}_{quarter}.parquet\")\n",
    "    combine_ground_truth.python_func(\n",
    "        year=year, \n",
    "        quarter=quarter, \n",
    "        s3_endpoint_url=\"http://mlflow-minio.local\", \n",
    "        inputs_outputs_bucket_name=INPUTS_OUTPUTS_BUCKET_NAME,\n",
    "        ground_truth_bucket_name=GROUND_TRUTH_BUCKET_NAME,\n",
    "        prod_data=prod_data)\n",
    "\n",
    "    df = pd.read_parquet(f\"prod_data_{year}_{quarter}.parquet\")\n",
    "    print(f\"The production DataFrame(inputs+outputs+ground truth) for quarter {quarter} in year {year} should have a shape of {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "The production DataFrame(inputs+outputs+ground truth) for quarter 1 in year 2016 should have a shape of (718, 18)\n",
    "The production DataFrame(inputs+outputs+ground truth) for quarter 2 in year 2016 should have a shape of (796, 18)\n",
    "The production DataFrame(inputs+outputs+ground truth) for quarter 3 in year 2016 should have a shape of (727, 18)\n",
    "The production DataFrame(inputs+outputs+ground truth) for quarter 4 in year 2016 should have a shape of (972, 18)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Component for finding the on-produciton model version and the corresponding MLflow RUN ID\n",
    "Similar to the `get_production_model_version` function in Assignment2, this component uses the name of the registered model to retrieves the on-production model version (model's stage should be \"Production\") and the corresponding MLflow Run ID. You can assume that only one model for predicting house price can be in the \"Production\" stage at any given time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"mlflow==2.3.2\"],\n",
    ")\n",
    "def get_production_model_version(registered_model_name: str, mlflow_tracking_uri: str) -> NamedTuple(\"Output\", [(\"model_version\", str), (\"run_id\", str),]):\n",
    "    \"\"\"\n",
    "    Retrieve the on-production model version (model's stage should be \"Production\") and the corresponding MLflow Run ID\n",
    "    Args:\n",
    "        registered_model_name: The name of the registered model, it's the name passed as the \"registered_model_name\" argument to the mlflow.lightgbm.log_model function\n",
    "    Returns:\n",
    "        A namedtuple consisting of the on-production model version and the corresponding MLflow Run ID\n",
    "    \"\"\"\n",
    "    from mlflow import MlflowClient\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    client = MlflowClient(tracking_uri=mlflow_tracking_uri)\n",
    "    output = namedtuple(\"Output\", [\"model_version\", \"run_id\"])\n",
    "    model_version = None # on-production model version\n",
    "    mlflow_run_id = None # corresponding MLflow Run ID\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return output(f\"{registered_model_name}-{model_version}\", mlflow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_output = get_production_model_version.python_func(\n",
    "    registered_model_name=registered_model_name,\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\"\n",
    ")\n",
    "model_version_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "Output(model_version='Week6LgbmHousePrice-<on-productioc-model-version>', run_id='<corresponding-mlflow-run-id>')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Component for preparing monitoring results and pushing them to the Evidently monitor service\n",
    "Again, similar to the `monitor` function in Assignment2, this component generates an Evidently Report and TestSuite for a given production dataset and push them to a remote Evidently Workspace.\n",
    "\n",
    "**Notes**\n",
    "1. You also need to declare the `prep_report` and `prepare_regression_test` functions inside the component. These two functions work in the same way as those you created in Assignment2. The `prep_report` function produces an Evidently Report using the following Metric Presets: `RegressionPreset`, `TargetDriftPreset` and `DataDriftPreset`. The `prep_regression_test` produces an Evidently TestSuite for testing MAE (of the predictions made on the production data). You can use the `TestValueMAE` test in the TestSuite. The test should fail if the MAE is not less than 40000. An exception is that the  `prep_report` and `prepare_regression_test` functions don't accept an `column_mapping_dict` argument but use a column mapping created beforehand. \n",
    "1. Unlike the previous assignment where the reference DataFrame is directly read from a local file, this component needs to first download the reference Parquet file from the MLflow's artifact store and then loads the reference DataFrame from the downloaded Parquet file. \n",
    "\n",
    "**Hints**:You can use [mlflow.artifacts.download_artifacts](https://mlflow.org/docs/2.3.2/python_api/mlflow.artifacts.html#mlflow.artifacts.download_artifacts) to download the reference data. When using the method, using the `run_id` argument may be easier than `artifact_uri` as the MLflow Run ID will be passed as an input to the component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"mlflow==2.3.2\", \"git+https://github.com/yumoL/evidently.git\", \"boto3~=1.28.85\"],\n",
    ")\n",
    "def monitor(\n",
    "        evidently_monitor_uri: str, \n",
    "        evidently_project_id: str,\n",
    "        column_mapping_dict: Dict[str, Any], \n",
    "        prod_dataset: Input[Dataset], \n",
    "        prod_model_version: str,\n",
    "        mlflow_tracking_uri: str,\n",
    "        mlflow_s3_endpoint_url: str, \n",
    "        mlflow_run_id: str,\n",
    "        year: int,\n",
    "        quarter: int\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate an Evidently Report and TestSuite for a given production dataset and push them to a given Evidently Workspace.\n",
    "    Args:\n",
    "        evidently_monitor_uri: The URL of the remote Evidently Workspace\n",
    "        evidently_project_id: The ID of the Evidently Project where the reports and test suites should be stored\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "        prod_dataset: An input of type Dataset where the production DataFrame (inputs+outputs+ground truth)\n",
    "        prod_model_version: The on-production model version to be monitored\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        mlflow_run_id: ID of the MLflow Run that trains the on-production model\n",
    "        year and quarter: The time range of the data to be monitored\n",
    "    \"\"\"\n",
    "\n",
    "    from typing import List\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    from evidently.report import Report\n",
    "    from evidently.test_suite import TestSuite\n",
    "    from evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset\n",
    "    from evidently.tests import TestValueMAE\n",
    "    from evidently import ColumnMapping\n",
    "    from evidently.ui.remote import RemoteWorkspace\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # This is the column mapping used by the prep_report and prep_regression_test functions\n",
    "    column_mapping = ColumnMapping(**column_mapping_dict)\n",
    "\n",
    "    def prep_report(prod_df: pd.DataFrame, ref_df: pd.DataFrame, tags: List[str], timestamp: datetime) -> Report:\n",
    "        ### START CODE HERE\n",
    "        pass\n",
    "        ### END CODE HERE\n",
    "        \n",
    "    \n",
    "    def prep_regression_test(prod_df: pd.DataFrame, ref_df: pd.DataFrame, tags: List[str], timestamp: datetime) -> TestSuite:\n",
    "        ### START CODE HERE\n",
    "        pass\n",
    "        ### END CODE HERE\n",
    "        \n",
    "    \n",
    "    prod_df = pd.read_parquet(prod_dataset.path)\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = mlflow_s3_endpoint_url\n",
    "    \n",
    "    # Download the reference dataset (a Parquet file) and load it into a DataFrame. \n",
    "    # The dataset was uploaded to MinIO (under the MLflow Run that trains the on-production model).\n",
    "    reference_data_artifact_path = \"reference_data\" # This is the run-relative artifact path of the reference dataset\n",
    "    \n",
    "    ### START CODE HERE\n",
    "        \n",
    "    ### END CODE HERE\n",
    "\n",
    "    time_tag = f\"{year}-quarter{quarter}\"\n",
    "    \n",
    "    # Tags of the Evidently Report and Test Suite\n",
    "    tags = [time_tag, prod_model_version]\n",
    "\n",
    "    # Timestamp of the Evidently Report and Test Suite\n",
    "    timestamp = None\n",
    "    if quarter < 4:\n",
    "        timestamp = datetime(year=year, month=quarter*3+1, day=1)\n",
    "    elif quarter == 4:\n",
    "        timestamp = datetime(year=year+1, month=1, day=1)\n",
    "\n",
    "    # Generate Evidently Report and Test Suite\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    # Create a workspace instance (like a connection) to the remote Evidently Workspace \n",
    "    workspace = RemoteWorkspace(evidently_monitor_uri)\n",
    "\n",
    "    # Upload the Report and TestSuite to the remote Evidently Workspace, just as you upload them to a local Workspace\n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "for quarter in range(1, 5):\n",
    "    monitor.python_func(\n",
    "        evidently_monitor_uri=\"http://evidently-monitor-ui.local\", \n",
    "        evidently_project_id=test_project.id,\n",
    "        column_mapping_dict=column_mapping_dict, \n",
    "        prod_dataset=create_autospec(Dataset, metadata=dict(), path=f\"prod_data_{year}_{quarter}.parquet\"), \n",
    "        prod_model_version=model_version_output.model_version,\n",
    "        mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    "        mlflow_s3_endpoint_url=MLFLOW_S3_ENDPOINT_URL, \n",
    "        mlflow_run_id=model_version_output.run_id,\n",
    "        year=year,\n",
    "        quarter=quarter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the same dashboard as in Assignment2 under an Evidently Project named \"test_project\" at [http://evidently-monitor-ui.local](http://evidently-monitor-ui.local). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d) Create a monitoring KFP pipeline\n",
    "Your need to use the three KFP components you just created to create a KFP monitoring pipeline. The KFP components should perform the tasks in the following order:\n",
    "\n",
    "<img src=\"./images/kfp_monitoring_pipeline.jpg\" width=600 />\n",
    "\n",
    "(Other inputs the KFP components need are passed as arguments to the `monitoring_pipeline` function.)\n",
    "\n",
    "**Note**: You need to assign the needed credentials to the combine_ground_truth and monitor tasks so that they can download the needed files from the MinIO service (the one used by MLflow). (Please check the week5 tutorial for more details.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"monitoring-pipeline\",\n",
    "    description=\"Monitoring model performance, target and data drift using Evidently\"\n",
    ")\n",
    "def monitoring_pipeline(\n",
    "    year: int,\n",
    "    quarter: int,\n",
    "    registered_model_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    inputs_outputs_bucket_name: str,\n",
    "    ground_truth_bucket_name: str,\n",
    "    evidently_monitor_uri: str,\n",
    "    evidently_project_id: str,\n",
    "    column_mapping_dict: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        year and quarter: The time range of the data to be monitored\n",
    "        registered_model_name: The name of the model registered to MLflow\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        inputs_outputs_bucket_name: Name of the bucket where model inputs+outputs data is stored\n",
    "        ground_truth_bucket_name: Name of the bucket where ground truth is stored\n",
    "        evidently_monitor_uri: URI of the remote Evidently Workspace\n",
    "        evidently_project_id: The ID of the Evidently Project where the monitoring results are stored\n",
    "        column_mapping_dict: A dictionary containing the configuration of the column mapping\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init another Evidently Project at the remote Workspace\n",
    "house_price_project = init_evidently_project(remote_workspace, project_name=\"house-price-model-monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first run the pipeline to generate the Evidently monitoring results for the data from the first quarter of 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"year\": 2018,\n",
    "    \"quarter\": 1,\n",
    "    \"registered_model_name\": registered_model_name,\n",
    "    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\n",
    "    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\n",
    "    \"inputs_outputs_bucket_name\": INPUTS_OUTPUTS_BUCKET_NAME,\n",
    "    \"ground_truth_bucket_name\": GROUND_TRUTH_BUCKET_NAME,\n",
    "    \"evidently_monitor_uri\": \"http://evidently-service.monitoring.svc.cluster.local:8000\",\n",
    "    \"evidently_project_id\": house_price_project.id,\n",
    "    \"column_mapping_dict\": column_mapping_dict\n",
    "}\n",
    "\n",
    "run_name = \"house-price-monitoring-run\"\n",
    "experiment_name = \"house-price-monitoring-experiment\"\n",
    "\n",
    "kfp_client = kfp.Client()\n",
    "\n",
    "kfp_client.create_run_from_pipeline_func(\n",
    "    pipeline_func=monitoring_pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments, # These are the arguments passed to the pipeline function\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE, # KFP SDK has two versions (v1 and v2). We use v2 here\n",
    "    enable_caching=False # Disable caching for this pipeline, more details at https://www.kubeflow.org/docs/components/pipelines/v1/overview/caching-v2/\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see there is one KFP Run created at [http://ml-pipeline-ui.local](http://ml-pipeline-ui.local) (under the \"house-price-monitoring-experiment\" KFP Experiment). The KFP Run should be like:\n",
    "\n",
    "<img src=\"./images/monitoring_kfp_run.png\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you create many KFP Runs, you might see some error message of \"no space left on the device\" from the logs of some failed tasks. You can delete the KFP experiments on Kubeflow Pipelines using the code below and recreate the KFP Run by running the code cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to delete the KFP experiment, uncomment the following code\n",
    "\n",
    "# experiment_name = \"house-price-monitoring-experiment\"\n",
    "# experiment_id = kfp_client.get_experiment(experiment_name=experiment_name).id\n",
    "\n",
    "# # Get all runs under the experiment\n",
    "# response = kfp_client.list_runs(experiment_id=experiment_id)\n",
    "\n",
    "# # Delete all the runs\n",
    "# for run in response.runs:\n",
    "#     kfp_client.runs.delete_run(id=run.id)\n",
    "\n",
    "# # Delete the experiment\n",
    "# kfp_client.delete_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous KFP Run is completed successfully, you can run the KFP pipeline three more times to generate the monitoring results for data from the rest three quarters of 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quarter in range(2, 5):\n",
    "    arguments[\"quarter\"] = quarter\n",
    "    kfp_client.create_run_from_pipeline_func(\n",
    "        pipeline_func=monitoring_pipeline,\n",
    "        run_name=run_name,\n",
    "        experiment_name=experiment_name,\n",
    "        arguments=arguments, # These are the arguments passed to the pipeline function\n",
    "        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE, # KFP SDK has two versions (v1 and v2). We use v2 here\n",
    "        enable_caching=False # Disable caching for this pipeline, more details at https://www.kubeflow.org/docs/components/pipelines/v1/overview/caching-v2/\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these three KFP Runs are completed successfully, you should see the familiar dashboard showing the quarterly MAE changes in 2018 under an Evidently Project named \"house-price-model-monitoring\" at [http://evidently-monitor-ui.local](http://evidently-monitor-ui.local). You should also see there are four Reports and four Test Suites under the project. \n",
    "\n",
    "Next, please run the following code to check if your KPF Runs uploaded the correct monitoring results (Reports+Test Suites) to the remote Evidently Workspace. \n",
    "(There won't be outputs printed by the code cell. You pass the check if there is no error thrown.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = remote_workspace.search_project(project_name=\"house-price-model-monitoring\")\n",
    "project_id = projects[0].id\n",
    "\n",
    "# Query the uploaded Reports from the remote Evidently Workspace\n",
    "response = requests.get(f\"{EVIDENTLY_MONITOR_URL}/api/projects/{project_id}/reports\")\n",
    "report_snapshots = response.json()\n",
    "\n",
    "# There should be 4 Reports\n",
    "assert len(report_snapshots) == 4\n",
    "\n",
    "year = 2018\n",
    "\n",
    "# Check if each Report uses the required Presets and has the required tags\n",
    "report_snapshots.sort(key=lambda s: sorted(s[\"tags\"])[0])\n",
    "for i in range(4):\n",
    "    snapshot = report_snapshots[i]\n",
    "    assert set(snapshot[\"metadata\"][\"metric_presets\"]) == set(['RegressionPreset', 'TargetDriftPreset', 'DataDriftPreset'])\n",
    "    assert set(snapshot[\"tags\"]) == set([f\"{year}-quarter{i+1}\", f\"{registered_model_name}-{model_version}\"])\n",
    "\n",
    "response2 = requests.get(f\"{EVIDENTLY_MONITOR_URL}/api/projects/{project_id}/test_suites\")\n",
    "test_snapshots = response2.json()\n",
    "\n",
    "# There should also be 4 Test Suites\n",
    "assert len(test_snapshots) == 4\n",
    "\n",
    "# Check if each Test Suite has the required tags\n",
    "test_snapshots.sort(key=lambda s: sorted(s[\"tags\"])[0])\n",
    "for i in range(4):\n",
    "    snapshot = test_snapshots[i]\n",
    "    assert set(snapshot[\"tags\"]) == set([f\"{year}-quarter{i+1}\", f\"{registered_model_name}-{model_version}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots for Assignment3\n",
    "**Please include the dashboard showing the MAE changes in 2018 in your PDF file.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: The effect of retraining (2 points)\n",
    "In the final assignment, you'll gain an intuition of how retraining can affect model performance. You will experiment with two cases:\n",
    "1. No retraining: Use the same model to predict data for each quarter of each year (2016-2019). \n",
    "1. Retraining with all available data: If the MAE is larger than the threshold of 40000 for one quarter, retrain the model using all available data (the old training data + data of of all the quarters up to and including the current quarter). Then use the retrained model to make predictions for the next quarter. \n",
    "\n",
    "More detailed instructions can be found later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import etl\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MAE threshold that determines whether a model needs to be retrained\n",
    "THRESHOLD = 40000\n",
    "\n",
    "# The name of the directory where the raw data is saved\n",
    "# The data is the same as the one used in week2 but split on a quarterly basis\n",
    "RAW_DATA_DIR = \"raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data for the initial model\n",
    "# We pre-processed all data in raw_data/reference using the etl function and saved the preprocessed data in the \"feature_stored\" directory in advance\n",
    "x1 = pd.read_parquet(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_X.parquet\")\n",
    "y1 = pd.read_csv(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_y.csv\")\n",
    "\n",
    "# x2 and y2 were used as the testing data in previous assignments, here we also include it into the training data\n",
    "x2 = pd.read_parquet(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"1_0_X.parquet\")\n",
    "y2 = pd.read_csv(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"1_0_y.csv\")\n",
    "train_x = pd.concat([x1, x2])\n",
    "train_y = pd.concat([y1, y2])\n",
    "\n",
    "# Model hyperparameters (hyperparameter optimization was performed)\n",
    "params = {\n",
    "    \"colsample_bytree\" : 0.7,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    \"max_depth\" : 50, \n",
    "    \"min_child_samples\" : 5,\n",
    "    \"min_split_gain\" : 20.0,\n",
    "    \"n_estimators\" : 1000,\n",
    "    \"num_leaves\" : 100,\n",
    "    \"reg_lambda\" : 50.0,\n",
    "    \"subsample\" : 0.1,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Train the initial model\n",
    "model = LGBMRegressor(**params)\n",
    "model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the `etl` function to feature-engineer the data used in the experiments beforehand. The `etl` function is adapted from the one used in the second week. The code below will uses the `etl` function to feature-engineer the data for each quarter of years 2016, 2017, 2018, and 2019. The engineered feature Parquet files will be stored under the `feature_store` directory with a name of `<year>_<quarter>_X.parquet`. Similarly, the target CSV files will also be stored under the `feature_store` directory with a name of `<year>_<quarter>_y.csv`. For example, the engineered features of data for the first quarter of 2016 will be saved in \"2016_1_X.parquet\" and the corresponding targets in \"2016_1_y.csv\". \n",
    "\n",
    "Please note that there are only three quarters of data for 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = sorted([x for x in os.listdir(WORKING_DIR/RAW_DATA_DIR) if x != 'reference']) # ignore data in the \"reference\" directory as they were used to train the initial model and had already been feature_engineered\n",
    "for year in data_folders:\n",
    "    quarter_folders = sorted(os.listdir(WORKING_DIR/RAW_DATA_DIR/year))\n",
    "    for quarter in quarter_folders:\n",
    "        feature_file_name=f\"{year}_{quarter}_X.parquet\"\n",
    "        target_file_name=f\"{year}_{quarter}_y.csv\"\n",
    "        etl(\n",
    "            path=WORKING_DIR/RAW_DATA_DIR/year/quarter,\n",
    "            feature_store_path=WORKING_DIR/FEATURE_STORE_DIR_NAME,\n",
    "            feature_file_name=feature_file_name,\n",
    "            encoder_file_name=\"0_0.pkl\",\n",
    "            target_file_name=target_file_name,\n",
    "            fit_encoder=False,\n",
    "            targets_included=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) No retraining\n",
    "\n",
    "First, you only need to use the initial model to make predictions. For the data for every quarter of every year, you need to do the following:\n",
    "1. Reading the engineered features and targets from the corresponding files (`<year>_<quarter>_X.parquet` and `<year>_<quarter>_y.csv`, respectively) in the \"feature_store\" directory.\n",
    "1. Using the initial model you trained above to make predictions on the features.\n",
    "1. Calculating the MAE for each quarter of predictions. (You can assume that the targets and predictions are aligned, i.e., the $n^{th}$ target is the ground truth of the $n^{th}$ prediction.)\n",
    "1. Append the calculated MAE to the `mae_no_retrain` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_no_retrain = []\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Retraining using all available data\n",
    "\n",
    "Next, let's retrain our model when the MAE for a quarter is too large. Similar to what you did previously, for each quarter of each year, you need to:\n",
    "1. Read the engineered features and targets from the corresponding files in the \"feature_store\" directory.\n",
    "1. Use the latest retrained model to make predictions on the features. If there is no retrained model, then just use the initial model you trained before Assignment 4a).\n",
    "1. Calculate the MAE for the quarterly predictions. (You can assume that the targets and predictions are aligned, i.e., the $n^{th}$ target is the ground truth of the $n^{th}$ prediction.)\n",
    "1. Append the MAE to the `mae_retrain_on_all` list. \n",
    "1. If the MAE is larger than the threshold of 40000, retrain the model using all available data up to this point. For example, if the MAE for the third quarter of 2016 is too large, then you can combine the initial `train_x` and the features in `feature_store/2016/{1,2,3}_X.parquet` (Same applies to the targets.) to retrain the model. You can then use the retrained model to make predictions for the forth quarter. No hyperparameter optimization is needed. You can use the hyperparameters for training the initial model to retrain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_retrain_on_all = []\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the code provided below to generate a figure, which shows the MAE for each quarter of each year in both cases. The resulted figure should be similar to the one below:\n",
    "\n",
    "<img src=\"./images/retrain_output.png\" width=600 /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the figure\n",
    "default_x_ticks = list(range(len(mae_no_retrain)))\n",
    "\n",
    "# Values for x-axis\n",
    "x_sticks = []\n",
    "for year in data_folders:\n",
    "    quarter_folders = sorted(os.listdir(WORKING_DIR/RAW_DATA_DIR/year))\n",
    "    for q in quarter_folders:\n",
    "        x_sticks.append(f\"{year}/{q}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(default_x_ticks, mae_no_retrain, label=\"No retraining\")\n",
    "plt.plot(default_x_ticks, mae_retrain_on_all, label=\"Retraining on all data\", color=\"g\")\n",
    "plt.axhline(y=THRESHOLD, color='grey', linestyle = '--', label = f\"Threshold = {THRESHOLD}\")\n",
    "plt.xlabel(\"quarter\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xticks(default_x_ticks, x_sticks)\n",
    "plt.locator_params(axis='x', nbins=8)\n",
    "plt.title(\"Performance per quarter\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please make sure you have the following files in your submission:\n",
    "- This notebook (with up-to-date outputs of the code cells)\n",
    "- prometheus-conf-patch.yaml\n",
    "- A PDF containing the required screenshots of Assignments 1, 2 and 3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
