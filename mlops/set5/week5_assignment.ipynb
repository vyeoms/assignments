{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55e65fa",
   "metadata": {},
   "source": [
    "# Week5 Assignments \n",
    "\n",
    "In this week's assignments, you will build and run a KFP pipeline that trains and deploys a LightGBM regression model to predict public bike sharing demand. You will first create some KFP components following the instructions below and then create a KFP pipeline using your KFP components. \n",
    "\n",
    "**Guidelines of submitting the assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments.\n",
    "- The final assignment also requires you to capture screenshots in order to earn points. Please put your screenshots into a single PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are up-to-date and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your screenshots in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62326c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset\n",
    ")\n",
    "from typing import NamedTuple, Dict, Any, Tuple\n",
    "\n",
    "from unittest.mock import create_autospec\n",
    "import pandas as pd\n",
    "import os\n",
    "from send_requests import send_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a440ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to KFP client, remember to ensure you're using the correct kubectl context\n",
    "client = kfp.Client(host=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ca",
   "metadata": {},
   "source": [
    "## Assignment 1: Create KFP components (10 points)\n",
    "You will need to create five KFP components, each KFP component gives 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf427987",
   "metadata": {},
   "source": [
    "### 1a) Create a pull data component\n",
    "\n",
    "This KFP component:\n",
    "\n",
    "1) downloads the dataset (a CSV file) as a Pandas DataFrame from a URL given as an input;\n",
    "\n",
    "2) saves the DataFrame to an output of type Dataset so that the dataset can be used by other KFP components.\n",
    "\n",
    "The dataset can be found [here](https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv). It's the same bike sharing demand dataset used in some of the previous weeks. Below is the explanation of each columns in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| datetime    | hourly date + timestamp| object\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise is 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| casual      | number of non-registered user rentals initiated | integer\n",
    "| registered  | number of registered user rentals initiated     | integer\n",
    "| count       | number of total rentals                         | integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad48f3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce4c027e3dec1267",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\"],\n",
    ")\n",
    "def pull_data(url: str, data: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: Dataset URL\n",
    "        data: Output of type Dataset where the downloaded dataset is saved\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there isn't a simply way to directly test the KFP component created by the @component decorator. A workaround is to access the inner decorated Python function through the component attribute `python_func`. \n",
    "\n",
    "Let's test if your `pull_data` works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock an output Dataset\n",
    "raw_dataset = create_autospec(Dataset, metadata=dict(), path=\"raw_data.csv\")\n",
    "\n",
    "pull_data.python_func(\n",
    "    url=\"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\", \n",
    "    data=raw_dataset)\n",
    "\n",
    "raw_df = pd.read_csv(\"raw_data.csv\")\n",
    "print(f\"The shape of the downloaded DataFrame should be {raw_df.shape}.\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "The shape of the downloaded DataFrame should be (10886, 12).\n",
    "\n",
    "![](./images/raw-df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b40825",
   "metadata": {},
   "source": [
    "### 1b) Create a data preprocessing component\n",
    "\n",
    "This KFP component: \n",
    "\n",
    "1. reads a dataset from an input of type Dataset;\n",
    "\n",
    "1. converts the \"datetime\" column into Pandas datetime object;\n",
    "\n",
    "1. creates three more features (hour, day and month) from the \"datetime\" column;\n",
    "\n",
    "1. removes the \"datetime\", \"casual\" and \"registered\" columns; \n",
    "\n",
    "1. splits the dataset into a training and a test dataset, using the last week of the dataset as the test dataset;\n",
    "\n",
    "1. saves the training and test datasets into two separate outputs of type Dataset.\n",
    "\n",
    "The dataset contains data from two years and the data were generated on an hourly basis. \n",
    "\n",
    "The resulted datasets should have 12 columns: season, holiday, workingday, weather, temp, atemp, humidity, windspeed, casual, registered, count, hour, day, and month. For example:\n",
    "\n",
    "![](./images/train-df.png)\n",
    "\n",
    "The following links may be helpful:\n",
    "- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.to_datetime.html)\n",
    "- [pandas.Series.dt.*](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.dt.hour.html)\n",
    "- [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a89f67",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a3c593a5b0f13726",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\"],\n",
    ")\n",
    "def preprocess_data(\n",
    "    data: Input[Dataset],\n",
    "    train_set: Output[Dataset],\n",
    "    test_set: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        data: Input of type Dataset where the dataset is read from\n",
    "        train_set: Output of type Dataset where the training dataset is saved\n",
    "        test_set: Output of type Dataset where the test dataset is saved  \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    " \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_autospec(Dataset, metadata=dict(), path=\"train_data.csv\")\n",
    "test_dataset = create_autospec(Dataset, metadata=dict(), path=\"test_data.csv\")\n",
    "\n",
    "preprocess_data.python_func(data=raw_dataset, train_set=train_dataset, test_set=test_dataset)\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "print(f\"The training DataFrame should have a shape of {train_df.shape} and the following columns: {train_df.columns}\")\n",
    "print(f\"The testing DataFrame should have a shape of {test_df.shape} and the following columns: {test_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "The training DataFrame should have a shape of (10718, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count', 'hour', 'day', 'month'], dtype='object')\n",
    "The testing DataFrame should have a shape of (168, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count', 'hour', 'day', 'month'], dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076ef62",
   "metadata": {},
   "source": [
    "### 1c) Create a train component\n",
    "\n",
    "This component \n",
    "\n",
    "1) loads the training and test datasets from two inputs of type Dataset, respectively;\n",
    "\n",
    "2) uses the training dataset and the given hyperparameters to train a [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html). The hyperparameters are given in a dictionary, e.g., `{\"num_leaves\": 1023, \"learning_rate\": 0.05}`. In the dictionary, each key is an argument name accepted by the LGBMRegressor class and the value is the value assigned to the corresponding argument;\n",
    "\n",
    "3) evaluates the trained model against the testing dataset, the evaluation metrics to be used are **mean squared error, mean absolute error, r2_score**;\n",
    "\n",
    "4) logs the configured hyperparameters and evaluation metrics to an MLflow Run. The following information is given as the component inputs: the name of the MLflow Experiment under which the MLflow Run should be stored, the URIs of the MLflow service and the artifact store;\n",
    "\n",
    "5) registers the model to MLflow, the artifact path relative to the MLflow Run is also given as an input.\n",
    "\n",
    "**Note**:\n",
    "- When logging hyperparameters, you can use the keys of the dictionary as the parameter names.\n",
    "- When logging metrics, please use **\"rmse\", \"mae\", \"r2\"** as the metric names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0d3e8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d66d96e115368fc0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\", \"numpy\", \"lightgbm~=3.3.5\", \"scikit-learn~=1.3.2\", \"mlflow==2.3.2\", \"boto3~=1.28.85\"],\n",
    ")\n",
    "def train(\n",
    "    train_set: Input[Dataset],\n",
    "    test_set: Input[Dataset],\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_artifact_path: str,\n",
    "    hyperparams: Dict[str, Any],\n",
    "    target: str = \"count\"\n",
    ") -> NamedTuple(\"Output\", [(\"storage_uri\", str), (\"run_id\", str),]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_set: Input where the training dataset is saved\n",
    "        test_set: Input where the test dataset is saved\n",
    "        mlflow_experiment_name: Name of the MLflow experiment\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        model_artifact_path: The path where the artifacts of the model are stored in MLflow's artifact store. It's relative to the MLflow Run.\n",
    "        hyperparams: Hyperparameters that need to be configured. The hyperparameters will be passed as a dictionary like {\"num_leaves\": 1023, \"learning_rate\": 0.05}\n",
    "        target: Target column name\n",
    "    \n",
    "    Returns: \n",
    "        namedtuple(\"Output\", [\"storage_uri\", \"run_id\"]) where storage_uri (e.g., s3://mlflow/13/e5559bc.../artifacts/bike-demand) is the S3 URI of the saved model in\n",
    "        Mlflow's artifact store and run_id the ID of the MLflow run that produces the model\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are needed for testing the function outside the MLOps platform\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "train_output = train.python_func(\n",
    "    train_set=train_dataset,\n",
    "    test_set=test_dataset,\n",
    "    mlflow_experiment_name=\"test\",\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    mlflow_s3_endpoint_url=\"http://mlflow-minio.local\",\n",
    "    model_artifact_path=\"test-model\",\n",
    "    hyperparams={\"learning_rate\": 0.2, \"num_leaves\": 63, \"random_state\": 42},\n",
    "    target=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "Output(storage_uri='s3://mlflow/<MLflow-Experiment-ID>/<MLflow-Run-ID>/artifacts/test-model', run_id=<MLflow-Run-ID>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At [http://mlflow-server.local](http://mlflow-server.local), you should see an MLflow Experiment named \"test\" and there is an MLflow Run under the \"test\" experiment. You should also see a registered model associated to the MLflow Run:\n",
    "\n",
    "<img src=\"./images/mlflow-test-run.png\" width=\"1000\" />\n",
    "<img src=\"./images/mlflow-test-model.png\" width=\"1000\" />\n",
    "\n",
    "The metric values may vary depending on how your model was trained. The key point is that all hyperparameters and metrics are logged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c217876",
   "metadata": {},
   "source": [
    "### 1d) Create a validate component\n",
    "\n",
    "This component \n",
    "\n",
    "1) uses an MLflow Run ID to fetch the evaluation metrics logged at that Run from the MLflow service. The MLflow Run ID and the URI of the MLflow service are given as inputs;\n",
    "\n",
    "2) compares the evaluation metrics with a set of thresholds specified in a dictionary, e.g., `{\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}`. The threshold dictionary has three keys \"rmse\", \"mae\", and \"r2\", which correspond to the metric names used when the `train` component logs the metrics to MLflow. For each key in the dictionary, a tuple is provided as its value. The first element of the tuple represents the threshold, while the second element indicates the optimization direction of the metric. To pass the validation, a metric should be below its threshold when the optimization direction is \"minimize\" or above its threshold when the direction is \"maximize\". If any metric fails to meet these criteria, the validation fails.\n",
    "\n",
    "The component should return True if the validation succeeds, otherwise false. \n",
    "\n",
    "Hint: You can use [get_run()](https://mlflow.org/docs/2.3.2/python_api/mlflow.client.html?highlight=mlflowclient#mlflow.client.MlflowClient.get_run) th find an MLflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d1d41",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d7620a9ec1bac271",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"numpy\", \"mlflow==2.3.2\"],\n",
    ")\n",
    "def validate(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    threshold_metrics: Dict[str, Tuple[int, str]]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        run_id: MLflow run ID, by which the evaluation metrics can be fetched\n",
    "        mlflow_tracking_uri: URI of the MLflow tracking service,\n",
    "        threshold_metrics: Minimum threshold values for each metric\n",
    "    Returns:\n",
    "        Boolean indicating whether the metrics pass the threshold or not. True means all metrics are smaller than \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loose threshold to guarantee a successful validation\n",
    "validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}\n",
    "\n",
    "# A strict threshold so that the validation will fail\n",
    "strict_validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.99999, \"maximize\"), \"mae\": (0.1, \"minimize\")}\n",
    "\n",
    "validation_output = validate.python_func(\n",
    "    run_id=train_output.run_id,\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    threshold_metrics=validate_threshold_metrics\n",
    ")\n",
    "print(validation_output) #should be True\n",
    "\n",
    "validation_output_strict = validate.python_func(\n",
    "    run_id=train_output.run_id,\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    threshold_metrics=strict_validate_threshold_metrics\n",
    ")\n",
    "print(validation_output_strict) #should be False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aaff4",
   "metadata": {},
   "source": [
    "### 1e) Create a deploy model component\n",
    "\n",
    "This component uses KServe Python SDK to deploy the trained model to KServe in the \"kserve-inference\" namespace. The component should create a new inference service or update an existing one. In the tutorial, you may notice that the component used for model deployment may be completed though the deployed inference service is not yet ready. Here, the component should remain running until the status of the deployed inference service is ready or a timeout of 6 minutes is reached.\n",
    "\n",
    "The name of the inference service and the S3 URI of the model are given as inputs.\n",
    "\n",
    "**Hint**: \n",
    "- Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model saved by MLflow.\n",
    "- [kserve.wait_isvc_ready](https://kserve.github.io/website/0.10/sdk_docs/docs/KServeClient/#wait_isvc_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7862cce",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7f76c1a21f272626",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"kserve==0.10.1\"],\n",
    ")\n",
    "def deploy_model(model_name: str, storage_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        storage_uri: the URI of the saved model in MLflow's artifact store\n",
    "    \"\"\"\n",
    "    from kubernetes import client\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ModelSpec\n",
    "    from kserve import V1beta1ModelFormat\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name = \"kserve-sa\"\n",
    "    api_version = constants.KSERVE_V1BETA1\n",
    "    logger.info(f\"MODEL URI: {storage_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=storage_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model.python_func(model_name=\"test-bike-demand\", storage_uri=train_output.storage_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"test-bike-demand\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc test-bike-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the inference service\n",
    "send_requests(isvc_name=\"test-bike-demand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{\"model_name\":\"test-bike-demand\",\"model_version\":null,\"id\":\"4a592a3e-bfe3-46b4-a07b-c8dc87954b67\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[42.1026868829854,30.14696834831473]}]}\n",
    "```\n",
    "The ID may vary. The prediction values may also vary depending on how your model was trained. The key point is that your output follows the same format as the expected one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the testing inference service\n",
    "!kubectl -n kserve-inference delete isvc test-bike-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19238798",
   "metadata": {},
   "source": [
    "## Assignment 2:  Create a KFP pipeline (2 points)\n",
    "You need to create a KFP pipeline using all the KFP components you created. \n",
    "\n",
    "The KFP components should perform their tasks in the following order: \n",
    "\n",
    "<img src=\"./images/kfp-assignment.jpg\" />\n",
    "\n",
    "(The other inputs needed by the components are provided as arguments of the `pipeline` function.)\n",
    "\n",
    "The **deploy model component** should perform its task only if the validation task succeeds. You can use [dsl.Condition](https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/control-flow/) to manage conditions in Kubeflow Pipelines. \n",
    "\n",
    "**Hints**: \n",
    "- You need to assign the needed credentials to the model training task so that the task can upload artifacts to MLflow's artifact store (MinIO). (Please check the tutorial for more details.) \n",
    "- We recall that if a component has a single Parameter output, this output can be accessed by `task.output`.\n",
    "- A boolean output will be converted to a string by Kubeflow Pipelines. For example, if there is component that returns a boolean as its output:\n",
    "```python\n",
    "@component()\n",
    "def component1():\n",
    "    if ...:\n",
    "        return True\n",
    "    return False\n",
    "task1 = component1()\n",
    "```\n",
    "then `task1.output` is the string \"true\" instead of the boolean True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd5f48",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-66fecedfd2224305",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"bike-demand-pipeline\",\n",
    "    description=\"An example pipeline that deploys a model for bike demanding prediction\"\n",
    ")\n",
    "def pipeline(\n",
    "    url: str,\n",
    "    target: str,\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_name: str,\n",
    "    hyperparams: Dict[str, Any],\n",
    "    threshold_metrics: Dict[str, Tuple[int, str]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: URL for downloading the dataset\n",
    "        target: Target column name of the dataset\n",
    "        mlflow_experiment_name: Name of the MLflow experiment\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        model_name: The name of the KServe inference service. It's also used as the model's artifact path\n",
    "        hyperparams: A dictionary of hyperparameters for training the LightGBM model, same as the one used by the train component\n",
    "        threshold_metrics: A dictionary of threshold values for each metric, same as the one used by the validate component \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144575f9",
   "metadata": {},
   "source": [
    "## Submit a KFP run\n",
    "After defining your KFP pipeline, you can test it by triggering a KFP run of your KFP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify pipeline argument values\n",
    "\n",
    "# A loose threshold to guarantee a successful validation\n",
    "validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}\n",
    "\n",
    "# A strict threshold so that the validation will fail\n",
    "strict_validate_threshold_metrics = {\"rmse\": (0.1, \"minimize\"), \"r2\": (0.99999, \"maximize\"), \"mae\": (0.1, \"minimize\")}\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train.csv\"\n",
    "\n",
    "arguments = {\n",
    "    \"url\": url,\n",
    "    \"target\": \"count\",\n",
    "    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\n",
    "    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\n",
    "    \"mlflow_experiment_name\": \"bike-notebook\",\n",
    "    \"model_name\": \"bike-demand\",\n",
    "    \"hyperparams\": {\"learning_rate\": 0.2, \n",
    "                    \"num_leaves\": 63, \n",
    "                    \"random_state\": 42},\n",
    "    \"threshold_metrics\": validate_threshold_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"bike-run\"\n",
    "experiment_name = \"bike-experiment\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abc9ed",
   "metadata": {},
   "source": [
    "Go to [http://ml-pipeline-ui.local](http://ml-pipeline-ui.local) to check the running state of the KFP run. Also go to [http://mlflow-server.local](http://mlflow-server.local) to check the created MLflow experiment and the registered model.\n",
    "\n",
    "Please capture the following screenshots and put them into your PDF file:\n",
    "\n",
    "1) The Kubeflow Pipelines UI of a succeeded KFP run of this KFP pipeline.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/kfp-run-ok.png\" width=400/>\n",
    "</details>\n",
    "\n",
    "2) The MLflow UI of the created MLflow experiment, including the logged hyperparameters and evaluation metrics.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-log.png\" width=1000/>\n",
    "</details>\n",
    "\n",
    "3) The MLflow UI of the registered model.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-model.png\" width=1000/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cf070",
   "metadata": {},
   "source": [
    "#### A hint for debugging\n",
    "\n",
    "Your KFP run may fail to complete because of bugs in some of the components. You can further investigate the problems by looking into the component logs:\n",
    "\n",
    "![](./images/kfp-debug.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0ebe8",
   "metadata": {},
   "source": [
    "## Testing the inference service\n",
    "After the KFP run in completed, let's ensure that it's ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc0bef",
   "metadata": {},
   "source": [
    "After your model is ready in KServe, you can send a request to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc69b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_requests(isvc_name=\"bike-demand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{\"model_name\":\"bike-demand\",\"model_version\":null,\"id\":\"093f23c2-dd69-4976-8b96-bfd4944e1294\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[23.743062591411174,35.72844242242732]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f7af9",
   "metadata": {},
   "source": [
    "## Another run of the pipeline\n",
    "\n",
    "Now you let's use the \"strict_validate_threshold_metrics\" as the threshold metrics and start another KFP run of your KFP pipeline. This time the KFP run will skip the deployment task because of the failed validation task. Capture a screenshot of the KFP run where the deployment task is skipped and put this screenshot into your PDF file.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/kfp-run-skip-deploy.png\" width=500/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"threshold_metrics\"] = strict_validate_threshold_metrics\n",
    "\n",
    "run_name = \"bike-run-incomplete\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In summary, your PDF file should have the following four screenshots: \n",
    "\n",
    "1) the Kubeflow Pipelines UI showing a succeeded KFP run of this KFP pipeline\n",
    "2) the MLflow UI of the created MLflow experiment\n",
    "3) the MLflow UI of the registered model\n",
    "4) the Kubeflow PIpelines UI of a KFP run where the deployment task is skipped due to the failed validation. \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
