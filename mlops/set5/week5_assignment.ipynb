{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55e65fa",
   "metadata": {},
   "source": [
    "# Week5 Assignments \n",
    "\n",
    "In this week's assignments, you will build and run a KFP pipeline that trains and deploys a LightGBM regression model to predict public bike sharing demand. You will first create some KFP components following the instructions below and then create a KFP pipeline using your KFP components. \n",
    "\n",
    "**Guidelines of submitting the assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments.\n",
    "- The final assignment also requires you to capture screenshots in order to earn points. Please put your screenshots into a single PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are up-to-date and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your screenshots in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62326c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset\n",
    ")\n",
    "from typing import NamedTuple, Dict, Any, Tuple\n",
    "\n",
    "from unittest.mock import create_autospec\n",
    "import pandas as pd\n",
    "import os\n",
    "from send_requests import send_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a440ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to KFP client, remember to ensure you're using the correct kubectl context\n",
    "client = kfp.Client(host=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ca",
   "metadata": {},
   "source": [
    "## Assignment 1: Create KFP components (10 points)\n",
    "You will need to create five KFP components, each KFP component gives 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf427987",
   "metadata": {},
   "source": [
    "### 1a) Create a pull data component\n",
    "\n",
    "This KFP component:\n",
    "\n",
    "1) downloads the dataset (a CSV file) as a Pandas DataFrame from a URL given as an input;\n",
    "\n",
    "2) saves the DataFrame to an output of type Dataset so that the dataset can be used by other KFP components.\n",
    "\n",
    "The dataset can be found [here](https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv). It's the same bike sharing demand dataset used in some of the previous weeks. Below is the explanation of each columns in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| datetime    | hourly date + timestamp| object\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise is 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| casual      | number of non-registered user rentals initiated | integer\n",
    "| registered  | number of registered user rentals initiated     | integer\n",
    "| count       | number of total rentals                         | integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fad48f3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce4c027e3dec1267",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\"],\n",
    ")\n",
    "def pull_data(url: str, data: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: Dataset URL\n",
    "        data: Output of type Dataset where the downloaded dataset is saved\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(url, sep=\",\")\n",
    "    df.to_csv(data.path, index=None)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there isn't a simply way to directly test the KFP component created by the @component decorator. A workaround is to access the inner decorated Python function through the component attribute `python_func`. \n",
    "\n",
    "Let's test if your `pull_data` works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the downloaded DataFrame should be (10886, 12).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mock an output Dataset\n",
    "raw_dataset = create_autospec(Dataset, metadata=dict(), path=\"raw_data.csv\")\n",
    "\n",
    "pull_data.python_func(\n",
    "    url=\"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\", \n",
    "    data=raw_dataset)\n",
    "\n",
    "raw_df = pd.read_csv(\"raw_data.csv\")\n",
    "print(f\"The shape of the downloaded DataFrame should be {raw_df.shape}.\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "The shape of the downloaded DataFrame should be (10886, 12).\n",
    "\n",
    "![](./images/raw-df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b40825",
   "metadata": {},
   "source": [
    "### 1b) Create a data preprocessing component\n",
    "\n",
    "This KFP component: \n",
    "\n",
    "1. reads a dataset from an input of type Dataset;\n",
    "\n",
    "1. converts the \"datetime\" column into Pandas datetime object;\n",
    "\n",
    "1. creates three more features (hour, day and month) from the \"datetime\" column;\n",
    "\n",
    "1. removes the \"datetime\", \"casual\" and \"registered\" columns; \n",
    "\n",
    "1. splits the dataset into a training and a test dataset, using the last week of the dataset as the test dataset;\n",
    "\n",
    "1. saves the training and test datasets into two separate outputs of type Dataset.\n",
    "\n",
    "The dataset contains data from two years and the data were generated on an hourly basis. \n",
    "\n",
    "The resulted datasets should have 12 columns: season, holiday, workingday, weather, temp, atemp, humidity, windspeed, casual, registered, count, hour, day, and month. For example:\n",
    "\n",
    "![](./images/train-df.png)\n",
    "\n",
    "The following links may be helpful:\n",
    "- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.to_datetime.html)\n",
    "- [pandas.Series.dt.*](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.dt.hour.html)\n",
    "- [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a89f67",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a3c593a5b0f13726",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\"],\n",
    ")\n",
    "def preprocess_data(\n",
    "    data: Input[Dataset],\n",
    "    train_set: Output[Dataset],\n",
    "    test_set: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        data: Input of type Dataset where the dataset is read from\n",
    "        train_set: Output of type Dataset where the training dataset is saved\n",
    "        test_set: Output of type Dataset where the test dataset is saved  \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(data.path)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"month\"] = df[\"datetime\"].dt.month\n",
    "    df = df.drop([\"datetime\", \"casual\", \"registered\"], axis=1)\n",
    "    train, test = df.iloc[:-7*24], df.iloc[-7*24:]\n",
    "    train.to_csv(train_set.path, index=None)\n",
    "    test.to_csv(test_set.path, index=None)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training DataFrame should have a shape of (10718, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'count', 'hour', 'day', 'month'],\n",
      "      dtype='object')\n",
      "The testing DataFrame should have a shape of (168, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'count', 'hour', 'day', 'month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_autospec(Dataset, metadata=dict(), path=\"train_data.csv\")\n",
    "test_dataset = create_autospec(Dataset, metadata=dict(), path=\"test_data.csv\")\n",
    "\n",
    "preprocess_data.python_func(data=raw_dataset, train_set=train_dataset, test_set=test_dataset)\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "print(f\"The training DataFrame should have a shape of {train_df.shape} and the following columns: {train_df.columns}\")\n",
    "print(f\"The testing DataFrame should have a shape of {test_df.shape} and the following columns: {test_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "The training DataFrame should have a shape of (10718, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count', 'hour', 'day', 'month'], dtype='object')\n",
    "The testing DataFrame should have a shape of (168, 12) and the following columns: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count', 'hour', 'day', 'month'], dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076ef62",
   "metadata": {},
   "source": [
    "### 1c) Create a train component\n",
    "\n",
    "This component \n",
    "\n",
    "1) loads the training and test datasets from two inputs of type Dataset, respectively;\n",
    "\n",
    "2) uses the training dataset and the given hyperparameters to train a [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html). The hyperparameters are given in a dictionary, e.g., `{\"num_leaves\": 1023, \"learning_rate\": 0.05}`. In the dictionary, each key is an argument name accepted by the LGBMRegressor class and the value is the value assigned to the corresponding argument;\n",
    "\n",
    "3) evaluates the trained model against the testing dataset, the evaluation metrics to be used are **mean squared error, mean absolute error, r2_score**;\n",
    "\n",
    "4) logs the configured hyperparameters and evaluation metrics to an MLflow Run. The following information is given as the component inputs: the name of the MLflow Experiment under which the MLflow Run should be stored, the URIs of the MLflow service and the artifact store;\n",
    "\n",
    "5) registers the model to MLflow, the artifact path relative to the MLflow Run is also given as an input.\n",
    "\n",
    "**Note**:\n",
    "- When logging hyperparameters, you can use the keys of the dictionary as the parameter names.\n",
    "- When logging metrics, please use **\"rmse\", \"mae\", \"r2\"** as the metric names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd0d3e8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d66d96e115368fc0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas~=1.5.3\", \"numpy\", \"lightgbm~=3.3.5\", \"scikit-learn~=1.3.2\", \"mlflow==2.3.2\", \"boto3~=1.28.85\"],\n",
    ")\n",
    "def train(\n",
    "    train_set: Input[Dataset],\n",
    "    test_set: Input[Dataset],\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_artifact_path: str,\n",
    "    hyperparams: Dict[str, Any],\n",
    "    target: str = \"count\"\n",
    ") -> NamedTuple(\"Output\", [(\"storage_uri\", str), (\"run_id\", str),]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_set: Input where the training dataset is saved\n",
    "        test_set: Input where the test dataset is saved\n",
    "        mlflow_experiment_name: Name of the MLflow experiment\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        model_artifact_path: The path where the artifacts of the model are stored in MLflow's artifact store. It's relative to the MLflow Run.\n",
    "        hyperparams: Hyperparameters that need to be configured. The hyperparameters will be passed as a dictionary like {\"num_leaves\": 1023, \"learning_rate\": 0.05}\n",
    "        target: Target column name\n",
    "    \n",
    "    Returns: \n",
    "        namedtuple(\"Output\", [\"storage_uri\", \"run_id\"]) where storage_uri (e.g., s3://mlflow/13/e5559bc.../artifacts/bike-demand) is the S3 URI of the saved model in\n",
    "        Mlflow's artifact store and run_id the ID of the MLflow run that produces the model\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    import os\n",
    "    import logging\n",
    "    from collections import namedtuple\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "    \n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = mlflow_s3_endpoint_url\n",
    "\n",
    "    train = pd.read_csv(train_set.path)\n",
    "    test = pd.read_csv(test_set.path)\n",
    "\n",
    "    train_x = train.drop([target], axis=1)\n",
    "    test_x = test.drop([target], axis=1)\n",
    "    train_y = train[[target]]\n",
    "    test_y = test[[target]]\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Run ID: {run_id}\")\n",
    "        model = LGBMRegressor(**hyperparams)\n",
    "        \n",
    "        logger.info(\"Fitting model...\")\n",
    "        model.fit(train_x, train_y)\n",
    "        \n",
    "        logger.info(\"Predicting...\")\n",
    "        predicted_qualities = model.predict(test_x)\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "        logger.info(\"RMSE: %s\" % rmse)\n",
    "        logger.info(\"MAE: %s\" % mae)\n",
    "        logger.info(\"R2: %s\" % r2)\n",
    "        \n",
    "        logger.info(\"Logging parameters and metrics to MLflow\")\n",
    "        mlflow.log_params(hyperparams)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        \n",
    "        logger.info(\"Logging trained model\")\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=model_artifact_path,\n",
    "            registered_model_name=\"LGBMweek5\",\n",
    "            serialization_format=\"pickle\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Logging predictions artifact to MLflow\")\n",
    "        np.save(\"predictions\", predicted_qualities)\n",
    "        mlflow.log_artifact(\n",
    "            local_path=\"predictions.npy\",\n",
    "            artifact_path=\"predicted_qualities\"\n",
    "        )\n",
    "        \n",
    "        # Prepare output\n",
    "        output = namedtuple(\"Output\", [\"storage_uri\", \"run_id\"])\n",
    "\n",
    "        # use get_artifact_uri to get the absolute S3 URI (s3://mlflow/<mlflow-experiment-id>/<mlflow-run-id>/artifacts/<model_artifact_path>)\n",
    "        return output(mlflow.get_artifact_uri(artifact_path=model_artifact_path), run_id)\n",
    "    ### END CODE HERE\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 17:11:39 INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.\n",
      "INFO:__main__:Run ID: f06ce79ce7694abbb224dc0a7ae4a564\n",
      "INFO:__main__:Fitting model...\n",
      "INFO:__main__:Predicting...\n",
      "INFO:__main__:RMSE: 79.49345042990697\n",
      "INFO:__main__:MAE: 58.89191297473423\n",
      "INFO:__main__:R2: 0.7801542541021345\n",
      "INFO:__main__:Logging parameters and metrics to MLflow\n",
      "INFO:__main__:Logging trained model\n",
      "/home/user/anaconda3/envs/mlops_eng/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "Successfully registered model 'LGBMweek5'.\n",
      "2023/12/07 17:11:55 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBMweek5, version 1\n",
      "Created version '1' of model 'LGBMweek5'.\n",
      "INFO:__main__:Logging predictions artifact to MLflow\n"
     ]
    }
   ],
   "source": [
    "# These are needed for testing the function outside the MLOps platform\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "train_output = train.python_func(\n",
    "    train_set=train_dataset,\n",
    "    test_set=test_dataset,\n",
    "    mlflow_experiment_name=\"test\",\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    mlflow_s3_endpoint_url=\"http://mlflow-minio.local\",\n",
    "    model_artifact_path=\"test-model\",\n",
    "    hyperparams={\"learning_rate\": 0.2, \"num_leaves\": 63, \"random_state\": 42},\n",
    "    target=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(storage_uri='s3://mlflow/8/f06ce79ce7694abbb224dc0a7ae4a564/artifacts/test-model', run_id='f06ce79ce7694abbb224dc0a7ae4a564')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "Output(storage_uri='s3://mlflow/<MLflow-Experiment-ID>/<MLflow-Run-ID>/artifacts/test-model', run_id=<MLflow-Run-ID>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At [http://mlflow-server.local](http://mlflow-server.local), you should see an MLflow Experiment named \"test\" and there is an MLflow Run under the \"test\" experiment. You should also see a registered model associated to the MLflow Run:\n",
    "\n",
    "<img src=\"./images/mlflow-test-run.png\" width=\"1000\" />\n",
    "<img src=\"./images/mlflow-test-model.png\" width=\"1000\" />\n",
    "\n",
    "The metric values may vary depending on how your model was trained. The key point is that all hyperparameters and metrics are logged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c217876",
   "metadata": {},
   "source": [
    "### 1d) Create a validate component\n",
    "\n",
    "This component \n",
    "\n",
    "1) uses an MLflow Run ID to fetch the evaluation metrics logged at that Run from the MLflow service. The MLflow Run ID and the URI of the MLflow service are given as inputs;\n",
    "\n",
    "2) compares the evaluation metrics with a set of thresholds specified in a dictionary, e.g., `{\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}`. The threshold dictionary has three keys \"rmse\", \"mae\", and \"r2\", which correspond to the metric names used when the `train` component logs the metrics to MLflow. For each key in the dictionary, a tuple is provided as its value. The first element of the tuple represents the threshold, while the second element indicates the optimization direction of the metric. To pass the validation, a metric should be below its threshold when the optimization direction is \"minimize\" or above its threshold when the direction is \"maximize\". If any metric fails to meet these criteria, the validation fails.\n",
    "\n",
    "The component should return True if the validation succeeds, otherwise false. \n",
    "\n",
    "Hint: You can use [get_run()](https://mlflow.org/docs/2.3.2/python_api/mlflow.client.html?highlight=mlflowclient#mlflow.client.MlflowClient.get_run) th find an MLflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8d1d41",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d7620a9ec1bac271",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"numpy\", \"mlflow==2.3.2\"],\n",
    ")\n",
    "def validate(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    threshold_metrics: Dict[str, Tuple[int, str]]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        run_id: MLflow run ID, by which the evaluation metrics can be fetched\n",
    "        mlflow_tracking_uri: URI of the MLflow tracking service,\n",
    "        threshold_metrics: Minimum threshold values for each metric\n",
    "    Returns:\n",
    "        Boolean indicating whether the metrics pass the threshold or not. True means all metrics are smaller than \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    from mlflow import MlflowClient\n",
    "\n",
    "    client = MlflowClient(mlflow_tracking_uri)\n",
    "    run_metrics = client.get_run(run_id).data.to_dictionary()['metrics']\n",
    "    success = True\n",
    "    for metric in threshold_metrics:\n",
    "        model_metric = run_metrics[metric]\n",
    "        threshold, direction = threshold_metrics[metric]\n",
    "        if direction == \"minimize\":\n",
    "            success = success & (model_metric <= threshold)\n",
    "        else:\n",
    "            success = success & (model_metric >= threshold)\n",
    "    return success\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# A loose threshold to guarantee a successful validation\n",
    "validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}\n",
    "\n",
    "# A strict threshold so that the validation will fail\n",
    "strict_validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.99999, \"maximize\"), \"mae\": (0.1, \"minimize\")}\n",
    "\n",
    "validation_output = validate.python_func(\n",
    "    run_id=train_output.run_id,\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    threshold_metrics=validate_threshold_metrics\n",
    ")\n",
    "print(validation_output) #should be True\n",
    "\n",
    "validation_output_strict = validate.python_func(\n",
    "    run_id=train_output.run_id,\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    threshold_metrics=strict_validate_threshold_metrics\n",
    ")\n",
    "print(validation_output_strict) #should be False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aaff4",
   "metadata": {},
   "source": [
    "### 1e) Create a deploy model component\n",
    "\n",
    "This component uses KServe Python SDK to deploy the trained model to KServe in the \"kserve-inference\" namespace. The component should create a new inference service or update an existing one. In the tutorial, you may notice that the component used for model deployment may be completed though the deployed inference service is not yet ready. Here, the component should remain running until the status of the deployed inference service is ready or a timeout of 6 minutes is reached.\n",
    "\n",
    "The name of the inference service and the S3 URI of the model are given as inputs.\n",
    "\n",
    "**Hint**: \n",
    "- Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model saved by MLflow.\n",
    "- [kserve.wait_isvc_ready](https://kserve.github.io/website/0.10/sdk_docs/docs/KServeClient/#wait_isvc_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7862cce",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7f76c1a21f272626",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"kserve==0.10.1\"],\n",
    ")\n",
    "def deploy_model(model_name: str, storage_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        storage_uri: the URI of the saved model in MLflow's artifact store\n",
    "    \"\"\"\n",
    "    from kubernetes import client\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ModelSpec\n",
    "    from kserve import V1beta1ModelFormat\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name = \"kserve-sa\"\n",
    "    api_version = constants.KSERVE_V1BETA1\n",
    "    logger.info(f\"MODEL URI: {storage_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=storage_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=model_name,\n",
    "            namespace=namespace,\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=service_account_name,\n",
    "                sklearn=modelspec\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "    try:\n",
    "        kserve.create(inferenceservice=isvc)\n",
    "    except RuntimeError:\n",
    "        kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)\n",
    "    kserve.wait_isvc_ready(service_account_name, namespace=namespace, watch=True, timeout_seconds=360)\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MODEL URI: s3://mlflow/8/f06ce79ce7694abbb224dc0a7ae4a564/artifacts/test-model\n"
     ]
    }
   ],
   "source": [
    "deploy_model.python_func(model_name=\"test-bike-demand\", storage_uri=train_output.storage_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               URL                                                    READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                        AGE\n",
      "test-bike-demand   http://test-bike-demand.kserve-inference.example.com   True           100                              test-bike-demand-predictor-default-00003   27m\n"
     ]
    }
   ],
   "source": [
    "# Ensure the \"test-bike-demand\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc test-bike-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"test-bike-demand\",\"model_version\":null,\"id\":\"8d04ef0c-752e-4f34-88d0-c523549d7b91\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[42.1026868829854,30.14696834831473]}]}\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the inference service\n",
    "send_requests(isvc_name=\"test-bike-demand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{\"model_name\":\"test-bike-demand\",\"model_version\":null,\"id\":\"4a592a3e-bfe3-46b4-a07b-c8dc87954b67\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[42.1026868829854,30.14696834831473]}]}\n",
    "```\n",
    "The ID may vary. The prediction values may also vary depending on how your model was trained. The key point is that your output follows the same format as the expected one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"test-bike-demand\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the testing inference service\n",
    "!kubectl -n kserve-inference delete isvc test-bike-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19238798",
   "metadata": {},
   "source": [
    "## Assignment 2:  Create a KFP pipeline (2 points)\n",
    "You need to create a KFP pipeline using all the KFP components you created. \n",
    "\n",
    "The KFP components should perform their tasks in the following order: \n",
    "\n",
    "<img src=\"./images/kfp-assignment.jpg\" />\n",
    "\n",
    "(The other inputs needed by the components are provided as arguments of the `pipeline` function.)\n",
    "\n",
    "The **deploy model component** should perform its task only if the validation task succeeds. You can use [dsl.Condition](https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/control-flow/) to manage conditions in Kubeflow Pipelines. \n",
    "\n",
    "**Hints**: \n",
    "- You need to assign the needed credentials to the model training task so that the task can upload artifacts to MLflow's artifact store (MinIO). (Please check the tutorial for more details.) \n",
    "- We recall that if a component has a single Parameter output, this output can be accessed by `task.output`.\n",
    "- A boolean output will be converted to a string by Kubeflow Pipelines. For example, if there is component that returns a boolean as its output:\n",
    "```python\n",
    "@component()\n",
    "def component1():\n",
    "    if ...:\n",
    "        return True\n",
    "    return False\n",
    "task1 = component1()\n",
    "```\n",
    "then `task1.output` is the string \"true\" instead of the boolean True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19bd5f48",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-66fecedfd2224305",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"bike-demand-pipeline\",\n",
    "    description=\"An example pipeline that deploys a model for bike demanding prediction\"\n",
    ")\n",
    "def pipeline(\n",
    "    url: str,\n",
    "    target: str,\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_name: str,\n",
    "    hyperparams: Dict[str, Any],\n",
    "    threshold_metrics: Dict[str, Tuple[int, str]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: URL for downloading the dataset\n",
    "        target: Target column name of the dataset\n",
    "        mlflow_experiment_name: Name of the MLflow experiment\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        model_name: The name of the KServe inference service. It's also used as the model's artifact path\n",
    "        hyperparams: A dictionary of hyperparameters for training the LightGBM model, same as the one used by the train component\n",
    "        threshold_metrics: A dictionary of threshold values for each metric, same as the one used by the validate component \n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    # When we call a component in a pipeline definition, it constructs a pipeline task (instantiated component)\n",
    "    pull_task = pull_data(url)\n",
    "    \n",
    "    # The preprocess task uses the output Artifact of the pull_data task as the input\n",
    "    preprocess_task = preprocess_data(data=pull_task.outputs[\"data\"])\n",
    "    \n",
    "    train_task = train(\n",
    "        train_set=preprocess_task.outputs[\"train_set\"],\n",
    "        test_set=preprocess_task.outputs[\"test_set\"],\n",
    "        target=target,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        mlflow_s3_endpoint_url=mlflow_s3_endpoint_url,\n",
    "        model_artifact_path=model_name,\n",
    "        hyperparams=hyperparams\n",
    "    )\n",
    "\n",
    "    # The train task uploads the trained model to the MLflow service\n",
    "    # so it needs to access the required credentials of the MinIO storage service that MLflow uses.\n",
    "    # These credentials (username and password) have been deployed as a secret object named \"aws-secret\" to the \n",
    "    # Kubernetes cluster during the setup.\n",
    "    train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))\n",
    "    uri = train_task.outputs[\"storage_uri\"]\n",
    "    run_id = train_task.outputs[\"run_id\"]\n",
    "    \n",
    "    val_task = validate(run_id, mlflow_tracking_uri, threshold_metrics)\n",
    "\n",
    "    with dsl.Condition(val_task.output == 'true'):\n",
    "        deploy_model(model_name, uri)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144575f9",
   "metadata": {},
   "source": [
    "## Submit a KFP run\n",
    "After defining your KFP pipeline, you can test it by triggering a KFP run of your KFP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e074b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify pipeline argument values\n",
    "\n",
    "# A loose threshold to guarantee a successful validation\n",
    "validate_threshold_metrics = {\"rmse\": (100, \"minimize\"), \"r2\": (0.1, \"maximize\"), \"mae\": (100, \"minimize\")}\n",
    "\n",
    "# A strict threshold so that the validation will fail\n",
    "strict_validate_threshold_metrics = {\"rmse\": (0.1, \"minimize\"), \"r2\": (0.99999, \"maximize\"), \"mae\": (0.1, \"minimize\")}\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\"\n",
    "\n",
    "arguments = {\n",
    "    \"url\": url,\n",
    "    \"target\": \"count\",\n",
    "    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\n",
    "    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\n",
    "    \"mlflow_experiment_name\": \"bike-notebook\",\n",
    "    \"model_name\": \"bike-demand\",\n",
    "    \"hyperparams\": {\"learning_rate\": 0.2, \n",
    "                    \"num_leaves\": 63, \n",
    "                    \"random_state\": 42},\n",
    "    \"threshold_metrics\": validate_threshold_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e10b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/mlops_eng/lib/python3.10/site-packages/kfp/compiler/compiler.py:79: UserWarning: V2_COMPATIBLE execution mode is at Beta quality. Some pipeline features may not work as expected.\n",
      "  warnings.warn('V2_COMPATIBLE execution mode is at Beta quality.'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/19adf943-96bd-4478-b4b7-30d7a903f7d7\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/b5dba7e0-0e2d-48f4-969d-de9bd3327394\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=b5dba7e0-0e2d-48f4-969d-de9bd3327394)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"bike-run\"\n",
    "experiment_name = \"bike-experiment\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abc9ed",
   "metadata": {},
   "source": [
    "Go to [http://ml-pipeline-ui.local](http://ml-pipeline-ui.local) to check the running state of the KFP run. Also go to [http://mlflow-server.local](http://mlflow-server.local) to check the created MLflow experiment and the registered model.\n",
    "\n",
    "Please capture the following screenshots and put them into your PDF file:\n",
    "\n",
    "1) The Kubeflow Pipelines UI of a succeeded KFP run of this KFP pipeline.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/kfp-run-ok.png\" width=400/>\n",
    "</details>\n",
    "\n",
    "2) The MLflow UI of the created MLflow experiment, including the logged hyperparameters and evaluation metrics.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-log.png\" width=1000/>\n",
    "</details>\n",
    "\n",
    "3) The MLflow UI of the registered model.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-model.png\" width=1000/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cf070",
   "metadata": {},
   "source": [
    "#### A hint for debugging\n",
    "\n",
    "Your KFP run may fail to complete because of bugs in some of the components. You can further investigate the problems by looking into the component logs:\n",
    "\n",
    "![](./images/kfp-debug.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0ebe8",
   "metadata": {},
   "source": [
    "## Testing the inference service\n",
    "After the KFP run in completed, let's ensure that it's ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462945ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
      "bike-demand   http://bike-demand.kserve-inference.example.com   True           100                              bike-demand-predictor-default-00001   80m\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc0bef",
   "metadata": {},
   "source": [
    "After your model is ready in KServe, you can send a request to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bc69b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"bike-demand\",\"model_version\":null,\"id\":\"6e4d5014-312e-4dcd-80ae-49efd2ce6b68\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[23.743062591411174,35.72844242242732]}]}\n"
     ]
    }
   ],
   "source": [
    "send_requests(isvc_name=\"bike-demand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{\"model_name\":\"bike-demand\",\"model_version\":null,\"id\":\"093f23c2-dd69-4976-8b96-bfd4944e1294\",\"parameters\":null,\"outputs\":[{\"name\":\"predict\",\"shape\":[2],\"datatype\":\"FP64\",\"parameters\":null,\"data\":[23.743062591411174,35.72844242242732]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f7af9",
   "metadata": {},
   "source": [
    "## Another run of the pipeline\n",
    "\n",
    "Now you let's use the \"strict_validate_threshold_metrics\" as the threshold metrics and start another KFP run of your KFP pipeline. This time the KFP run will skip the deployment task because of the failed validation task. Capture a screenshot of the KFP run where the deployment task is skipped and put this screenshot into your PDF file.\n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/kfp-run-skip-deploy.png\" width=500/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/19adf943-96bd-4478-b4b7-30d7a903f7d7\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/1a8b2499-e526-455d-a63c-bd164f70d516\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=1a8b2499-e526-455d-a63c-bd164f70d516)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments[\"threshold_metrics\"] = strict_validate_threshold_metrics\n",
    "\n",
    "run_name = \"bike-run-incomplete\"\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=pipeline,\n",
    "    run_name=run_name,\n",
    "    experiment_name=experiment_name,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1fdf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In summary, your PDF file should have the following four screenshots: \n",
    "\n",
    "1) the Kubeflow Pipelines UI showing a succeeded KFP run of this KFP pipeline\n",
    "2) the MLflow UI of the created MLflow experiment\n",
    "3) the MLflow UI of the registered model\n",
    "4) the Kubeflow PIpelines UI of a KFP run where the deployment task is skipped due to the failed validation. \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
