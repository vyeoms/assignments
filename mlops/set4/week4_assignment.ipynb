{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 Assignments\n",
    "In this week's assignments, you'll gain more hands-on experience with deploying ML models, especially using KServe. \n",
    "\n",
    "### Prerequisite: \n",
    "To do this week's assignments, we assume you have previously trained a LightGBM regression model for bike sharing demand prediction and have already uploaded the model artifact to your MLflow service (week1 assignments). If you haven't completed this step, uncomment and run the next code cell before proceeding to the assignments for this week.\n",
    "\n",
    "### Guidelines for submitting the assignments\n",
    "- As usual, please submit this assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them.\n",
    "- Unlike the assignments of the previous weeks where you write Python code in the assignment notebooks, you will need to fill some configurations in JSON/YAML files in most of the assignments. In other words, you will need to put your answers in the required JSON/YAML files (not in this notebook, you can use the commands in the notebooks to check if you're progressing correctly).\n",
    "- For submission, please also include these JSON/YAML files in your submission. More precisely, these files include `model-settings.json` in the \"assignment1\" directory and `*.yaml` in the \"manifests\" directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite\n",
    "\n",
    "# from train_helper import train\n",
    "# params = {\n",
    "#     \"num_leaves\": 63,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"random_state\": 42\n",
    "# }\n",
    "# train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Use MLServer to deploy a model locally (2 points)\n",
    "[MLServer](https://mlserver.readthedocs.io/en/latest/index.html) is an open-source inference server implementation for ML models. It provides an easy way to expose a model through an HTTP or gRPC endpoint. \n",
    "\n",
    "You already trained a LightGBM model for predicting bike sharing demand and upload it to the MLflow service in the first week. In this assignment, your task is to configure MLServer to serve your LightGBM model as an inference service locally. Detailed instructions can be found later. \n",
    "\n",
    "**Hints**:\n",
    "- Reading the following MLServer documentation may be enough to complete the assignment:\n",
    "    - [Getting started with MLServer](https://mlserver.readthedocs.io/en/latest/getting-started/index.html#). You'll see an example of using MLServer SDK to implement a custom model server in this documentation. You don't need to implement your own model server to complete this assignment as MLServer has an out-of-box inference server implementation for models registered to MLflow (see the second documentation). \n",
    "    - [Serving MLflow models](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the dependency requirements of running the LightGBM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own LightGM model's S3 URI\n",
    "model_s3_uri = \"s3://mlflow/8/161672a9ab4d45579b80820054e8706e/artifacts/lgbm-bike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os \n",
    "\n",
    "# Configure MLflow\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "\n",
    "# Configure the credentials needed for accessing the MinIO storage service\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "# Download the requirements.txt file of the model and print the file's location\n",
    "file_path = mlflow.pyfunc.get_model_dependencies(model_uri=model_s3_uri)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move requirements.txt to the \"assignment1\" directory\n",
    "!mv {file_path} ./assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not mess up the \"mlops_eng\" environment, let's create a new Python environment named \"try_mlserver\" and install the dependencies. Open a new terminal and run the following commands\n",
    "```bash\n",
    "conda create -n try_mlserver -yf python==3.10 ipykernel\n",
    "conda activate try_mlserver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Switch the Python environment of this notebook to the new \"try_mlserver\" environment** by clicking the current environment name at the upper right corner and install the dependencies by running the following two code cells. \n",
    "\n",
    "If you can't find the environment name, close VS Code and open it again, then reopen the notebook. This may force VS Code to detect all available Python environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your notebook environment is try_mlserver\n",
    "%pip install -r assignment1/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's also install the mlserver packages. mlserver-mlflow is the out-of-box server implementation for MLflow models and boto3 is required by MLServer to load the model from the MLflow service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlserver==1.3.5 mlserver-mlflow==1.3.5 boto3~=1.28.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1 instructions\n",
    "Now you need to \n",
    "1. Add configurations to the empty [assignment1/model-settings.json](./assignment1/model-settings.json) to use the MLServer's MLflow runtime to serve your LightGBM model. The inference service name should be ***bike-demand-predictor***. The configuration can be adapted from the [one provided by this MLServer doc.](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html#serving)\n",
    "1. In a separate terminal (where the \"try_mlserver\" conda environment is activated), start an MLServer inference service to serve the LightGBM model.\n",
    "1. Keep the inference service running and use the following code cell to check whether or not your configuration works. **Please keep the output of the following code cells.** The reviewer will use the output to check your MLServer configuration. \n",
    "\n",
    "Notes:\n",
    "- When starting an MLServer inference service in a terminal, you need to change the conda environment of that terminal session to \"try_mlserver\" by running `conda activate try_mlserver` so that the mlserver command can be found. \n",
    "\n",
    "- MLServer will load the model from your MinIO storage service so you need to specify the following environment variables to allow MLServer to use the correct credentials to load the model from the correct MinIO service endpoint:\n",
    "```bash\n",
    "# Run the following command in a terminal\n",
    "export AWS_ACCESS_KEY_ID=minioadmin\n",
    "export AWS_SECRET_ACCESS_KEY=minioadmin\n",
    "export MLFLOW_S3_ENDPOINT_URL=http://mlflow-minio.local\n",
    "```\n",
    "These environment variables are only available in the terminal session where you defined them, so you need to start your MLServer inference service in the same terminal session where you defined the above environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from mlserver.codecs import PandasCodec\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# We need to use some functions in ../train_helper.py so we just append it to the Python path at runtime. \n",
    "# The Python path is a list of directory locations where Python looks for modules and packages when you try to import them in your code.\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from train_helper import pull_data, preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_url = \"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\"\n",
    "\n",
    "# Prepare some data to send in requests\n",
    "df = pull_data(dataset_url)\n",
    "_, test = preprocess(df)\n",
    "test_x = test.drop([\"count\"], axis=1)\n",
    "\n",
    "request_data = test_x.head()\n",
    "\n",
    "# Encode the request data following the V2 inference protocol\n",
    "encoded_request_data = PandasCodec.encode_request(request_data).dict()\n",
    "print(encoded_request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request\n",
    "response = requests.post(\"http://localhost:8080/v2/models/bike-demand-predictor/infer\", json=encoded_request_data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-demand-predictor',\n",
    " 'id': 'f021577e-16fb-4686-8f1e-70f3ae2a7b76',\n",
    " 'parameters': {'content_type': 'np'},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [5, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'parameters': {'content_type': 'np'},\n",
    "   'data': [42.09539399666931,\n",
    "    23.974666238188583,\n",
    "    13.463013296846174,\n",
    "    8.769204532023744,\n",
    "    8.769204532023744]}]}\n",
    "```\n",
    "The id may vary. The output data ([42.09..., 23.97..., ...]) may also vary depending on how you trained the model in the first week. The key point is that the response should follow the same format as the expected output. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, you need to **switch the notebook's environment back to \"mlops_eng\"** for the rest of the assignments. \n",
    "\n",
    "### Guidelines for doing Assignments 2-5\n",
    "- In 2a), you'll need to write some Python code, so please put your code between the `### START CODE HERE` and `### END CODE HERE` comments. \n",
    "- In other assignments, you'll need to complete some configurations in given YAML files. Please write your configurations between the `### START CONF HERE` and `### END CONF HERE` comments in each YAML file.\n",
    "- You will use a command `kubectl -n kserve-inference get isvc <name-of-inference-service>` (or `kubectl -n kserve-inference get ig <name-of-inference-graph>`) a few times when running this notebook. This command checks whether your inference service (or inference graph) deployed to KServe is ready. It takes some time (up to a few minutes) for a inference service/graph to become ready, so you may need to run the same command a few times to follow the readiness of your inference service/graph. You can also use the \"-w\" option to continuously watch the status of the inference service/graph (`kubectl get isvc <name-of-inference-service> -n kserve-inference -w`) and then terminate the code cell when the inference service/graph is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Deploy a model to KServe (3 points)\n",
    "In this assignment, you need to deploy your LightGBM model for predicting bike sharing demand as an inference service to KServe. You can use the model trained in the first week. \n",
    "\n",
    "Similar to the tutorial, the deployed inference service should run in the \"kserve-inference\" namespace and the service account name containing the credentials for accessing the MinIO storage service is also \"kserve-sa\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1ModelSpec\n",
    "from kserve import V1beta1ModelFormat\n",
    "import logging\n",
    "\n",
    "from send_request import send_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Use Python SDK to deploy your LightGBM model\n",
    "Complete the `deploy_model` function that uses the KServe SDK to deploy your LightGBM model. \n",
    "\n",
    "**Hint**: Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model uploaded to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(model_name: str, model_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        model_uri: the S3 URI of the model saved in MLflow\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name=\"kserve-sa\"\n",
    "    kserve_version=\"v1beta1\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    \n",
    "    logger.info(f\"MODEL URI: {model_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=model_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "        ### START CODE HERE\n",
    "        # define api_version, kind, and metadata\n",
    "        \n",
    "        ### END CODE HERE\n",
    "\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                ### START CODE HERE\n",
    "                \n",
    "                ### END CODE HERE\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    # Create or update an inference service\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bike-lgbm-2a\"\n",
    "\n",
    "# Replace the storage_uri to your own one\n",
    "model_uri = \"s3://mlflow/8/161672a9ab4d45579b80820054e8706e/artifacts/lgbm-bike\"\n",
    "\n",
    "# Test the deploy_model function\n",
    "deploy_model(model_name, model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the \"bike-lgbm-2a\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\n",
    "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-default-00001   72s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-2a-predictor-default-00001-deployment-6499598b7-wc28j   2/2     Running   0          65s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the inference service\n",
    "send_request(model_name=\"bike-lgbm-2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-2a', \n",
    "'model_version': None, \n",
    "'id': '4a91cc4c-3a04-4aa1-95ee-6bbbf04207b7', \n",
    "'parameters': None, \n",
    "'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]\n",
    "}\n",
    "```\n",
    "**Note**: The id varies. The output data ([51.0..., 35.1...]) may also vary depending on how your model was trained. The important point is that the response has the correct fields as shown in the above expected output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S.* KServe also uses MLServer to serve the models uploaded to the MLflow service, which means your inference service uses V2 inference protocol. If you take a look at the `send_request` function in [send_request.py](./send_request.py), you'll observe that the input data formats and the URL format align with what was used in Assignment 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by removing the \"bike-lgbm-2a\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Use a YAML file to deploy the model\n",
    "Instead of using the KServe SDK, now you need to use a YAML file to deploy your LightGBM model again. Please complete the configuration in [manifests/bike-lgbm-basic.yaml](./manifests/bike-lgbm-basic.yaml).\n",
    "\n",
    "**Hint**: You can check from [this KServe doc](https://kserve.github.io/website/0.10/modelserving/v1beta1/mlflow/v2/#deploy-with-inferenceservice) on how to use a YAML manifest to deploy a model stored in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the LightGBM model for bike demand prediction as an inference service named \"bike-lgbm\"\n",
    "!kubectl apply -f manifests/bike-lgbm-basic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the \"bike-lgbm\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   2m24s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-9d7b87595-k9kpk   2/2     Running   0          70s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send some requests to the \"bike-lgbm\" inference service\n",
    "send_request(model_name=\"bike-lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm', \n",
    "'model_version': None, \n",
    "'id': '6783fc56-a759-41b0-9d01-94be32238b01', \n",
    "'parameters': None, \n",
    "'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [51.00457318737209, 35.13687405851507]}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please don't delete the \"bike-lgbm\" inference service, you will need it in Assignment3 later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Canary deployment in KServe (2 points)\n",
    "You'll train a new LightGBM model for predicting the bike sharing demand. In this assignment, your task is to deploy the new model to KServe using the canary deployment strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to make sure there's already a \"bike-lgbm\" inference service running in KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   47m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a new LightGBM model and upload it to MLflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new LightGBM model for predicting bike sharing demand\n",
    "# This is the same as the assignment in Week1, we just change the hyperparameters of the model and skip the part that uses Deepchecks to perform offline model evaluation\n",
    "# The model's S3 URI will be printed at the end\n",
    "from train_helper import train\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 127, # Before it was 63\n",
    "    \"learning_rate\": 0.1, # Before it was 0.05\n",
    "    \"random_state\": 42 \n",
    "}\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to complete the configuration in [manifests/bike-lgbm-canary.yaml](./manifests/bike-lgbm-canary.yaml) to deploy the new LightGBM model using canary deployment. Your new model should receive **30%** of the user traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the \"bike-lgbm\" inference service to use the new model\n",
    "!kubectl apply -f manifests/bike-lgbm-canary.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm configured\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the traffic is splitted between the old and the new version\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION               LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True    70     30       bike-lgbm-predictor-default-00001   bike-lgbm-predictor-default-00002   61m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are two pods (one old and one new one) running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-cc96598f-rr2xz    2/2     Running   0          63m\n",
    "bike-lgbm-predictor-default-00002-deployment-6d9f5bbff-8mhn8   2/2     Running   0          3m36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by removing the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Horizontal autoscaling (2 points)\n",
    "\n",
    "In this assignment, your task is to complete the configuration in [manifests/bike-lgbm-scale.yaml](./manifests/bike-lgbm-scale.yaml) to deploy your LightGBM model to KServe and configure the horizontal autoscaling feature for the deployed inference service. Specifically, the horizontal autoscaling of the inference service should satisfy the following requirements:\n",
    "1. The inference service should have ae least **2** pods running;\n",
    "2. The inference service can have at most **8** pods running when it's being scaled up;\n",
    "3. The inference service should be scaled up when each pod is receiving no less than 5 requests per second.\n",
    "\n",
    "You can use whichever LightGBM model you trained for predicting bike sharing demand in this assignment. \n",
    "\n",
    "**Hint**: \"rps\" should be used as the scaling metric. \n",
    "\n",
    "*rps (requests per second) VS concurrency: These two metrics may look similar at the first glance. Both of them are metrics used to measure service performance. rps quantifies the number of requests a service can process within a specific time frame, often a second, whereas concurrency focuses on how many tasks a service can handle simultaneously.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy an inference service named \"bike-lgbm-scale\"\n",
    "!kubectl apply -f manifests/bike-lgbm-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the \"bike-lgbm-scale\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\n",
    "bike-lgbm-scale   http://bike-lgbm-scale.kserve-inference.example.com   True           100                              bike-lgbm-scale-predictor-default-00001   90s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are two pods (replicas) running for the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcd67mr   2/2     Running   0          7m26s\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcjb6qx   2/2     Running   0          7m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please use the command below to print the configuration of your \"bike-lgbm-scale\" inference service. The output will be used to check if your configuration is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl -n kserve-inference get isvc bike-lgbm-scale -o json|jq .spec.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by removing the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Inference graph in KServe (3 points)\n",
    "\n",
    "## 5a) Inference graph for ensemble\n",
    "So far you already have two LightGBM models for predicting bike sharing demand. One was trained using the hyperparameters of {learning_rate=0.05, num_leaves=63} (denoted by Model A) and another {learning_rate=0.1, num_leaves=127} (denoted by Model B). \n",
    "\n",
    "You need to first complete the configuration in [manifests/bike-lgbm-graph1.yaml](./manifests/bike-lgbm-graph1.yaml) to deploy two inference services named \"bike-lgbm-1\" and \"bike-lgbm-2\". The \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services should serve Model A and B, respectively.\n",
    "\n",
    "Next, you need to complete [manifests/inference-graph1.yaml](./manifests/inference-graph1.yaml) to deploy an inference graph that includes one ensemble routing node. With this inference graph, a user will receive two predictions (one from each inference service) when they send a request.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services\n",
    "!kubectl apply -f manifests/bike-lgbm-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-1 created\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the two inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   105m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   105m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are two pods running for the two inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          109m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          109m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the inference graph named \"my-graph1\"\n",
    "!kubectl apply -f manifests/inference-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph1 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the \"my-graph1\" inference graph is ready\n",
    "!kubectl -n kserve-inference get ig my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph1   http://my-graph1.kserve-inference.example.com   True    102s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferencegraph=my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph1-00001-deployment-7c4d7cfbf9-tr5tz   2/2     Running   0          2m7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send a request to the \"my-graph1\" inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request\n",
    "from send_request import send_request\n",
    "\n",
    "send_request(to_ig=True, ig_name=\"my-graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output (i.e., the response) is expected to contain a prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': 'ec476473-c736-478d-9830-e2b6f53548db', 'model_name': 'bike-lgbm-1', 'model_version': None, 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}, 'bike-lgbm-v2': {'id': 'fd17e0c6-d17a-42e0-8cbe-d452e0107d34', 'model_name': 'bike-lgbm-2', 'model_version': None, 'outputs': [{'data': [34.87125805456099, 32.68341881111533], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Do not delete the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. They're still needed in the next assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) More complicated inference graph\n",
    "In this assignment, you need to deploy a more complex inference graph containing more than one routing node. \n",
    "\n",
    "First let's train the third model, denoted by Model C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_helper import train\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 127, \n",
    "    \"learning_rate\": 0.17,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"random_state\": 42 \n",
    "}\n",
    "\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Assignment 5a, your tasks are to\n",
    "1. Complete [manifests/bike-lgbm-graph2.yaml](./manifests/bike-lgbm-graph2.yaml) to deploy third inference service named \"bike-lgbm-3\" that serves Model C you just trained. \n",
    "2. Complete [manifests/inference-graph2.yaml](./manifests/inference-graph2.yaml) to deploy an inference graph containing a Switch and a Ensemble routing node. \n",
    "The requests that will be sent to the inference graph look like:\n",
    "    ```python\n",
    "    {\n",
    "      'inputs': ...,\n",
    "      'userType': 'basic'\n",
    "    }\n",
    "    ```\n",
    "    The inference graph should satisfy the following requirements:\n",
    "    - If there is a field named \"userType\" in the request and its value is \"basic\", the request should be forwarded to an ensemble consisting of the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. At this time, the \"basic\" user should receive one prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service, just like in the previous assignment. \n",
    "    - If the value of \"userType is \"advanced\", the request should be forwarded to the \"bike-lgbm-3\" inference service. At this time, the \"advanced\" user should receive one prediction from the \"bike-lgbm-3\" inference service.\n",
    "    - Otherwise the request should be directly returned. \n",
    "    \n",
    "    The behavior of the inference graph is illustrated in the figure below:\n",
    "\n",
    "    <img src=\"./images/complex-inference-graph.jpg\" width=600/>\n",
    "\n",
    "**Hints**:\n",
    "You may notice that you need to route requests from one routing node to another (instead of from a routing node to an inference service). Below is an example of configuring a routing node to forward requests to another routing node:\n",
    "```yaml\n",
    "...\n",
    "spec: \n",
    "  nodes: \n",
    "    # The first routing node\n",
    "    root: \n",
    "      routerType: ...\n",
    "      steps: \n",
    "      # This routing node forwards requests to the second routing node named \"ensembleNode\"\n",
    "      - nodeName: ensembleNode\n",
    "\n",
    "    # The second routing node\n",
    "    ensembleNode:\n",
    "      routerType: ...\n",
    "      steps:\n",
    "      ...\n",
    "```\n",
    "You can use `\"[@this].#(userType==\\\"...\\\")\"` as the condition that determines whether a request should be routed to an ensemble of model A and model B, or to to the standalone model C.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the third inference service named \"bike-lgbm-3\"\n",
    "!kubectl apply -f manifests/bike-lgbm-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-3 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the all of the three inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2 bike-lgbm-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   132m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   132m\n",
    "bike-lgbm-3   http://bike-lgbm-3.kserve-inference.example.com   True           100                              bike-lgbm-3-predictor-default-00001   22s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are three pods running for the three inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2,bike-lgbm-3)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          132m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          132m\n",
    "bike-lgbm-3-predictor-default-00001-deployment-7684784979-zmmgw   2/2     Running   0          42s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the second inference graph (\"my-graph2\")\n",
    "!kubectl apply -f manifests/inference-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inference graph named \"my-graph2\" is ready\n",
    "!kubectl -n kserve-inference get ig my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph2   http://my-graph2.kserve-inference.example.com   True    35s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l  serving.kserve.io/inferencegraph=my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph2-00001-deployment-557678dfbd-zglcg   2/2     Running   0          49s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send some requests\n",
    "from send_request import send_request\n",
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is expected to contain predictions from both \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '03f3bc30-a628-418f-9539-2372f66d353f', 'model_name': 'bike-lgbm-1', 'model_version': None, 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}, 'bike-lgbm-v2': {'id': 'a8a68d72-03b4-4b2b-91a9-3e5a08cbefbd', 'model_name': 'bike-lgbm-2', 'model_version': None, 'outputs': [{'data': [34.87125805456099, 32.68341881111533], 'datatype': 'FP64', 'name': 'predict', 'parameters': None, 'shape': [2]}], 'parameters': None}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should only have predictions from the \"bike-lgbm-3\" inference service.\n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-3', 'model_version': None, 'id': '6bfde99b-e23a-47cd-8192-a1cdc0773c4a', 'parameters': None, 'outputs': [{'name': 'predict', 'shape': [2], 'datatype': 'FP64', 'parameters': None, 'data': [43.59313309843417, 32.17377957904267]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", user_type=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request should be directly returned.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'parameters': {'content_type': 'pd'}, 'inputs': [{'name': 'season', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'holiday', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 0]}, {'name': 'workingday', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 0]}, {'name': 'weather', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'temp', 'shape': [2], 'datatype': 'FP64', 'data': [9.84, 9.02]}, {'name': 'atemp', 'shape': [2], 'datatype': 'FP64', 'data': [14.395, 13.635]}, {'name': 'humidity', 'shape': [2], 'datatype': 'UINT64', 'data': [81, 80]}, {'name': 'windspeed', 'shape': [2], 'datatype': 'FP64', 'data': [0.0, 0.0]}, {'name': 'hour', 'shape': [2], 'datatype': 'UINT64', 'data': [0, 1]}, {'name': 'day', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}, {'name': 'month', 'shape': [2], 'datatype': 'UINT64', 'data': [1, 1]}], 'userType': 'random'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all of the three inference services\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-1 bike-lgbm-2 bike-lgbm-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-3\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all inference graphs\n",
    "!kubectl -n kserve-inference delete ig my-graph1 my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
    "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please make sure you have the following files in your submission:\n",
    "- This notebook (with up-to-date outputs of the code cells)\n",
    "- model-settings.json \n",
    "- all YAML files listed in the \"manifests\" directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
