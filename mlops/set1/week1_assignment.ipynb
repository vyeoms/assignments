{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "In this week's assignments, you will train a LightGBM regression model to predict public bike sharing demand given the attributes like datetime and weather conditions. The dataset (\"bike_sharing_demand.csv\", located under the same directory as this notebook) used in this week's assignments is a preprocessed version of [this kaggle dataset](https://www.kaggle.com/competitions/bike-sharing-demand/overview). You'll learn more about data preprocessing next week. Additionally, you'll use MLflow to track the model training and Deepchecks to evaluate the trained model. \n",
    "\n",
    "**Guidelines of submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to answer questions or capture screenshots in order to earn points. Please put all your answers and the required screenshots into a single PDF file. For each answer or screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your answers and screenshots in your submission. \n",
    "\n",
    "In case type hints are new to you, you'll see something below in some code skeletons:\n",
    "```python\n",
    "def greeting(name: str) -> str:\n",
    "    return 'Hello ' + name  \n",
    "```\n",
    "The annotation `name: str` means the parameter `name` is expected to be of type `str` and `-> str` means the type of the returned value is also `str`. These type hints are provided to help you understand the function's input requirements and its expected output in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Set up the course environment (4 points)\n",
    "You can earn 4 points for successfully setting up the course environment. To do so, simply write \"yes\" in your PDF file. Additionally, feel free to provide suggestions for improving the instructions within your PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular.checks import TrainTestPerformance, ModelInferenceTime, MultiModelPerformanceReport\n",
    "from deepchecks.tabular import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Download and split the dataset (2 points)\n",
    "\n",
    "### 1a) Load the data\n",
    "First, implement the `pull_data` function that loads a CSV as a Pandas DataFrame from a given location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(dataset_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download the data set from a given path\n",
    "    Args: \n",
    "        dataset_path (Path): Path of the CSV \n",
    "    Returns:\n",
    "        A Pandas DataFrame of the dataset\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this code cell to check if your pull_data function works correctly\n",
    "dataset_path = Path.cwd() / \"bike_sharing_demand.csv\"\n",
    "df = pull_data(dataset_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Expected output</summary>\n",
    "    <img src=\"./images/dataset-info.png\"/>\n",
    "</details>\n",
    "\n",
    "Below is the explanation of each column in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "| hour        | the hours of the datetime| integer\n",
    "| day         | the day of the datetime| integer\n",
    "| month       | the month of the datetime| integer\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| count       | number of total rentals                         | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Split the data into train and test DataFrames\n",
    "Then implement the `splitData` function that splits the dataset into a training and a test dataset, using the last 168 rows of the dataset as the test data.\n",
    "\n",
    "Hint: You may find [pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.iloc.html) useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(input_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a DataFrame into training and testing sets\n",
    "    Args:\n",
    "        input_df (DataFrame): The DataFrame to be splitted\n",
    "    Return:\n",
    "        A tuple of training and testing DataFrame\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pull_data(dataset_path)\n",
    "train, test = splitData(df)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Expected output</summary>\n",
    "    <img src=\"./images/train-df.png\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dimension of the training dataset should be {train.shape}\")\n",
    "print(f\"The dimension of the testing dataset should be {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "The dimension of the training dataset should be (10718, 12)\n",
    "\n",
    "The dimension of the testing dataset should be (168, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing DataFrames into features and targets\n",
    "target = \"count\"\n",
    "input_df = pull_data(Path.cwd() / \"bike_sharing_demand.csv\")\n",
    "train, test = splitData(input_df)\n",
    "train_x = train.drop([target], axis=1)\n",
    "test_x = test.drop([target], axis=1)\n",
    "train_y = train[[target]]\n",
    "test_y = test[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Offline model evaluation using Deepchecks (2 points)\n",
    "\n",
    "### 2a) Construct the Dataset objects used by Deepchecks\n",
    "First, let's construct the Deepchecks Dataset objects from the train and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features, remember to specify the categorical features when you construct the datasets handled by Deepchecks\n",
    "# See https://docs.deepchecks.com/stable/tabular/usage_guides/dataset_object.html\n",
    "cat_features = [\"season\", \"holiday\", \"workingday\", \"weather\", \"hour\", \"day\", \"month\"]\n",
    "\n",
    "### START CODE HERE\n",
    "# Replace None with correct arguments\n",
    "train_dataset = Dataset(None)\n",
    "test_dataset = Dataset(None)\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Deepchecks Suite with conditions\n",
    "Implement the `evaluate` function that uses Deepchecks Suite to perform the following two tests:\n",
    "1) Evaluate the model's MAE and RMSE on both training and testing dataset. This test should fail if the MAE or RMSE drops more than 20% on the testing dataset compared to the training dataset;\n",
    "2) Evaluate the model's inference time. This test should fail if the inference time exceeds 0.1 second. \n",
    "\n",
    "Additionally, this function should save the evaluation results into an HTML file under the same directory as this notebook. \n",
    "\n",
    "**Hints**:\n",
    "- [How to add conditions to a test?](https://docs.deepchecks.com/stable/general/usage/customizations/auto_examples/plot_configure_check_conditions.html)\n",
    "- [Train test performance](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.html)\n",
    "- [Model inference time](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_model_inference_time.html)\n",
    "- [Condition for comparing model performance between training and testing dataset](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than.html#deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than)\n",
    "- [Condition for validating inference time](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.ModelInferenceTime.add_condition_inference_time_less_than.html)\n",
    "- [How to export the evaluation result into a file?](https://docs.deepchecks.com/stable/general/usage/export_save_results.html#save-result-as-an-html-report-save-as-html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_dataset: Dataset, test_dataset: Dataset, model: LGBMRegressor, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Deepchecks to evaluate 1) model's MAE and RMSE on both training and testing dataset, 2) model's inference time. Then save the evaluation results to an HTML file\n",
    "    Args:\n",
    "        train_dataset (Dataset): training Dataset\n",
    "        test_dataset (Dataset): testing Dataset\n",
    "        model (LGBMRegressor): The LightGBM regression model to be evaluated\n",
    "        filename: The file name of the evaluation results\n",
    "    Return:\n",
    "        Name of the created HTML file\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a testing model trained on the same bike demand dataset to help you check if your evaluate function works correctly\n",
    "test_model = pickle.load(open(\"test-model.pkl\", \"rb\"))\n",
    "result_file_name = evaluate(train_dataset, test_dataset, test_model, \"test-result.html\")\n",
    "print(result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code cell, you should see a file named \"test-result.html\" appear under the same directory with this notebook.\n",
    "\n",
    "Let's render this HTML file in a browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"file://\" + str(Path.cwd() / result_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> Expected output (in the browser) </summary>\n",
    "    <br />\n",
    "    The test of MAE/RMSE should fail:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-train-test-performance.png\">\n",
    "    <br />\n",
    "    The test of inference time should pass:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-inference-time.png\">\n",
    "</details>\n",
    "\n",
    "### Screenshots to be submitted for Assignment 2\n",
    "Similar to the expected output above, please submit screenshots of the web page showing which test passed and which test failed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Tracking model training in MLflow (2 points)\n",
    "Similar to what you see in the MLflow tutorial, you need to complete the following tasks in this assignment: \n",
    "1) Use LightGBM to train a regression model to predict the bike sharing demand. The model should be trained using the training DataFrame you prepared previously and with the following hyperparameters: num_leaves = 63, learning_rate = 0.05, random_state = 42 (for reproducibility).\n",
    "2) Use MLflow to track the model training. Specifically, you need to log the used hyperparameters and use the `evaluation` function you created above to evaluate the trained model. You also need to upload the Deepchecks evaluation result file and register the trained model to MLflow.  \n",
    "\n",
    "**Hint**: You can check [LightGBM documentation on how to train a regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#) if you're not familiar with it.\n",
    "\n",
    "**Note**: LightGBM provides two types of APIs, the native APIs and the Scikit-learn (sklearn) APIs. The sklearn APIs can be seen as an interface that allows you to use a LightGBM model as if it was trained using the sklearn library. The model trained in this assignment will be used in other assignments in the upcoming weeks. Please use the **sklearn API** in this assignment to avoid unexpected issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow configuration\n",
    "MLFLOW_S3_ENDPOINT_URL = \"http://mlflow-minio.local\"\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server.local\"\n",
    "AWS_ACCESS_KEY_ID = \"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY = \"minioadmin\"\n",
    "mlflow_experiment_name = \"week1-lgbm-bike-demand\"\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "# model hyperparameters\n",
    "num_leaves = 63\n",
    "learning_rate = 0.05\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(num_leaves=num_leaves, learning_rate=learning_rate, random_state=random_state)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots to be submitted for Assignment 3\n",
    "To get the points from Assignment 3, please submit the following screenshots:\n",
    "1. The logs of your MLflow run. Please include the parameters of the model in your screenshot. \n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-run.png\" width=1000>\n",
    "</details>\n",
    "\n",
    "2. The details of the MLflow run including uploaded Deepchecks evaluation result file;\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-run-detail1.png\" width=1000>\n",
    "    <img src=\"./images/mlflow-run-detail2.png\" width=1000>\n",
    "    <img src=\"./images/mlflow-run-detail3.png\" width=1000>\n",
    "</details>\n",
    "\n",
    "3. The registered model.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-model.png\" width=1000>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Evaluate the trained model against another model (2 points)\n",
    "Suppose your colleague had trained an ElasticNet model for the same use case of bike sharing demand prediction. In this assignment, your tasks are:\n",
    "1. Using Deepchecks to compare MAE and RMSE of your LightGBM model to the ElasticNet model and saving the result as an HTML file;\n",
    "1. Uploading the result file to MLflow, the file should be under the MLflow Run where you trained your LightGBM model in Assignment 3. E.g.,\n",
    "\n",
    "<img src=\"./images/ass4-example.png\" width=300 />\n",
    "\n",
    "Note that the idea here is to attach a file to an existing MLflow Run, not to create a new MLflow Run and then upload the file under the new MLflow Run. \n",
    "\n",
    "You may find the following docs helpful: \n",
    "- [Multi model performance report](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_multi_model_performance_report.html).\n",
    "- [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ElasticNet model first\n",
    "old_model = pickle.load(open(\"old-model.pkl\", \"rb\"))\n",
    "old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Deepchecks to compare your LightGBM model to the ElasticNet model and save the result to an HTML file\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the result file to MLflow. The file should be under the MLflow Run where you trained your LightGBM model\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots to be submitted for Assignment 4\n",
    "The details of the MLflow run including uploaded Deepchecks model comparison result file.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/deepchecks-compare-models.png\" />\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
