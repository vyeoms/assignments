{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932690c",
   "metadata": {},
   "source": [
    "# Week3 Assignments\n",
    "You will get some hands-on experience with Optuna through this week's assignments. Similar to the tutorial of the first week, the [red wine dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) will be used in this week's assignments. \n",
    "\n",
    "**Guidelines of submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to answer questions (in text) or capture screenshots in order to earn points. For each question, please put your text answers **in the same Markdown cell as the question** and your screenshots into a single PDF file. For each screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your screenshots in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dfe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd5f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# MLflow service URI\n",
    "mlflow_tracking_uri = \"http://mlflow-server.local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cf832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and testing data\n",
    "data = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = data.drop(\"quality\", axis=1)\n",
    "y = data[\"quality\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80f7a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An overview of the original dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae2bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of train_x is (1199, 11)\n",
      "The dimension of train_y is (1199,)\n",
      "The dimension of test_x is (400, 11)\n",
      "The dimension of train_x is (400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimension of train_x is {train_x.shape}\")\n",
    "print(f\"The dimension of train_y is {train_y.shape}\")\n",
    "print(f\"The dimension of test_x is {test_x.shape}\")\n",
    "print(f\"The dimension of train_x is {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {},
   "source": [
    "## Assignment 1: The basic use of Optuna (2 points)\n",
    "Train a [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) to predict red wine quality and use Optuna to find the optimal hyperparameter combination for the model. **Please use the sklearn API as you did in the first week's assignments.**\n",
    "\n",
    "This assignment has the following requirements:\n",
    "\n",
    "1) Create an Optuna study named ***lgbm-wine-1***. The target of the optimization is to minimize the MAE (mean absolute error) of the model when evaluating the model against the testing dataset. The hyperparameters to be tuned and their search ranges are shown below. Some of the hyperparameter values are fixed. The hyperparameter values should be sampled in a linear domain if not separately specified. \n",
    "\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 0.5]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 0.5]                                                              |\n",
    "| min_child_samples | A leaf node should have `min_child_samples` data points to be further splitted. | integer | [20, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value, RANDOM_SEED has been defined as a variable in a previous cell)                                                |\n",
    "\n",
    "2) Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "\n",
    "3) The Optuna study should perform 100 trials.\n",
    "\n",
    "4) The study history should be persisted in a relational database (`optuna.sqlite3`) so that the study can be loaded and analyzed later. \n",
    "\n",
    "Hints:\n",
    "- [How to sample hyperparameter values in the logarithmic domain?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float)\n",
    "- [How to configure a study to use a specific sampler and persist study history in a specific database?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html#optuna-create-study)\n",
    "- It may take some time run 100 trials, so you could test your code using a smaller number of trials and increase the number to 100 after you're sure your code works. Remember to delete the study before rerunning the study to avoid the error of duplicated study. The code for deleting an existing study from a database can be found after the assignment code cell of this assignment. \n",
    "\n",
    "\n",
    "*More reading material: If you are interested, [the LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters) explains the use of each hyperparameter in more details.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39b96c3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-15d27f8fdf5190ad",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-21 14:17:59,247] A new study created in RDB with name: lgbm-wine-1\n",
      "[I 2023-11-21 14:18:07,012] Trial 0 finished with value: 0.48305757514051956 and parameters: {'learning_rate': 0.005611516415334507, 'subsample': 0.4778214378844623, 'colsample_bytree': 0.3793972738151323, 'min_child_samples': 68, 'num_leaves': 161}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:10,974] Trial 1 finished with value: 0.6685091743119267 and parameters: {'learning_rate': 0.002051110418843397, 'subsample': 0.07613762547568977, 'colsample_bytree': 0.4397792655987208, 'min_child_samples': 68, 'num_leaves': 726}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:15,570] Trial 2 finished with value: 0.5330860406894041 and parameters: {'learning_rate': 0.0010994335574766201, 'subsample': 0.48645943347289744, 'colsample_bytree': 0.4245991883601898, 'min_child_samples': 37, 'num_leaves': 188}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:19,112] Trial 3 finished with value: 0.5338040960963436 and parameters: {'learning_rate': 0.002327067708383781, 'subsample': 0.186909009331792, 'colsample_bytree': 0.28614039423450705, 'min_child_samples': 54, 'num_leaves': 299}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:22,267] Trial 4 finished with value: 0.5076269460531386 and parameters: {'learning_rate': 0.01673808578875214, 'subsample': 0.11277223729341883, 'colsample_bytree': 0.1814650918408482, 'min_child_samples': 49, 'num_leaves': 468}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:23,256] Trial 5 finished with value: 0.5076748898380093 and parameters: {'learning_rate': 0.037183641805732096, 'subsample': 0.1398532019712619, 'colsample_bytree': 0.28140549728612524, 'min_child_samples': 67, 'num_leaves': 49}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:24,986] Trial 6 finished with value: 0.6685091743119267 and parameters: {'learning_rate': 0.016409286730647923, 'subsample': 0.1267358556592812, 'colsample_bytree': 0.0792732168433758, 'min_child_samples': 96, 'num_leaves': 989}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:27,941] Trial 7 finished with value: 0.5051750796616936 and parameters: {'learning_rate': 0.041380401125610165, 'subsample': 0.1870761961280168, 'colsample_bytree': 0.09395245130287275, 'min_child_samples': 75, 'num_leaves': 452}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:30,701] Trial 8 finished with value: 0.602185209067021 and parameters: {'learning_rate': 0.0017541893487450805, 'subsample': 0.2728296095500716, 'colsample_bytree': 0.06547483450184828, 'min_child_samples': 93, 'num_leaves': 266}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:31,725] Trial 9 finished with value: 0.49454634175435186 and parameters: {'learning_rate': 0.02113705944064573, 'subsample': 0.19026998424023495, 'colsample_bytree': 0.28403060953001485, 'min_child_samples': 64, 'num_leaves': 191}. Best is trial 0 with value: 0.48305757514051956.\n",
      "[I 2023-11-21 14:18:35,915] Trial 10 finished with value: 0.45233172719956855 and parameters: {'learning_rate': 0.0059057803847388166, 'subsample': 0.46978789997535003, 'colsample_bytree': 0.48629169985453957, 'min_child_samples': 21, 'num_leaves': 17}. Best is trial 10 with value: 0.45233172719956855.\n",
      "[I 2023-11-21 14:18:39,252] Trial 11 finished with value: 0.47633974214588887 and parameters: {'learning_rate': 0.0055881031669443425, 'subsample': 0.48995674302519066, 'colsample_bytree': 0.49091454950925223, 'min_child_samples': 26, 'num_leaves': 5}. Best is trial 10 with value: 0.45233172719956855.\n",
      "[I 2023-11-21 14:18:43,535] Trial 12 finished with value: 0.457045974419484 and parameters: {'learning_rate': 0.005615594319846422, 'subsample': 0.4001693423687098, 'colsample_bytree': 0.48811588618406093, 'min_child_samples': 22, 'num_leaves': 18}. Best is trial 10 with value: 0.45233172719956855.\n",
      "[I 2023-11-21 14:18:48,489] Trial 13 finished with value: 0.45228641223565036 and parameters: {'learning_rate': 0.006708985402155976, 'subsample': 0.4019317030502952, 'colsample_bytree': 0.4995196756302996, 'min_child_samples': 20, 'num_leaves': 662}. Best is trial 13 with value: 0.45228641223565036.\n",
      "[I 2023-11-21 14:18:51,908] Trial 14 finished with value: 0.4679703012775917 and parameters: {'learning_rate': 0.009110013355620833, 'subsample': 0.40587610919957545, 'colsample_bytree': 0.3625713537543647, 'min_child_samples': 36, 'num_leaves': 687}. Best is trial 13 with value: 0.45228641223565036.\n",
      "[I 2023-11-21 14:18:56,579] Trial 15 finished with value: 0.44491889961791514 and parameters: {'learning_rate': 0.08538648795388477, 'subsample': 0.38404037985013173, 'colsample_bytree': 0.498207439286346, 'min_child_samples': 35, 'num_leaves': 678}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:00,017] Trial 16 finished with value: 0.4534286733960903 and parameters: {'learning_rate': 0.09970682275036434, 'subsample': 0.3593416108926655, 'colsample_bytree': 0.4166526735522039, 'min_child_samples': 35, 'num_leaves': 710}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:03,320] Trial 17 finished with value: 0.4666925709424738 and parameters: {'learning_rate': 0.09909565063638189, 'subsample': 0.3280054759452771, 'colsample_bytree': 0.3743826674083961, 'min_child_samples': 44, 'num_leaves': 885}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:07,906] Trial 18 finished with value: 0.46710965135785415 and parameters: {'learning_rate': 0.06437647701272971, 'subsample': 0.2936018218648819, 'colsample_bytree': 0.34383239283437844, 'min_child_samples': 30, 'num_leaves': 565}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:11,402] Trial 19 finished with value: 0.4537187518710503 and parameters: {'learning_rate': 0.029960624806812438, 'subsample': 0.4135757771377759, 'colsample_bytree': 0.48617143342192304, 'min_child_samples': 43, 'num_leaves': 835}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:15,453] Trial 20 finished with value: 0.447940897337388 and parameters: {'learning_rate': 0.010722926587551451, 'subsample': 0.433902354799973, 'colsample_bytree': 0.45015459185509876, 'min_child_samples': 30, 'num_leaves': 584}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:19,356] Trial 21 finished with value: 0.4516003748922974 and parameters: {'learning_rate': 0.011206719773903487, 'subsample': 0.43524358052312223, 'colsample_bytree': 0.44511714041588407, 'min_child_samples': 31, 'num_leaves': 592}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:23,558] Trial 22 finished with value: 0.4489556716781 and parameters: {'learning_rate': 0.01244390082823864, 'subsample': 0.44218419993494124, 'colsample_bytree': 0.4454279495542907, 'min_child_samples': 29, 'num_leaves': 532}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:28,130] Trial 23 finished with value: 0.4623308057706593 and parameters: {'learning_rate': 0.024946832736474595, 'subsample': 0.4379199559044892, 'colsample_bytree': 0.4529755655591175, 'min_child_samples': 55, 'num_leaves': 363}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:31,740] Trial 24 finished with value: 0.46122172738328937 and parameters: {'learning_rate': 0.043393934601476405, 'subsample': 0.3520566353077483, 'colsample_bytree': 0.3949906793765289, 'min_child_samples': 43, 'num_leaves': 793}. Best is trial 15 with value: 0.44491889961791514.\n",
      "[I 2023-11-21 14:19:35,930] Trial 25 finished with value: 0.4342331555289691 and parameters: {'learning_rate': 0.05913905272326255, 'subsample': 0.45031259532779655, 'colsample_bytree': 0.45559473938434125, 'min_child_samples': 28, 'num_leaves': 520}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:39,668] Trial 26 finished with value: 0.4684920769871428 and parameters: {'learning_rate': 0.06349947862234823, 'subsample': 0.4978238756541076, 'colsample_bytree': 0.4070831257980554, 'min_child_samples': 86, 'num_leaves': 400}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:43,126] Trial 27 finished with value: 0.4602439747018141 and parameters: {'learning_rate': 0.05753110952080387, 'subsample': 0.379904787194608, 'colsample_bytree': 0.33468367674304317, 'min_child_samples': 39, 'num_leaves': 624}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:45,390] Trial 28 finished with value: 0.45822004063085203 and parameters: {'learning_rate': 0.027616105352891038, 'subsample': 0.4260205449783876, 'colsample_bytree': 0.46619988624829567, 'min_child_samples': 48, 'num_leaves': 509}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:50,962] Trial 29 finished with value: 0.44806583391439925 and parameters: {'learning_rate': 0.07724833057753773, 'subsample': 0.46236799124240197, 'colsample_bytree': 0.395533806404369, 'min_child_samples': 27, 'num_leaves': 795}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:52,610] Trial 30 finished with value: 0.46823287131172575 and parameters: {'learning_rate': 0.053991160415325694, 'subsample': 0.44797219120502024, 'colsample_bytree': 0.42843842525265574, 'min_child_samples': 76, 'num_leaves': 919}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:19:59,010] Trial 31 finished with value: 0.4445983514656934 and parameters: {'learning_rate': 0.07230087884538487, 'subsample': 0.4507615905473845, 'colsample_bytree': 0.3950430134574376, 'min_child_samples': 26, 'num_leaves': 619}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:20:02,181] Trial 32 finished with value: 0.4402175677984128 and parameters: {'learning_rate': 0.07388919727589129, 'subsample': 0.4662571397680406, 'colsample_bytree': 0.46171942265511695, 'min_child_samples': 34, 'num_leaves': 646}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:20:04,255] Trial 33 finished with value: 0.4533557329196081 and parameters: {'learning_rate': 0.08026816350786252, 'subsample': 0.474522796777465, 'colsample_bytree': 0.46380079851149847, 'min_child_samples': 34, 'num_leaves': 725}. Best is trial 25 with value: 0.4342331555289691.\n",
      "[I 2023-11-21 14:20:09,084] Trial 34 finished with value: 0.42965076984451656 and parameters: {'learning_rate': 0.052552056156990906, 'subsample': 0.463491458124007, 'colsample_bytree': 0.42373067592669533, 'min_child_samples': 25, 'num_leaves': 646}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:11,752] Trial 35 finished with value: 0.43220440949326205 and parameters: {'learning_rate': 0.04716520440085523, 'subsample': 0.49523373514285723, 'colsample_bytree': 0.4137788714503278, 'min_child_samples': 25, 'num_leaves': 470}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:16,046] Trial 36 finished with value: 0.43398724655866266 and parameters: {'learning_rate': 0.049806423742470726, 'subsample': 0.4929917703501335, 'colsample_bytree': 0.4239966896405894, 'min_child_samples': 25, 'num_leaves': 442}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:20,608] Trial 37 finished with value: 0.43619076085413055 and parameters: {'learning_rate': 0.042321513672650204, 'subsample': 0.4936108267298752, 'colsample_bytree': 0.42451923059293306, 'min_child_samples': 24, 'num_leaves': 350}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:23,616] Trial 38 finished with value: 0.45651220522616326 and parameters: {'learning_rate': 0.05066521196633169, 'subsample': 0.4961136017675955, 'colsample_bytree': 0.314490050654138, 'min_child_samples': 40, 'num_leaves': 458}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:28,163] Trial 39 finished with value: 0.4631976644848161 and parameters: {'learning_rate': 0.03570734361430684, 'subsample': 0.47072672543292043, 'colsample_bytree': 0.38474015915781096, 'min_child_samples': 51, 'num_leaves': 507}. Best is trial 34 with value: 0.42965076984451656.\n",
      "[I 2023-11-21 14:20:31,832] Trial 40 finished with value: 0.42419066258170246 and parameters: {'learning_rate': 0.05154594962883767, 'subsample': 0.4598121663033559, 'colsample_bytree': 0.4171881546022748, 'min_child_samples': 20, 'num_leaves': 255}. Best is trial 40 with value: 0.42419066258170246.\n",
      "[I 2023-11-21 14:20:35,312] Trial 41 finished with value: 0.42909073841273526 and parameters: {'learning_rate': 0.050392051361916976, 'subsample': 0.46543483050439177, 'colsample_bytree': 0.42013781364978237, 'min_child_samples': 24, 'num_leaves': 266}. Best is trial 40 with value: 0.42419066258170246.\n",
      "[I 2023-11-21 14:20:39,956] Trial 42 finished with value: 0.4226681296589453 and parameters: {'learning_rate': 0.04770626576261023, 'subsample': 0.4691471215531713, 'colsample_bytree': 0.4237429377975127, 'min_child_samples': 20, 'num_leaves': 113}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:20:43,544] Trial 43 finished with value: 0.4327282718696499 and parameters: {'learning_rate': 0.033543756001998506, 'subsample': 0.4720077919405392, 'colsample_bytree': 0.3641533452750781, 'min_child_samples': 20, 'num_leaves': 113}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:20:48,128] Trial 44 finished with value: 0.43769211081988246 and parameters: {'learning_rate': 0.03830408490319354, 'subsample': 0.4624991830778099, 'colsample_bytree': 0.4056704971809904, 'min_child_samples': 24, 'num_leaves': 203}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:20:51,474] Trial 45 finished with value: 0.4266611840502412 and parameters: {'learning_rate': 0.022992859099597925, 'subsample': 0.41226154880404103, 'colsample_bytree': 0.4335061492881147, 'min_child_samples': 20, 'num_leaves': 108}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:20:55,060] Trial 46 finished with value: 0.4298177792276609 and parameters: {'learning_rate': 0.021979530481551875, 'subsample': 0.4187121121283618, 'colsample_bytree': 0.4344874444022684, 'min_child_samples': 20, 'num_leaves': 104}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:20:59,338] Trial 47 finished with value: 0.4538440144997527 and parameters: {'learning_rate': 0.02879675288326685, 'subsample': 0.4198232874948399, 'colsample_bytree': 0.3784225604022436, 'min_child_samples': 31, 'num_leaves': 254}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:02,574] Trial 48 finished with value: 0.43761396236543093 and parameters: {'learning_rate': 0.03526245629083986, 'subsample': 0.45454141727850805, 'colsample_bytree': 0.353773516239124, 'min_child_samples': 23, 'num_leaves': 100}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:05,099] Trial 49 finished with value: 0.4402955674228977 and parameters: {'learning_rate': 0.04367117287181349, 'subsample': 0.3926893887324634, 'colsample_bytree': 0.37783161180098424, 'min_child_samples': 20, 'num_leaves': 157}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:07,387] Trial 50 finished with value: 0.4613701489191473 and parameters: {'learning_rate': 0.017464725573809445, 'subsample': 0.4246622801004741, 'colsample_bytree': 0.43140007843297473, 'min_child_samples': 60, 'num_leaves': 248}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:12,096] Trial 51 finished with value: 0.42636039057538583 and parameters: {'learning_rate': 0.023582625771989683, 'subsample': 0.4070661186293407, 'colsample_bytree': 0.4339315470736534, 'min_child_samples': 20, 'num_leaves': 88}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:16,121] Trial 52 finished with value: 0.44322530389163534 and parameters: {'learning_rate': 0.023335283650491673, 'subsample': 0.48047335635610433, 'colsample_bytree': 0.4016232642053001, 'min_child_samples': 23, 'num_leaves': 63}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:19,522] Trial 53 finished with value: 0.44339849231757855 and parameters: {'learning_rate': 0.029300232001461406, 'subsample': 0.3997687837962434, 'colsample_bytree': 0.4761939009138587, 'min_child_samples': 28, 'num_leaves': 158}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:23,087] Trial 54 finished with value: 0.44186026776461845 and parameters: {'learning_rate': 0.033183191831209526, 'subsample': 0.4386434612009684, 'colsample_bytree': 0.417872988489677, 'min_child_samples': 32, 'num_leaves': 216}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:25,826] Trial 55 finished with value: 0.4296981509456157 and parameters: {'learning_rate': 0.02009698924434777, 'subsample': 0.4788722788982728, 'colsample_bytree': 0.47256158138576343, 'min_child_samples': 22, 'num_leaves': 293}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:29,349] Trial 56 finished with value: 0.4363994430071348 and parameters: {'learning_rate': 0.0402681734065825, 'subsample': 0.4164589646260864, 'colsample_bytree': 0.44313984671507356, 'min_child_samples': 27, 'num_leaves': 52}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:32,512] Trial 57 finished with value: 0.4355081565842231 and parameters: {'learning_rate': 0.05275840892605679, 'subsample': 0.4554765354726644, 'colsample_bytree': 0.4366639477598994, 'min_child_samples': 20, 'num_leaves': 123}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:35,042] Trial 58 finished with value: 0.46512268785019695 and parameters: {'learning_rate': 0.06806768599137311, 'subsample': 0.43071995690174636, 'colsample_bytree': 0.24689756568350085, 'min_child_samples': 71, 'num_leaves': 322}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:39,266] Trial 59 finished with value: 0.4489618346840534 and parameters: {'learning_rate': 0.04598034804147859, 'subsample': 0.3772839267318349, 'colsample_bytree': 0.47877068003622253, 'min_child_samples': 38, 'num_leaves': 74}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:42,715] Trial 60 finished with value: 0.44448660047129196 and parameters: {'learning_rate': 0.06208493910914696, 'subsample': 0.403712062622279, 'colsample_bytree': 0.49737065313848816, 'min_child_samples': 33, 'num_leaves': 155}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:44,865] Trial 61 finished with value: 0.43744015251764623 and parameters: {'learning_rate': 0.019887454654753344, 'subsample': 0.477061832260812, 'colsample_bytree': 0.471584968491029, 'min_child_samples': 25, 'num_leaves': 304}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:48,521] Trial 62 finished with value: 0.4295886927390826 and parameters: {'learning_rate': 0.02586805355706382, 'subsample': 0.47794521710052723, 'colsample_bytree': 0.45549744510923945, 'min_child_samples': 23, 'num_leaves': 237}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:52,140] Trial 63 finished with value: 0.4430506124629058 and parameters: {'learning_rate': 0.024205051075636492, 'subsample': 0.44550294457196554, 'colsample_bytree': 0.4548479409915838, 'min_child_samples': 29, 'num_leaves': 231}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:21:55,869] Trial 64 finished with value: 0.4245870058766461 and parameters: {'learning_rate': 0.026716714653710758, 'subsample': 0.4582896636042023, 'colsample_bytree': 0.4161822863952031, 'min_child_samples': 22, 'num_leaves': 26}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:01,020] Trial 65 finished with value: 0.4341528862440165 and parameters: {'learning_rate': 0.014524708465271144, 'subsample': 0.48503009161651944, 'colsample_bytree': 0.4097984731150061, 'min_child_samples': 22, 'num_leaves': 198}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:03,044] Trial 66 finished with value: 0.47708691666915415 and parameters: {'learning_rate': 0.02665312328606197, 'subsample': 0.4352632297009374, 'colsample_bytree': 0.44753196376074744, 'min_child_samples': 100, 'num_leaves': 13}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:04,812] Trial 67 finished with value: 0.4488371875768975 and parameters: {'learning_rate': 0.03276423743742234, 'subsample': 0.41501664969237345, 'colsample_bytree': 0.38811814931596217, 'min_child_samples': 28, 'num_leaves': 134}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:08,659] Trial 68 finished with value: 0.4327715511427279 and parameters: {'learning_rate': 0.025151253443944716, 'subsample': 0.44854953779468815, 'colsample_bytree': 0.43508418591847087, 'min_child_samples': 23, 'num_leaves': 88}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:12,014] Trial 69 finished with value: 0.43638820127372313 and parameters: {'learning_rate': 0.03982228369726725, 'subsample': 0.4825403061525888, 'colsample_bytree': 0.4603824630696539, 'min_child_samples': 31, 'num_leaves': 27}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:15,314] Trial 70 finished with value: 0.42618079687999627 and parameters: {'learning_rate': 0.03167069298754709, 'subsample': 0.4338521793962059, 'colsample_bytree': 0.4865138128652159, 'min_child_samples': 22, 'num_leaves': 171}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:20,374] Trial 71 finished with value: 0.42536110485327583 and parameters: {'learning_rate': 0.0315036998169653, 'subsample': 0.4595180169158516, 'colsample_bytree': 0.4917425137388524, 'min_child_samples': 20, 'num_leaves': 178}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:24,015] Trial 72 finished with value: 0.42819206233839213 and parameters: {'learning_rate': 0.031626416718896225, 'subsample': 0.45927266709881426, 'colsample_bytree': 0.49872481053499274, 'min_child_samples': 20, 'num_leaves': 187}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:27,362] Trial 73 finished with value: 0.42789012446088137 and parameters: {'learning_rate': 0.03126812148976232, 'subsample': 0.42919002644633464, 'colsample_bytree': 0.48656178836561464, 'min_child_samples': 20, 'num_leaves': 181}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:30,761] Trial 74 finished with value: 0.44149358332047994 and parameters: {'learning_rate': 0.029989830511546606, 'subsample': 0.4297389329326861, 'colsample_bytree': 0.4869246201142529, 'min_child_samples': 27, 'num_leaves': 37}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:34,789] Trial 75 finished with value: 0.4331601325730113 and parameters: {'learning_rate': 0.03767002083681472, 'subsample': 0.4108450502478128, 'colsample_bytree': 0.48245910573976464, 'min_child_samples': 22, 'num_leaves': 174}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:37,182] Trial 76 finished with value: 0.4391053054827172 and parameters: {'learning_rate': 0.017530419975095513, 'subsample': 0.4418707405207565, 'colsample_bytree': 0.46917682176440073, 'min_child_samples': 26, 'num_leaves': 71}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:39,460] Trial 77 finished with value: 0.47876926568828515 and parameters: {'learning_rate': 0.022202618080255056, 'subsample': 0.38776969411071827, 'colsample_bytree': 0.48229813732157667, 'min_child_samples': 83, 'num_leaves': 134}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:42,801] Trial 78 finished with value: 0.45438072730155626 and parameters: {'learning_rate': 0.02763205692190752, 'subsample': 0.42552626200826826, 'colsample_bytree': 0.45002519906496813, 'min_child_samples': 20, 'num_leaves': 6}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:45,661] Trial 79 finished with value: 0.4451079933856662 and parameters: {'learning_rate': 0.03640082165332323, 'subsample': 0.3701589579844144, 'colsample_bytree': 0.4880904601976474, 'min_child_samples': 30, 'num_leaves': 131}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:48,033] Trial 80 finished with value: 0.4395572730259136 and parameters: {'learning_rate': 0.05900563618202071, 'subsample': 0.39815394615641203, 'colsample_bytree': 0.44144394193317843, 'min_child_samples': 25, 'num_leaves': 47}. Best is trial 42 with value: 0.4226681296589453.\n",
      "[I 2023-11-21 14:22:52,091] Trial 81 finished with value: 0.42049257783045446 and parameters: {'learning_rate': 0.044003653606792495, 'subsample': 0.46109394814578664, 'colsample_bytree': 0.49833000276908107, 'min_child_samples': 20, 'num_leaves': 183}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:22:55,734] Trial 82 finished with value: 0.4305684241619697 and parameters: {'learning_rate': 0.045170908086908663, 'subsample': 0.4570745992152243, 'colsample_bytree': 0.4646962056960105, 'min_child_samples': 22, 'num_leaves': 170}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:00,052] Trial 83 finished with value: 0.4265701289511831 and parameters: {'learning_rate': 0.03018578879760998, 'subsample': 0.4391691182730356, 'colsample_bytree': 0.4994906161429482, 'min_child_samples': 21, 'num_leaves': 91}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:03,840] Trial 84 finished with value: 0.4298433359056496 and parameters: {'learning_rate': 0.04199049424873716, 'subsample': 0.49963821474250003, 'colsample_bytree': 0.49524120282955403, 'min_child_samples': 22, 'num_leaves': 99}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:07,434] Trial 85 finished with value: 0.43416996758524645 and parameters: {'learning_rate': 0.035653055982900354, 'subsample': 0.4459536090235274, 'colsample_bytree': 0.4645821048782592, 'min_child_samples': 27, 'num_leaves': 73}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:11,250] Trial 86 finished with value: 0.44960491372024974 and parameters: {'learning_rate': 0.020362540276913494, 'subsample': 0.4685235760233759, 'colsample_bytree': 0.49943242436818613, 'min_child_samples': 36, 'num_leaves': 143}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:14,884] Trial 87 finished with value: 0.4355447889104087 and parameters: {'learning_rate': 0.028153763470141577, 'subsample': 0.40586688904974677, 'colsample_bytree': 0.4782911807325536, 'min_child_samples': 24, 'num_leaves': 218}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:17,514] Trial 88 finished with value: 0.4384064746038379 and parameters: {'learning_rate': 0.022898875996480973, 'subsample': 0.45920043685384143, 'colsample_bytree': 0.4284814109759944, 'min_child_samples': 29, 'num_leaves': 281}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:20,255] Trial 89 finished with value: 0.4362049203992794 and parameters: {'learning_rate': 0.04863649844335354, 'subsample': 0.44265110870600094, 'colsample_bytree': 0.44477450214997805, 'min_child_samples': 26, 'num_leaves': 44}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:23,871] Trial 90 finished with value: 0.431829347680489 and parameters: {'learning_rate': 0.05581242236780793, 'subsample': 0.46901841440567854, 'colsample_bytree': 0.4142754237876334, 'min_child_samples': 23, 'num_leaves': 98}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:27,643] Trial 91 finished with value: 0.42483434246348123 and parameters: {'learning_rate': 0.03274124022282287, 'subsample': 0.43331390034195055, 'colsample_bytree': 0.4877454896258292, 'min_child_samples': 20, 'num_leaves': 174}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:31,940] Trial 92 finished with value: 0.4239572238947213 and parameters: {'learning_rate': 0.038706804341228467, 'subsample': 0.4878997745216252, 'colsample_bytree': 0.4744215544221646, 'min_child_samples': 21, 'num_leaves': 209}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:36,163] Trial 93 finished with value: 0.4320354547125245 and parameters: {'learning_rate': 0.04065731460365937, 'subsample': 0.4873980497828688, 'colsample_bytree': 0.4724005077894611, 'min_child_samples': 22, 'num_leaves': 201}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:39,516] Trial 94 finished with value: 0.4277906900798588 and parameters: {'learning_rate': 0.03549373724618361, 'subsample': 0.4521198514904643, 'colsample_bytree': 0.4887515296808077, 'min_child_samples': 25, 'num_leaves': 265}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:43,370] Trial 95 finished with value: 0.42477953272689334 and parameters: {'learning_rate': 0.03164086317829024, 'subsample': 0.48581915856400637, 'colsample_bytree': 0.45623764083045826, 'min_child_samples': 21, 'num_leaves': 153}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:46,628] Trial 96 finished with value: 0.43343467860813967 and parameters: {'learning_rate': 0.04535634456638468, 'subsample': 0.4843417395392632, 'colsample_bytree': 0.4560908455003562, 'min_child_samples': 24, 'num_leaves': 332}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:51,349] Trial 97 finished with value: 0.422700257515868 and parameters: {'learning_rate': 0.03987034865879906, 'subsample': 0.4883591445330426, 'colsample_bytree': 0.47174575186919915, 'min_child_samples': 21, 'num_leaves': 387}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:23:56,309] Trial 98 finished with value: 0.43673367187812345 and parameters: {'learning_rate': 0.03881951770769323, 'subsample': 0.4880347457480092, 'colsample_bytree': 0.4689258387325086, 'min_child_samples': 29, 'num_leaves': 1022}. Best is trial 81 with value: 0.42049257783045446.\n",
      "[I 2023-11-21 14:24:00,067] Trial 99 finished with value: 0.43881606055110195 and parameters: {'learning_rate': 0.06560807407088874, 'subsample': 0.4721428649308395, 'colsample_bytree': 0.46027291584023433, 'min_child_samples': 26, 'num_leaves': 405}. Best is trial 81 with value: 0.42049257783045446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE 0.42049257783045446\n",
      "Best params: {'learning_rate': 0.044003653606792495, 'subsample': 0.46109394814578664, 'colsample_bytree': 0.49833000276908107, 'min_child_samples': 20, 'num_leaves': 183}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    param = {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 0.5),\n",
    "        \"subsample_freq\":1,\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 0.5),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    model.fit(train_x, train_y)\n",
    "    y_hat = model.predict(test_x)\n",
    "    \n",
    "    return mean_absolute_error(test_y, y_hat)\n",
    "    ### END CODE HERE\n",
    "\n",
    "# Remember to assign \"lgbm-wine-1\" as the study_name to your Optuna study. \n",
    "study_name = \"lgbm-wine-1\"\n",
    "\n",
    "# For this assignment, it is enough to use a simple sqlite3 database for persisting study history\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# Create (and run) the study and record the history in the SQlite3 database file\n",
    "### START CODE HERE\n",
    "study = optuna.create_study(\n",
    "    direction = \"minimize\",\n",
    "    sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    study_name = study_name,\n",
    "    storage = storage,\n",
    "    load_if_exists = True\n",
    ")\n",
    "study.optimize(objective, n_trials=100)\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76dd15",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.42049257783045446\n",
    "\n",
    "Best params: {'colsample_bytree': 0.49833000276908107, 'learning_rate': 0.044003653606792495, 'min_child_samples': 20, 'num_leaves': 183, 'subsample': 0.46109394814578664}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and run this cell if you want to delete the study created in assignment 1 in the database\n",
    "\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-1\", storage=\"sqlite:///optuna.sqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff141c0",
   "metadata": {},
   "source": [
    "## Assignment 2: Analyzing an Optuna study (2 points)\n",
    "Optuna offers utility functions for visualizing the optimization process (i.e., study history). For example, it can plot the hyperparameter importance and the relationship between a hyperparameter and the objective. In this assignment, you need to analyze the study created in Assignment 1, adjust the search ranges of some hyperparameters to obtain better MAE.\n",
    "\n",
    "This assignment is divided into three parts and you need to answer a question after completing the coding in each part. For each question, **please put your answer to the same Markdown cell as the question.** Detailed instructions of each part can be found below. (There is not a single correct answer to these questions, so any reasonable answer will be considered for points.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# Configure Jupyter Notebook to render plotly figures drawn by Optuna\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7db4ce",
   "metadata": {},
   "source": [
    "### 2a) Hyperparameter importance\n",
    "Load the \"lgbm-wine-1\" study created in Assignment 1 and plot the importance of the hyperparameters in the study. Similar to the tutorial, use the [FanovaImportanceEvaluator](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator) as the importance evaluator and set `RANDOM_STATE` as the seed for the evaluator for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de9c8f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56495b0b9ae677e4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the hyperparameter importance\n",
    "study_name = \"lgbm-wine-1\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "### START CODE HERE\n",
    "study = None\n",
    "fig = None\n",
    "fig.show()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f056c23",
   "metadata": {},
   "source": [
    "### Question for Assignment 2a\n",
    "What are the most two important hyperparameters in the \"lgbm-wine-1\" study created in Assignment 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8161",
   "metadata": {},
   "source": [
    "### 2b) The impact of hyperparameters\n",
    "Plot the relationships of the most two important hyperparameters and the objective in a slice plot. \n",
    "\n",
    "Hint: [How to plot the relationship between a hyperparameter and the objective?](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dd0bf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cd0ab0fecac2d570",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the relationships between the most two important hyperparameters and the objective\n",
    "\n",
    "### START CODE HERE\n",
    "fig = None\n",
    "fig.show()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55828f",
   "metadata": {},
   "source": [
    "### Question for Assignment 2b\n",
    "Now you know the most two important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which area the points that resulted in better MAE are concentrated on? How would you adjust the search ranges of these two hyperparameters in the next study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b6a3",
   "metadata": {},
   "source": [
    "### 2c) An improved Optuna study\n",
    "Create a new study named ***lgbm-wine-2*** and use your observation in Assignment 2b to adjust the search ranges of the most two important hyperparameters to improve MAE (keep values/search ranges of other hyperparameters as the same as in Assignment 1).\n",
    "\n",
    "In this assignment, you need to:\n",
    "1. run 20 trials in this study, starting with the best hyperparameter combination of the previous study.\n",
    "1. use TPESampler (with `RANDOM_SEED` as the seed) for hyperparameter sampling,\n",
    "1. track the trials in an MLflow experiment,\n",
    "1. persist the study history in the \"optuna.sqlite3\" database file,\n",
    "\n",
    "**Hints**: \n",
    "- [How to log Optuna trials to MLflow?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.MLflowCallback.html)\n",
    "- To start the new study using the best hyperparameter combination of the previous study, you can 1) load the previous study named \"lgbm-wine-1\", 2) retrieve the optimal hyperparameters combination of the \"lgbm-wine-1\" study, 3) create a new study, and 4) start the new study from the optimal hyperparameters combination of the previous study. \n",
    "\n",
    "You may also find the following links helpful:\n",
    "- [optuna.load_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.load_study.html)\n",
    "- [study.best_params](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.best_params)\n",
    "- [enqueue_trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.enqueue_trial). \n",
    "\n",
    "**Note**:\n",
    "- You will find two approaches for logging trials to an MLflow service: 1) using a callback or 2) using a decorator. The code skeleton has been designed for the first approach but feel free to modify the code if you prefer the second one.  \n",
    "- If you want to redo this assignment, please remember to first delete the study from the database as well as the trials logged to MLflow. If you want to delete all the trials in MLflow, don't delete the experiment, as deleting the experiment using the UI will not really delete the experiment in the PostgreSQL database used by MLflow, which will cause problems when recording trials under an experiment with the same name as the deleted experiment. Instead, you can keep the experiment and delete the trials, as shown in the image below. (If you feel like permanently deleting MLflow experiments from the PostgreSQL database, please check [here](https://version.helsinki.fi/luoyumo/engineering_of_ml_systems/-/blob/master/mlflow.md?ref_type=heads)).\n",
    "\n",
    "![](./images/mlflow-delete-trials.jpg)\n",
    "\n",
    "<details>\n",
    "    <summary>If callback and decorator are new to you...</summary>\n",
    "    <p>Briefly speaking, both callbacks and decorators are used to modify or enhance the behavior of another function without modifying its original source code. A callback is a function that is typically provided as an argument to another function. A decorator typically takes another function as an argument. Feel free to google more about them. </p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee89bc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a0685c1f324066ae",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# name of the previous study in Assignment 1\n",
    "prev_study_name = \"lgbm-wine-1\"\n",
    "\n",
    "# name of the new study\n",
    "new_study_name = \"lgbm-wine-2\"\n",
    "\n",
    "# Load the \"lgbm-wine-1\" study created in Assignment 1, create a new study that performs 20 trials, starting with the optimal hyperparameter combination \n",
    "# of the \"lgbm-wine-1\" study. You also need to log the trials to MLflow while running the new study\n",
    "### START CODE HERE\n",
    "# Load the previous study\n",
    "prev_study = None\n",
    "\n",
    "# define the callback function that logs trials to MLflow\n",
    "mlflc = None\n",
    "\n",
    "# Create a new study\n",
    "study = None\n",
    "\n",
    "# Run 20 trials, starting with the best combination of the previous study\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and run this cell if you want to delete the study created in assignment 2 in the database\n",
    "\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-2\", storage=\"sqlite:///optuna.sqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58366bb",
   "metadata": {},
   "source": [
    "### Question for Assignment 2c\n",
    "What are the best MAE and the best hyperparameter combination obtained from the new study (lgbm-wine-2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929df5b",
   "metadata": {},
   "source": [
    "### Screenshot for Assignment 2c\n",
    "Submit a screenshot of the trails of the \"lgbm-wine-2\" study in MLflow UI. **Please put the screenshot into your PDF file.**\n",
    "\n",
    "<details>\n",
    "    <summary>Example:</summary>\n",
    "    <img src=\"./images/trials-mlflow.png\" width=1000/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a66a70",
   "metadata": {},
   "source": [
    "## Assignment 3: More about MLflow (2 points)\n",
    "### 3a) Find the MLflow run with the best hyperparameter combination\n",
    "In assignment 2c), you found the best hyperparameter combination for your model. Now, your task is to complete the `find_best_run_id` function. The function receives an MLflow Experiment name as the argument and returns the best MLflow Run ID that resulted in the best hyperparameter combination. In other words, this function should find the MLflow Run with the smallest MAE. \n",
    "\n",
    "You may find the following MLflow docs useful:\n",
    "- [How to retrieve an MLflow experiment given an experiment name?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=get_experiment_by_name#mlflow.get_experiment_by_name)\n",
    "- [How to search runs inside an experiment?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=search_runs#mlflow.search_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a4a5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5167cdef32da16d4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "def find_best_run_id(mlflow_experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the ID of the MLflow run with the smallest MAE\n",
    "    Args:\n",
    "        mlflow_experiment_name (str): The name of the MLflow experiment where the run should be found\n",
    "    Return:\n",
    "        An MLflow run ID\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_name = \"lgbm-wine-2\" # same as the Optuna study name \n",
    "best_run_id = find_best_run_id(mlflow_experiment_name)\n",
    "print(best_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Train a model using the best hyperparameter combination\n",
    "Your task is to train a model using the best hyperparameter combination found in Assignment 2c). You also need to upload and register the model to MLflow, the model needs to be associated with the MLflow Run where the optimal hyperparameter combination was found.\n",
    "\n",
    "For example, suppose running the \"lgbm-wine-2\" study of Assignment 2 created an MLflow Run 14 where the best hyperparameter combination was found, the model created in this assignment needs to be associated with MLflow Run 14, as shown below:\n",
    "\n",
    "<img src=\"./images/mlflow-best-run.png\" width=1000/>\n",
    "\n",
    "<img src=\"./images/mlflow-best-model.png\" width=1000/>\n",
    "\n",
    "Hints:\n",
    "- You may find the following function helpful: [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter).\n",
    "- It would probably be more convenient to retrieve the best hyperparameter combination using `optuna.load_study` rather than from the Mlflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"lgbm-wine-2\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71744984",
   "metadata": {},
   "source": [
    "### Screenshots for Assignment 3b)\n",
    "Submit the screenshots of the registered model version info and the corresponding MLflow run. You can take the images in the assignment instructions above as an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dfc29",
   "metadata": {},
   "source": [
    "## Assignment 4: Conditional search spaces with Optuna (2 points)\n",
    "Similar to Example 2 in the Optuna tutorial, your task is to create a new study named \"multimodel-wine\" that optimizes the hyperparameters of a LightGBM and an [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) regression model for the red wine quality prediction use case and see which library is better in the use case. Similar to the previous assignments, the objective is also a smaller MAE. The hyperparameters to be tuned and their search ranges are given below. Use TPESampler with `RANDOM_SEED` as the random seed. You don't need to track the trials in MLflow or persist them in a relational database. This study should perform 20 trials. \n",
    "\n",
    "**LightGBM**: \n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 1.0]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_samples | A leaf node should have at least `min_child_samples` data points to be further splitted. | integer | [1, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value) \n",
    "\n",
    "**XGBoost**\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | Same as LightGBM.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | Same as LightGBM.             | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | Same as LightGBM.            | float   | [0.05, 1.0]                                                              |                                         \n",
    "| colsample_bytree  | Same as LightGBM.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_weight | Minimum sum of instance weight needed for a leaf mode to be further splitted (similar to min_child_samples in LightGBN). | integer | [1, 100]                                                                |\n",
    "| max_depth        | Max depth of a single tree                                       | integer | [1, 10]                                                                |\n",
    "| random_state      | Same as LightGBM.                                   | integer | RANDOM_SEED (fixed value)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd88b5a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a6549ecacde675f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "study_name = \"multimodel-wine\"\n",
    "\n",
    "# Create and run the study\n",
    "### START CODE HERE\n",
    "study = None\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519e22e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.41046696687679574\n",
    "\n",
    "Best params: {'regressor': 'lgbm', 'learning_rate': 0.08779135743847766, 'subsample': 0.9716999552570724, 'colsample_bytree': 0.9953849922441481, 'min_child_samples': 39, 'num_leaves': 633}\n",
    "\n",
    "or\n",
    "\n",
    "Best MAE 0.42251256346702576\n",
    "\n",
    "Best params: {'regressor': 'xgb', 'learning_rate': 0.08770632679265472, 'subsample': 0.9837224327754701, 'colsample_bytree': 0.9992869046800568, 'min_child_weight': 45, 'max_depth': 7}\n",
    "\n",
    "(This depends on the element order in the list that is used for configuring the search space for the model type.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eabfa",
   "metadata": {},
   "source": [
    "## Assignment 5: Ensemble averaging (2 points)\n",
    "Similar to Example 3 in the tutorial, use Optuna to find the optimal weight combination to combine the predictions of an Sklearn's RandomForest, a XGBoost, and a LightGBM model to obtain better predictions for the red wine quality us case. Detailed instructions are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdb0a7",
   "metadata": {},
   "source": [
    "Before going to the assignment, first train three models for the red wine quality use case using Sklearn's RandomForest, XGBoost, and LightGBM and evaluate MAE of each model. For simplicity, the default configurations of the hyperparameters are used except for \"random_state\" which is set as `RANDOM_SEED` as in the previous assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca98e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "xgb_model = xgb.XGBRegressor(random_state=RANDOM_SEED)\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "model_names = [\"rf_model\", \"xgb_model\", \"lgbm_model\"]\n",
    "\n",
    "for name in model_names:\n",
    "    model = eval(name)\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    print(f\"{name}: {mean_absolute_error(test_y, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e656f5",
   "metadata": {},
   "source": [
    "You can see the best performing model is the random forest one, with an MAE of 0.4228.\n",
    "\n",
    "Now, it is time to combine the predictions of these models to improve the final prediction. In this assignment, your task is to create a new study named ***ensemble*** to find the best weight combination for the three models that have been trained in the cell above to obtain smaller MAE. Use 3 as the [step of discretization](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) when searching weight for each model. Use TPESampler (with `RANDOM_SEED` as the seed) in the study. You don't need to track the trials in MLflow or persist the study history in a database in this assignment. Run 20 trials in the study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab33a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c58cfc6b2eb0a37f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "\n",
    "def objective(trial):\n",
    "    pass\n",
    "    \n",
    "### END CODE HERE\n",
    "\n",
    "# Create and run the study\n",
    "### START CODE HERE\n",
    "study = None\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1550bb0",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.4111545941609471\n",
    "\n",
    "Best params: {'rf_model': 76, 'xgb_model': 82, 'lgbm_model': 19}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a01494",
   "metadata": {},
   "source": [
    "## Assignment 6: Parallelizing the optimization process (2 points)\n",
    "Let's go back to Assignment 1, where you tried to find the optimal hyperparameter combination for you LightGBM regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4763a",
   "metadata": {},
   "source": [
    "Let's load the \"lgbm-wine-1\" study created in Assignment 1 and use [optuna.visualization.plot_timeline](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_timeline.html) to show the timeline of the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"lgbm-wine-1\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "fig = optuna.visualization.plot_timeline(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2357b69",
   "metadata": {},
   "source": [
    "You can notice that the trials were performed sequentially. To parallelize a study using multi-processing, a relational database is needed so that multiple processes can share the same view of how the optimization is progressing. Recall that an Sqlite database file was used to store studies in some of the previous assignments. However, as mentioned in this [Optuna doc](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html), Sqlite is not recommended when parallelizing study using multi-processing because it may cause deadlocks and serious performance issues. The Optuna community recommends using a database server, such as MySQL and PostgreSQL. Recall that the MLOps platform already has a PostgreSQL database server, which is originally used by MLflow. This PostgreSQL database server will also be used to persist a study in this assignment. \n",
    "\n",
    "**Note**: You may already notice that parallelization can also be achieved by setting the argument `n_jobs` in `optuna.study.Study.optimize()`. However, Optuna will use multiple threads (instead of processes) to parallelize a study when setting `n_jobs!=1`. Multi-threading will not speed up the study due to the Python Global Interpreter Lock, which is a lock that allows only one thread to execute Python bytecode at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d2009",
   "metadata": {},
   "source": [
    "Before going to the assignment, some preparation is needed to enable the study parallelization. First, you need to create a connection between your local environment and the PostgreSQL database server running in the MLOps platform as the database server is not exposed to external users by default. Open a new terminal and run the following command: \n",
    "```bash\n",
    "kubectl -n mlflow port-forward svc/postgres 5432:5432\n",
    "\n",
    "# You should see\n",
    "Forwarding from 127.0.0.1:5432 -> 5432\n",
    "Forwarding from [::1]:5432 -> 5432\n",
    "```\n",
    "After doing this, the PostgreSQL database server is listening on port 5432 in your local environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5357778",
   "metadata": {},
   "source": [
    "Next, we need to create a database named \"optuna_db\" in the PostgreSQL database server. This database will be used by Optuna to persist studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf191bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "POSTGRES_USER = \"mlflow\"\n",
    "POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "POSTGRES_DB = \"optuna_db\" #name of the database used by Optuna\n",
    "storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "            POSTGRES_USER,\n",
    "            POSTGRES_PASSWORD,\n",
    "            POSTGRES_DB\n",
    "        )\n",
    "\n",
    "# Establish a connection to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if the \"optuna_db\" database exists\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'optuna_db';\")\n",
    "database_exists = cursor.fetchone()\n",
    "\n",
    "if not database_exists:\n",
    "    # Create the \"optuna_db\" database if it doesn't exist\n",
    "    cursor.execute(\"CREATE DATABASE optuna_db;\")\n",
    "    print(f\"Database {POSTGRES_DB} created.\")\n",
    "else:\n",
    "    print(f\"Database {POSTGRES_DB} already exists.\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb27cf",
   "metadata": {},
   "source": [
    "Now, time for the assignment!\n",
    "\n",
    "In this assignment, create a new study named ***lgbm-wine-parallel*** which uses the same objective function in Assignment 1. Recall that the \"lgbm-wine-1\" study performed 100 trials in a single run in Assignment 1. In other words, the study was executed only once. In this assignment, you need to parallelize 100 trials by running the \"lgbm-wine-parallel\" study 10 times in parallel using 4 processes. You don't need to track the trials in MLflow. \n",
    "\n",
    "Instead of opening 10 terminals and manually running the study 10 times (as shown in this [Optuna doc](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)), you need to do the parallelization in an automatic way, e.g., using the joblib package as exemplified in the tutorial. \n",
    "\n",
    "**Hints**: \n",
    "- Unlike assignment 1 where the random seed of the TPESampler is fixed to 42, don't fix the random seed of the TPESampler here, otherwise you will get repeated hyperparameter combination in multiple trials. \n",
    "- If you need to parallelize 100 trials by running a study 10 times, how many trials the study should perform in a single run?\n",
    "- You may encounter an error complaining that the study is already existing if you create the study every time in a new run. A workaround could be create the study only once and load the study every time when starting a new run of the study.\n",
    "\n",
    "**Note**: Remember to delete the study from the database if you want to restart the assignment. Code for deleting the study can be found after the assignment code cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8003e",
   "metadata": {},
   "source": [
    "Define the database and the study name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db506490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "POSTGRES_USER = \"mlflow\"\n",
    "POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "POSTGRES_DB = \"optuna_db\" \n",
    "\n",
    "# Similar to \"sqlite:///optuna.sqlite3\" in some of the previous assignments,\n",
    "# the database used in this assignment is this\n",
    "storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "            POSTGRES_USER,\n",
    "            POSTGRES_PASSWORD,\n",
    "            POSTGRES_DB\n",
    "        )\n",
    "\n",
    "# Use this as the study name\n",
    "study_name = \"lgbm-wine-parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c1a6e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e6d15b06e8354073",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "# Parallelize the study using multi-processing\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546248c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following code if you want to delete the \"lgbm-wine-parallel\" study from the PostgreSQL database\n",
    "\n",
    "# POSTGRES_USER = \"mlflow\"\n",
    "# POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "# POSTGRES_DB = \"optuna_db\" \n",
    "\n",
    "# storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "#             POSTGRES_USER,\n",
    "#             POSTGRES_PASSWORD,\n",
    "#             POSTGRES_DB\n",
    "#         )\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-parallel\", storage=storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeline of the \"lgbm-wine-parallel\" study\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "fig = optuna.visualization.plot_timeline(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, you should see some of the trials were executed in parallel.\n",
    "\n",
    "<details>\n",
    "    <summary> Example </summary>\n",
    "    <img src=\"./images/parallel-timeplot.png\" width=1000/>\n",
    "</details>\n",
    "\n",
    "(You may find the running time of the parallel optimization is almost the same as or even longer than the sequential optimization in the first assignment, this is probably because the database used in this assignment is a remote one (in your cPouta VM). As a result, it takes time for the local processes to communicate with the remote database. If you change the database to this remote one in the first assignment, the running time is likely to increase to around 5 minutes.)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
