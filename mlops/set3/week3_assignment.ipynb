{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932690c",
   "metadata": {},
   "source": [
    "# Week3 Assignments\n",
    "You will get some hands-on experience with Optuna through this week's assignments. Similar to the tutorial of the first week, the [red wine dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) will be used in this week's assignments. \n",
    "\n",
    "**Guidelines of submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to answer questions (in text) or capture screenshots in order to earn points. For each question, please put your text answers **in the same Markdown cell as the question** and your screenshots into a single PDF file. For each screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your screenshots in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# MLflow service URI\n",
    "mlflow_tracking_uri = \"http://mlflow-server.local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and testing data\n",
    "data = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = data.drop(\"quality\", axis=1)\n",
    "y = data[\"quality\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of the original dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dimension of train_x is {train_x.shape}\")\n",
    "print(f\"The dimension of train_y is {train_y.shape}\")\n",
    "print(f\"The dimension of test_x is {test_x.shape}\")\n",
    "print(f\"The dimension of train_x is {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {},
   "source": [
    "## Assignment 1: The basic use of Optuna (2 points)\n",
    "Train a [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) to predict red wine quality and use Optuna to find the optimal hyperparameter combination for the model. **Please use the sklearn API as you did in the first week's assignments.**\n",
    "\n",
    "This assignment has the following requirements:\n",
    "\n",
    "1) Create an Optuna study named ***lgbm-wine-1***. The target of the optimization is to minimize the MAE (mean absolute error) of the model when evaluating the model against the testing dataset. The hyperparameters to be tuned and their search ranges are shown below. Some of the hyperparameter values are fixed. The hyperparameter values should be sampled in a linear domain if not separately specified. \n",
    "\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 0.5]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 0.5]                                                              |\n",
    "| min_child_samples | A leaf node should have `min_child_samples` data points to be further splitted. | integer | [20, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value, RANDOM_SEED has been defined as a variable in a previous cell)                                                |\n",
    "\n",
    "2) Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use `RANDOM_SEED` as the seed of the sampler. \n",
    "\n",
    "3) The Optuna study should perform 100 trials.\n",
    "\n",
    "4) The study history should be persisted in a relational database (`optuna.sqlite3`) so that the study can be loaded and analyzed later. \n",
    "\n",
    "Hints:\n",
    "- [How to sample hyperparameter values in the logarithmic domain?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float)\n",
    "- [How to configure a study to use a specific sampler and persist study history in a specific database?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html#optuna-create-study)\n",
    "- It may take some time run 100 trials, so you could test your code using a smaller number of trials and increase the number to 100 after you're sure your code works. Remember to delete the study before rerunning the study to avoid the error of duplicated study. The code for deleting an existing study from a database can be found after the assignment code cell of this assignment. \n",
    "\n",
    "\n",
    "*More reading material: If you are interested, [the LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters) explains the use of each hyperparameter in more details.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b96c3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-15d27f8fdf5190ad",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "# Remember to assign \"lgbm-wine-1\" as the study_name to your Optuna study. \n",
    "study_name = \"lgbm-wine-1\"\n",
    "\n",
    "# For this assignment, it is enough to use a simple sqlite3 database for persisting study history\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# Create (and run) the study and record the history in the SQlite3 database file\n",
    "### START CODE HERE\n",
    "study = None\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76dd15",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.42049257783045446\n",
    "\n",
    "Best params: {'colsample_bytree': 0.49833000276908107, 'learning_rate': 0.044003653606792495, 'min_child_samples': 20, 'num_leaves': 183, 'subsample': 0.46109394814578664}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and run this cell if you want to delete the study created in assignment 1 in the database\n",
    "\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-1\", storage=\"sqlite:///optuna.sqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff141c0",
   "metadata": {},
   "source": [
    "## Assignment 2: Analyzing an Optuna study (2 points)\n",
    "Optuna offers utility functions for visualizing the optimization process (i.e., study history). For example, it can plot the hyperparameter importance and the relationship between a hyperparameter and the objective. In this assignment, you need to analyze the study created in Assignment 1, adjust the search ranges of some hyperparameters to obtain better MAE.\n",
    "\n",
    "This assignment is divided into three parts and you need to answer a question after completing the coding in each part. For each question, **please put your answer to the same Markdown cell as the question.** Detailed instructions of each part can be found below. (There is not a single correct answer to these questions, so any reasonable answer will be considered for points.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# Configure Jupyter Notebook to render plotly figures drawn by Optuna\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7db4ce",
   "metadata": {},
   "source": [
    "### 2a) Hyperparameter importance\n",
    "Load the \"lgbm-wine-1\" study created in Assignment 1 and plot the importance of the hyperparameters in the study. Similar to the tutorial, use the [FanovaImportanceEvaluator](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator) as the importance evaluator and set `RANDOM_STATE` as the seed for the evaluator for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de9c8f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56495b0b9ae677e4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the hyperparameter importance\n",
    "study_name = \"lgbm-wine-1\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "### START CODE HERE\n",
    "study = None\n",
    "fig = None\n",
    "fig.show()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f056c23",
   "metadata": {},
   "source": [
    "### Question for Assignment 2a\n",
    "What are the most two important hyperparameters in the \"lgbm-wine-1\" study created in Assignment 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8161",
   "metadata": {},
   "source": [
    "### 2b) The impact of hyperparameters\n",
    "Plot the relationships of the most two important hyperparameters and the objective in a slice plot. \n",
    "\n",
    "Hint: [How to plot the relationship between a hyperparameter and the objective?](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dd0bf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cd0ab0fecac2d570",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the relationships between the most two important hyperparameters and the objective\n",
    "\n",
    "### START CODE HERE\n",
    "fig = None\n",
    "fig.show()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55828f",
   "metadata": {},
   "source": [
    "### Question for Assignment 2b\n",
    "Now you know the most two important hyperparameters that affect the objective value. Looking at the positions of the points in the slice plot, which area the points that resulted in better MAE are concentrated on? How would you adjust the search ranges of these two hyperparameters in the next study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b6a3",
   "metadata": {},
   "source": [
    "### 2c) An improved Optuna study\n",
    "Create a new study named ***lgbm-wine-2*** and use your observation in Assignment 2b to adjust the search ranges of the most two important hyperparameters to improve MAE (keep values/search ranges of other hyperparameters as the same as in Assignment 1).\n",
    "\n",
    "In this assignment, you need to:\n",
    "1. run 20 trials in this study, starting with the best hyperparameter combination of the previous study.\n",
    "1. use TPESampler (with `RANDOM_SEED` as the seed) for hyperparameter sampling,\n",
    "1. track the trials in an MLflow experiment,\n",
    "1. persist the study history in the \"optuna.sqlite3\" database file,\n",
    "\n",
    "**Hints**: \n",
    "- [How to log Optuna trials to MLflow?](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.MLflowCallback.html)\n",
    "- To start the new study using the best hyperparameter combination of the previous study, you can 1) load the previous study named \"lgbm-wine-1\", 2) retrieve the optimal hyperparameters combination of the \"lgbm-wine-1\" study, 3) create a new study, and 4) start the new study from the optimal hyperparameters combination of the previous study. \n",
    "\n",
    "You may also find the following links helpful:\n",
    "- [optuna.load_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.load_study.html)\n",
    "- [study.best_params](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.best_params)\n",
    "- [enqueue_trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.enqueue_trial). \n",
    "\n",
    "**Note**:\n",
    "- You will find two approaches for logging trials to an MLflow service: 1) using a callback or 2) using a decorator. The code skeleton has been designed for the first approach but feel free to modify the code if you prefer the second one.  \n",
    "- If you want to redo this assignment, please remember to first delete the study from the database as well as the trials logged to MLflow. If you want to delete all the trials in MLflow, don't delete the experiment, as deleting the experiment using the UI will not really delete the experiment in the PostgreSQL database used by MLflow, which will cause problems when recording trials under an experiment with the same name as the deleted experiment. Instead, you can keep the experiment and delete the trials, as shown in the image below. (If you feel like permanently deleting MLflow experiments from the PostgreSQL database, please check [here](https://version.helsinki.fi/luoyumo/engineering_of_ml_systems/-/blob/master/mlflow.md?ref_type=heads)).\n",
    "\n",
    "![](./images/mlflow-delete-trials.jpg)\n",
    "\n",
    "<details>\n",
    "    <summary>If callback and decorator are new to you...</summary>\n",
    "    <p>Briefly speaking, both callbacks and decorators are used to modify or enhance the behavior of another function without modifying its original source code. A callback is a function that is typically provided as an argument to another function. A decorator typically takes another function as an argument. Feel free to google more about them. </p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee89bc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a0685c1f324066ae",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "# name of the previous study in Assignment 1\n",
    "prev_study_name = \"lgbm-wine-1\"\n",
    "\n",
    "# name of the new study\n",
    "new_study_name = \"lgbm-wine-2\"\n",
    "\n",
    "# Load the \"lgbm-wine-1\" study created in Assignment 1, create a new study that performs 20 trials, starting with the optimal hyperparameter combination \n",
    "# of the \"lgbm-wine-1\" study. You also need to log the trials to MLflow while running the new study\n",
    "### START CODE HERE\n",
    "# Load the previous study\n",
    "prev_study = None\n",
    "\n",
    "# define the callback function that logs trials to MLflow\n",
    "mlflc = None\n",
    "\n",
    "# Create a new study\n",
    "study = None\n",
    "\n",
    "# Run 20 trials, starting with the best combination of the previous study\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below and run this cell if you want to delete the study created in assignment 2 in the database\n",
    "\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-2\", storage=\"sqlite:///optuna.sqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58366bb",
   "metadata": {},
   "source": [
    "### Question for Assignment 2c\n",
    "What are the best MAE and the best hyperparameter combination obtained from the new study (lgbm-wine-2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929df5b",
   "metadata": {},
   "source": [
    "### Screenshot for Assignment 2c\n",
    "Submit a screenshot of the trails of the \"lgbm-wine-2\" study in MLflow UI. **Please put the screenshot into your PDF file.**\n",
    "\n",
    "<details>\n",
    "    <summary>Example:</summary>\n",
    "    <img src=\"./images/trials-mlflow.png\" width=1000/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a66a70",
   "metadata": {},
   "source": [
    "## Assignment 3: More about MLflow (2 points)\n",
    "### 3a) Find the MLflow run with the best hyperparameter combination\n",
    "In assignment 2c), you found the best hyperparameter combination for your model. Now, your task is to complete the `find_best_run_id` function. The function receives an MLflow Experiment name as the argument and returns the best MLflow Run ID that resulted in the best hyperparameter combination. In other words, this function should find the MLflow Run with the smallest MAE. \n",
    "\n",
    "You may find the following MLflow docs useful:\n",
    "- [How to retrieve an MLflow experiment given an experiment name?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=get_experiment_by_name#mlflow.get_experiment_by_name)\n",
    "- [How to search runs inside an experiment?](https://mlflow.org/docs/2.3.2/python_api/mlflow.html?highlight=search_runs#mlflow.search_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700a4a5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5167cdef32da16d4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "def find_best_run_id(mlflow_experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the ID of the MLflow run with the smallest MAE\n",
    "    Args:\n",
    "        mlflow_experiment_name (str): The name of the MLflow experiment where the run should be found\n",
    "    Return:\n",
    "        An MLflow run ID\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_name = \"lgbm-wine-2\" # same as the Optuna study name \n",
    "best_run_id = find_best_run_id(mlflow_experiment_name)\n",
    "print(best_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Train a model using the best hyperparameter combination\n",
    "Your task is to train a model using the best hyperparameter combination found in Assignment 2c). You also need to upload and register the model to MLflow, the model needs to be associated with the MLflow Run where the optimal hyperparameter combination was found.\n",
    "\n",
    "For example, suppose running the \"lgbm-wine-2\" study of Assignment 2 created an MLflow Run 14 where the best hyperparameter combination was found, the model created in this assignment needs to be associated with MLflow Run 14, as shown below:\n",
    "\n",
    "<img src=\"./images/mlflow-best-run.png\" width=1000/>\n",
    "\n",
    "<img src=\"./images/mlflow-best-model.png\" width=1000/>\n",
    "\n",
    "Hints:\n",
    "- You may find the following function helpful: [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter).\n",
    "- It would probably be more convenient to retrieve the best hyperparameter combination using `optuna.load_study` rather than from the Mlflow Run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"lgbm-wine-2\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71744984",
   "metadata": {},
   "source": [
    "### Screenshots for Assignment 3b)\n",
    "Submit the screenshots of the registered model version info and the corresponding MLflow run. You can take the images in the assignment instructions above as an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dfc29",
   "metadata": {},
   "source": [
    "## Assignment 4: Conditional search spaces with Optuna (2 points)\n",
    "Similar to Example 2 in the Optuna tutorial, your task is to create a new study named \"multimodel-wine\" that optimizes the hyperparameters of a LightGBM and an [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) regression model for the red wine quality prediction use case and see which library is better in the use case. Similar to the previous assignments, the objective is also a smaller MAE. The hyperparameters to be tuned and their search ranges are given below. Use TPESampler with `RANDOM_SEED` as the random seed. You don't need to track the trials in MLflow or persist them in a relational database. This study should perform 20 trials. \n",
    "\n",
    "**LightGBM**: \n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | The number of decision trees.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | The percentage of training samples to be used to train each tree. `subsample*100%` of the training samples will be randomly selected for training.        | float   | [0.05, 1.0]                                                              |\n",
    "| subsample_freq    | Subsampling frequency. The subsampling will be performed again after `subsample_freq` trees have been trained.                                                     | integer | 1 (fixed value)                                                          |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_samples | A leaf node should have at least `min_child_samples` data points to be further splitted. | integer | [1, 100]                                                                |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | RANDOM_SEED (fixed value) \n",
    "\n",
    "**XGBoost**\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| n_estimators      | Same as LightGBM.                                               | integer | 1000 (fixed value)                                                       |\n",
    "| learning_rate     | Same as LightGBM.             | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| subsample         | Same as LightGBM.            | float   | [0.05, 1.0]                                                              |                                         \n",
    "| colsample_bytree  | Same as LightGBM.                | float   | [0.05, 1.0]                                                              |\n",
    "| min_child_weight | Minimum sum of instance weight needed for a leaf mode to be further splitted (similar to min_child_samples in LightGBN). | integer | [1, 100]                                                                |\n",
    "| max_depth        | Max depth of a single tree                                       | integer | [1, 10]                                                                |\n",
    "| random_state      | Same as LightGBM.                                   | integer | RANDOM_SEED (fixed value)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd88b5a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a6549ecacde675f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "study_name = \"multimodel-wine\"\n",
    "\n",
    "# Create and run the study\n",
    "### START CODE HERE\n",
    "study = None\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519e22e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.41046696687679574\n",
    "\n",
    "Best params: {'regressor': 'lgbm', 'learning_rate': 0.08779135743847766, 'subsample': 0.9716999552570724, 'colsample_bytree': 0.9953849922441481, 'min_child_samples': 39, 'num_leaves': 633}\n",
    "\n",
    "or\n",
    "\n",
    "Best MAE 0.42251256346702576\n",
    "\n",
    "Best params: {'regressor': 'xgb', 'learning_rate': 0.08770632679265472, 'subsample': 0.9837224327754701, 'colsample_bytree': 0.9992869046800568, 'min_child_weight': 45, 'max_depth': 7}\n",
    "\n",
    "(This depends on the element order in the list that is used for configuring the search space for the model type.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eabfa",
   "metadata": {},
   "source": [
    "## Assignment 5: Ensemble averaging (2 points)\n",
    "Similar to Example 3 in the tutorial, use Optuna to find the optimal weight combination to combine the predictions of an Sklearn's RandomForest, a XGBoost, and a LightGBM model to obtain better predictions for the red wine quality us case. Detailed instructions are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdb0a7",
   "metadata": {},
   "source": [
    "Before going to the assignment, first train three models for the red wine quality use case using Sklearn's RandomForest, XGBoost, and LightGBM and evaluate MAE of each model. For simplicity, the default configurations of the hyperparameters are used except for \"random_state\" which is set as `RANDOM_SEED` as in the previous assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca98e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "xgb_model = xgb.XGBRegressor(random_state=RANDOM_SEED)\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "model_names = [\"rf_model\", \"xgb_model\", \"lgbm_model\"]\n",
    "\n",
    "for name in model_names:\n",
    "    model = eval(name)\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    print(f\"{name}: {mean_absolute_error(test_y, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e656f5",
   "metadata": {},
   "source": [
    "You can see the best performing model is the random forest one, with an MAE of 0.4228.\n",
    "\n",
    "Now, it is time to combine the predictions of these models to improve the final prediction. In this assignment, your task is to create a new study named ***ensemble*** to find the best weight combination for the three models that have been trained in the cell above to obtain smaller MAE. Use 3 as the [step of discretization](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) when searching weight for each model. Use TPESampler (with `RANDOM_SEED` as the seed) in the study. You don't need to track the trials in MLflow or persist the study history in a database in this assignment. Run 20 trials in the study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab33a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c58cfc6b2eb0a37f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "\n",
    "def objective(trial):\n",
    "    pass\n",
    "    \n",
    "### END CODE HERE\n",
    "\n",
    "# Create and run the study\n",
    "### START CODE HERE\n",
    "study = None\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"Best MAE\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1550bb0",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "Best MAE 0.4111545941609471\n",
    "\n",
    "Best params: {'rf_model': 76, 'xgb_model': 82, 'lgbm_model': 19}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a01494",
   "metadata": {},
   "source": [
    "## Assignment 6: Parallelizing the optimization process (2 points)\n",
    "Let's go back to Assignment 1, where you tried to find the optimal hyperparameter combination for you LightGBM regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4763a",
   "metadata": {},
   "source": [
    "Let's load the \"lgbm-wine-1\" study created in Assignment 1 and use [optuna.visualization.plot_timeline](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_timeline.html) to show the timeline of the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"lgbm-wine-1\"\n",
    "storage = \"sqlite:///optuna.sqlite3\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "fig = optuna.visualization.plot_timeline(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2357b69",
   "metadata": {},
   "source": [
    "You can notice that the trials were performed sequentially. To parallelize a study using multi-processing, a relational database is needed so that multiple processes can share the same view of how the optimization is progressing. Recall that an Sqlite database file was used to store studies in some of the previous assignments. However, as mentioned in this [Optuna doc](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html), Sqlite is not recommended when parallelizing study using multi-processing because it may cause deadlocks and serious performance issues. The Optuna community recommends using a database server, such as MySQL and PostgreSQL. Recall that the MLOps platform already has a PostgreSQL database server, which is originally used by MLflow. This PostgreSQL database server will also be used to persist a study in this assignment. \n",
    "\n",
    "**Note**: You may already notice that parallelization can also be achieved by setting the argument `n_jobs` in `optuna.study.Study.optimize()`. However, Optuna will use multiple threads (instead of processes) to parallelize a study when setting `n_jobs!=1`. Multi-threading will not speed up the study due to the Python Global Interpreter Lock, which is a lock that allows only one thread to execute Python bytecode at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d2009",
   "metadata": {},
   "source": [
    "Before going to the assignment, some preparation is needed to enable the study parallelization. First, you need to create a connection between your local environment and the PostgreSQL database server running in the MLOps platform as the database server is not exposed to external users by default. Open a new terminal and run the following command: \n",
    "```bash\n",
    "kubectl -n mlflow port-forward svc/postgres 5432:5432\n",
    "\n",
    "# You should see\n",
    "Forwarding from 127.0.0.1:5432 -> 5432\n",
    "Forwarding from [::1]:5432 -> 5432\n",
    "```\n",
    "After doing this, the PostgreSQL database server is listening on port 5432 in your local environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5357778",
   "metadata": {},
   "source": [
    "Next, we need to create a database named \"optuna_db\" in the PostgreSQL database server. This database will be used by Optuna to persist studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf191bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "POSTGRES_USER = \"mlflow\"\n",
    "POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "POSTGRES_DB = \"optuna_db\" #name of the database used by Optuna\n",
    "storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "            POSTGRES_USER,\n",
    "            POSTGRES_PASSWORD,\n",
    "            POSTGRES_DB\n",
    "        )\n",
    "\n",
    "# Establish a connection to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if the \"optuna_db\" database exists\n",
    "cursor.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'optuna_db';\")\n",
    "database_exists = cursor.fetchone()\n",
    "\n",
    "if not database_exists:\n",
    "    # Create the \"optuna_db\" database if it doesn't exist\n",
    "    cursor.execute(\"CREATE DATABASE optuna_db;\")\n",
    "    print(f\"Database {POSTGRES_DB} created.\")\n",
    "else:\n",
    "    print(f\"Database {POSTGRES_DB} already exists.\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb27cf",
   "metadata": {},
   "source": [
    "Now, time for the assignment!\n",
    "\n",
    "In this assignment, create a new study named ***lgbm-wine-parallel*** which uses the same objective function in Assignment 1. Recall that the \"lgbm-wine-1\" study performed 100 trials in a single run in Assignment 1. In other words, the study was executed only once. In this assignment, you need to parallelize 100 trials by running the \"lgbm-wine-parallel\" study 10 times in parallel using 4 processes. You don't need to track the trials in MLflow. \n",
    "\n",
    "Instead of opening 10 terminals and manually running the study 10 times (as shown in this [Optuna doc](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)), you need to do the parallelization in an automatic way, e.g., using the joblib package as exemplified in the tutorial. \n",
    "\n",
    "**Hints**: \n",
    "- Unlike assignment 1 where the random seed of the TPESampler is fixed to 42, don't fix the random seed of the TPESampler here, otherwise you will get repeated hyperparameter combination in multiple trials. \n",
    "- If you need to parallelize 100 trials by running a study 10 times, how many trials the study should perform in a single run?\n",
    "- You may encounter an error complaining that the study is already existing if you create the study every time in a new run. A workaround could be create the study only once and load the study every time when starting a new run of the study.\n",
    "\n",
    "**Note**: Remember to delete the study from the database if you want to restart the assignment. Code for deleting the study can be found after the assignment code cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8003e",
   "metadata": {},
   "source": [
    "Define the database and the study name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db506490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "POSTGRES_USER = \"mlflow\"\n",
    "POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "POSTGRES_DB = \"optuna_db\" \n",
    "\n",
    "# Similar to \"sqlite:///optuna.sqlite3\" in some of the previous assignments,\n",
    "# the database used in this assignment is this\n",
    "storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "            POSTGRES_USER,\n",
    "            POSTGRES_PASSWORD,\n",
    "            POSTGRES_DB\n",
    "        )\n",
    "\n",
    "# Use this as the study name\n",
    "study_name = \"lgbm-wine-parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c1a6e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e6d15b06e8354073",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the objective\n",
    "def objective(trial):\n",
    "    ### START CODE HERE\n",
    "    pass\n",
    "    ### END CODE HERE\n",
    "\n",
    "# Parallelize the study using multi-processing\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546248c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following code if you want to delete the \"lgbm-wine-parallel\" study from the PostgreSQL database\n",
    "\n",
    "# POSTGRES_USER = \"mlflow\"\n",
    "# POSTGRES_PASSWORD = \"KFSg-AYoiPdfRun64z2-w89Kk7z5cJL2IbVvSd3l8Og\"\n",
    "# POSTGRES_DB = \"optuna_db\" \n",
    "\n",
    "# storage=\"postgresql://{}:{}@localhost:5432/{}\".format(\n",
    "#             POSTGRES_USER,\n",
    "#             POSTGRES_PASSWORD,\n",
    "#             POSTGRES_DB\n",
    "#         )\n",
    "# optuna.delete_study(study_name=\"lgbm-wine-parallel\", storage=storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeline of the \"lgbm-wine-parallel\" study\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "fig = optuna.visualization.plot_timeline(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, you should see some of the trials were executed in parallel.\n",
    "\n",
    "<details>\n",
    "    <summary> Example </summary>\n",
    "    <img src=\"./images/parallel-timeplot.png\" width=1000/>\n",
    "</details>\n",
    "\n",
    "(You may find the running time of the parallel optimization is almost the same as or even longer than the sequential optimization in the first assignment, this is probably because the database used in this assignment is a remote one (in your cPouta VM). As a result, it takes time for the local processes to communicate with the remote database. If you change the database to this remote one in the first assignment, the running time is likely to increase to around 5 minutes.)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
