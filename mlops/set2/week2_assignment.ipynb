{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6802fe72",
   "metadata": {},
   "source": [
    "# Week 2 assignments \n",
    "In this week's assignments, you will practice data cleaning and validation, feature extraction, and feature engineering. You will use Pandas to write scripts to automate the data wrangling parts and [Great Expectations](https://greatexpectations.io/) for data validation.\n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to answer questions or capture screenshots in order to earn points. Please put all your answers and the required screenshots into a single PDF file. For each answer or screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your answers and screenshots in your submission. In the last assignment, you will need to add some code to `etl.py`. Besides this assignment notebook and the PDF file, please also include `etl.py` in your submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226046f6",
   "metadata": {},
   "source": [
    "Import all the packages needed in the assignments and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import great_expectations as gx\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "from thefuzz import fuzz, process\n",
    "\n",
    "from great_expectations.data_context import EphemeralDataContext, FileDataContext\n",
    "from great_expectations.datasource.fluent import BatchRequest\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.expectations.expectation import Expectation\n",
    "from great_expectations.checkpoint.types.checkpoint_result import CheckpointResult\n",
    "\n",
    "\n",
    "# Random seed for making the assignments reproducible\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Path of current working directory\n",
    "WORKING_DIR = Path.cwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55b4eb",
   "metadata": {},
   "source": [
    "## Assignment 1: Data extraction (2 points)\n",
    "We will be working with a synthetic housing price dataset (based on [this](https://www.kaggle.com/datasets/htagholdings/property-sales) and [this](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction) dataset), which lists house prices at the Canberra area in Australia from 2009 to 2016. The data description is provided in the file [Data_description.csv](./Data_description.csv). We will use this dataset to train a regression model to predict prices of houses given some info about the houses. \n",
    "\n",
    "### 1a) Load the data\n",
    "Let's first load the training dataset, which consists of two files. The file `deals.csv` holds the price information for the sold houses along with some minor information about the houses. The file `house_info.json` has more detailed information about the houses. Complete the function `file_reader` that loads both files into Pandas DataFrames. \n",
    "\n",
    "Hints:\n",
    "1. The `.json` file is saved in a \"list-like\" format, use parameter \"`orient='records'`\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reader(path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read files of deals and house information into DataFrames\n",
    "    Args:\n",
    "        path (Path): Path to the folder where the files exist.\n",
    "    Returns:\n",
    "        a tuple consisting of a Pandas DataFrame of deals and a Pandas DataFrame of house information\n",
    "    \"\"\"\n",
    "    \n",
    "    price_file_path = path / \"deals.csv\"\n",
    "    house_file_path = path / \"house_info.json\" \n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d020c5",
   "metadata": {},
   "source": [
    "Now, run the cell below to call the function you just created. Let's take a quick look at the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "prices, house_data = file_reader(train_path)\n",
    "\n",
    "print(f\"DataFrame 'price' ({prices.shape[0]} rows):\") \n",
    "display(prices.head())\n",
    "print(f\"\\nDataFrame 'house_data' ({house_data.shape[0]} rows):\") \n",
    "display(house_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fd040",
   "metadata": {},
   "source": [
    "Expected outputs:\n",
    "\n",
    "<img src=\"./images/ex1-outputs.png\" width=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932e443",
   "metadata": {},
   "source": [
    "It seems that both DataFrames share a date column as well as four other columns (building year, number of bedrooms, postcode and area), so we can use these columns to merge the datasets into one. To be able to do so, we need to make sure the column names and types are same in both DataFrames.\n",
    "\n",
    "### 1b) Check column types\n",
    "Let's first check the data type of the shared columns. Complete the `show_col_dtype` function to print the dtype of a given column of a given DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_col_dtype(df: pd.DataFrame, col_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Print the dtype of a column of a DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame whose column needs to be checked\n",
    "        col_name(str): the name of the column to be checked\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    col_type = None\n",
    "    ### END CODE HERE\n",
    "\n",
    "    print(f\"The dtype of column {col_name} is: {col_type} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fa2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date column\n",
    "show_col_dtype(prices, \"datesold\")\n",
    "show_col_dtype(house_data, \"date\")\n",
    "\n",
    "# building year column\n",
    "show_col_dtype(prices, \"building_year\")\n",
    "show_col_dtype(house_data, \"yr_built\")\n",
    "\n",
    "# bedroom number column\n",
    "show_col_dtype(prices, \"bedrooms\")\n",
    "show_col_dtype(house_data, \"bedrooms\")\n",
    "\n",
    "# postcode column\n",
    "show_col_dtype(prices, \"postcode\")\n",
    "show_col_dtype(house_data, \"postcode\")\n",
    "\n",
    "# area column\n",
    "show_col_dtype(prices, \"area\")\n",
    "show_col_dtype(house_data, \"area\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f2081",
   "metadata": {},
   "source": [
    "From the output, you should see that only the date columns have different dtypes:\n",
    "\n",
    "The dtype of column datesold is: object \n",
    "\n",
    "The dtype of column date is: datetime64[ns] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330080d0",
   "metadata": {},
   "source": [
    "### 1c) Merge price and house_data DataFrames\n",
    "Now, it seems that only the column `datesold` needs to be converted to datetime format. Additionally, one of the building year columns needs to be renamed to the same as the other one. Do the following in the `dataframe_merge` function:\n",
    "1. Convert the `datesold` column in the price DataFrame into `panda.datetime` format and store it in a new column named 'date' in the same DataFrame. (Do not delete the `datesold` column for now.)\n",
    "1. Rename the column `building_year` to `yr_built` in the price DataFrame.\n",
    "1. Merge the DataFrames using columns `date`, `postcode`, `bedrooms`, `area` and `yr_built` as keys.\n",
    "\n",
    "The following functions may be helpful:\n",
    "- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.to_datetime.html)\n",
    "- [pandas.DataFrame.rename](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.rename.html)\n",
    "- [pandas.merge](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.merge.html)\n",
    "\n",
    "NOTE: _In reality, we could not rely on these 5 columns to form a unique indicator for the merge to succeed. However, in the scope of this exercise we know a priori that this property holds for our data._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_merger(prices: pd.DataFrame, house_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge the two DataFrames given as inputs\n",
    "    Args:\n",
    "        prices (DataFrame): A pandas DataFrame holding the price information\n",
    "        house_data (DataFrame): A pandas DataFrame holding the detailed information about the sold houses\n",
    "    Returns:\n",
    "        df: The merged pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567aa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataframe_merger(prices, house_data)\n",
    "print(f\"DataFrame 'train_df' ({train_df.shape[0]} rows):\") \n",
    "display(train_df.head())\n",
    "print(f\"The merged DataFrame should have {len(train_df.columns)} columns: {train_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c3282",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "DataFrame 'train_df' (11046 rows):\n",
    "| | datesold | price | yr_built | bedrooms | postcode | area | date | bathrooms | yr_renovated | condition | ... | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | 01-April-2009 | 437100 | 1977 | 4 | 2906 | Conder | 2009-04-01 | 2.50 | NaN | satisfactory | ... | \n",
    "| 1 | 01-April-2009 | 461300 | 1969 | 4\t| 2615 | Kippax Centre | 2009-04-01 | 2.25 | NaN | Satisfactory\t| ... |\t\n",
    "| 2 | 01-April-2009 | 280900 | 1966 | 3\t| 2615 | Higgins | 2009-04-01 |\t1.25 | NaN | Satisfactory | ... |\t\n",
    "| 3 | 01-April-2009 | 351600 | 1978 | 3 | 2620 | Tinderry | 2009-04-01 | 2.00 |\tNaN | Satisfactory | ... |\t\n",
    "| 4 | 01-April-2009 | 431300 | 2005 | 3 | 2914 | Bonner | 2009-04-01 | 2.00\t| NaN | Satisfactory | ... |\t\n",
    "\n",
    "The merged DataFrame should have 22 columns: Index(['datesold', 'price', 'yr_built', 'bedrooms', 'postcode', 'area', 'date',\n",
    "       'bathrooms', 'yr_renovated', 'condition', 'grade', 'floors',\n",
    "       'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement',\n",
    "       'sqft_living15', 'sqft_lot15', 'waterfront', 'view', 'distance',\n",
    "       'prev_owner'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1805063",
   "metadata": {},
   "source": [
    "## Assignment 2: Data cleaning (2 points)\n",
    "### 2a) Missing values\n",
    "We have successfully extracted our data, we need to do some cleaning. By now we've noticed there are missing values in some columns. Let's start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b96c3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-15d27f8fdf5190ad",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f4e31",
   "metadata": {},
   "source": [
    "The column `yr_renovated` contains $\\frac{11045}{11046}$ missing values. With such a high fraction, it is not likely that any imputation scheme would result in a usable feature. So let's just drop that column as useless. While we're at it, we might also drop some other columns as well: \n",
    "- The name of the previous owner will not help in our prediction task, so the column `prev_owner` can be dropped as well. \n",
    "- The column `datesold` contains redundant duplicate information (the same info is included in the column `date`), so we won't need that either. \n",
    "- The values in the `sqft_living` column represent the summation of the values in the `sqft_above` and `sqft_basement` columns, so any one of the `sqft_above` and `sqft_basement` columns is individually redundant.\n",
    "\n",
    "The columns `sqft_living15` and `sqft_lot15` have roughly only 1.5% and 10% of values missing respectively so we'll be able to use them. We will deal with the missing values in these columns later in the feature engineering part.\n",
    "\n",
    "Remove the three columns `yr_renovated`, `prev_owner`, `datesold`, and `sqft_above` from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_futile_columns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Removes unneeded columns from the argument DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03176b26",
   "metadata": {},
   "source": [
    "Run the cell below to see if our function works and the columns are indeed dropped. At this point, we should have 18 columns remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1090449",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_futile_columns(train_df)\n",
    "print(\"The remaining columns are:\", train_df.columns)\n",
    "\n",
    "print(\"The total number of columns is\", len(train_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad648af",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "The remaining columns are: Index(['price', 'yr_built', 'bedrooms', 'postcode', 'area', 'date',\n",
    "       'bathrooms', 'condition', 'grade', 'floors', 'sqft_living', 'sqft_lot',\n",
    "       'sqft_basement', 'sqft_living15', 'sqft_lot15', 'waterfront', 'view',\n",
    "       'distance'],\n",
    "      dtype='object')\n",
    "The total number of columns is 18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa28c2",
   "metadata": {},
   "source": [
    "### 2b) Inconsistent entries\n",
    "\n",
    "It's very common for the extracted data to contain inconsistencies. Let's take a closer look at our columns to see if we can spot some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c10d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed572116",
   "metadata": {},
   "source": [
    "This gives us an overview of all numerical data in the form of some statistics. In general, everything else seems to be in order, but the column `distance` seems to have some extremely high values. The data description says that this column tells the distance from the city center in km, so the max value of 22210.0 is clearly wrong. Let's take a further look at this column. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low = train_df[train_df['distance']<100]\n",
    "df_high = train_df[train_df['distance']>=100]\n",
    "print(f\"df_low ({len(df_low.index)} rows):{df_low['distance'].describe()}\")\n",
    "print(f\"\\ndf_high ({len(df_high.index)} rows):{df_high['distance'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebecb14",
   "metadata": {},
   "source": [
    "It seems there are 134 rows where the distance has been stated in metres instead of kilometres. Complete the `correct_distance_unit` function that changes those values into kilometers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a57b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_distance_unit(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Correct the falsely input values in column 'distance'\n",
    "    Args:\n",
    "        df (DataFrame): A Pandas DataFrame holding all of the housing data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd89b5",
   "metadata": {},
   "source": [
    "Let's run the cell below to see if our function works. The maximum distance should now be 45.68 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_distance_unit(train_df)\n",
    "print(train_df['distance'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2b2bb",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "count    11046.000000\n",
    "mean        11.090669\n",
    "std          4.906347\n",
    "min          0.000000\n",
    "25%          7.990000\n",
    "50%         11.160000\n",
    "75%         13.877500\n",
    "max         45.680000\n",
    "Name: distance, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365a5c7",
   "metadata": {},
   "source": [
    "### 2c) Fixing typos\n",
    "\n",
    "Now we should have our numerical columns in order. Let's next take a look at the string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_unique = train_df['condition'].unique()\n",
    "print(f\"Column 'condition' has {len(condition_unique)} unique values:\\n{condition_unique}\")\n",
    "area_unique = train_df['area'].unique()\n",
    "print(f\"\\nColumn 'area' has {len(area_unique)} unique values:\\n{area_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20938f6d",
   "metadata": {},
   "source": [
    "The column `area` seems to be ok, but the column `condition` contains several typos. \n",
    "\n",
    "Start by lowercasing the conditions and removing any unnecessary trailing white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3219193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_transformer(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Lowercases all values in the column 'condition' and removes trailing white space.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9856d",
   "metadata": {},
   "source": [
    "Let's apply this function to our training data to see what changed. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_transformer(train_df)\n",
    "print(\"The column 'condition' contains following unique values:\\n\", train_df['condition'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906865b2",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "The column 'condition' contains following unique values:\n",
    " ['satisfactory' 'good' 'excellent' 'satisfactoryy' 'goood' 'satisfactry'\n",
    " 'satisfatory' 'satisfattory' 'god']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fd78d",
   "metadata": {},
   "source": [
    "We see that the data contains three conditions, namely: 'excellent', 'good' and 'satisfactory'. Other values are clearly typos. Let's use [TheFuzz](https://github.com/seatgeek/thefuzz) library (already imported above) to automate the process of correcting typos. \n",
    "\n",
    "Implement the following to complete the `typo_fixer` function:\n",
    "1. Loop over the condition column and find the best approximate match from a list of correct conditions for each entry in the condition column. You can use [process.extractOne](https://github.com/seatgeek/thefuzz#process) to find the single best match for an entry and use `fuzz.ratio` as the scorer inside the `extractOne` function.\n",
    "2. Update the column `condition` by replacing each entry with its best approximate match if the similarity score is greater than the specified threshold. Otherwise, keep the entry unchanged. In this way, we fix only those strings that feel confident enough to be typos of the correct conditions. Using a threshold is a good practice so that we are not too aggressive in correcting the typos. \n",
    "3. Create a new column `similarity_scores`, where you store the similarity score of each (original entry)-(approximate match) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_fixer(df: pd.DataFrame, threshold: float, correct_condition_values: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Uses fuzzy string matching to fix typos in the column 'condition'. It loops through each entry in the column and \n",
    "    replaces them with suggested corrections if the similarity score is high enough. \n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data\n",
    "        threshold (int): A number between 0-100. Only the entries with score above this number are replaced.\n",
    "        correct_condition_values (List): A list of correct strings that we hope the condition column to include. For example, correct_ones=['excellent', 'good', 'satisfactory'] in the case of the training dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6528114",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "typo_fixer(train_df, threshold, ['excellent', 'good', 'satisfactory'])\n",
    "print(\"The column 'condition' contains following unique values:\\n\", train_df['condition'].unique())\n",
    "print(f\"The lowest similarity score is {train_df['similarity_scores'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ca52b",
   "metadata": {},
   "source": [
    "You're expected to see all typos are corrected. Expected output:\n",
    "```text\n",
    "The column 'condition' contains following unique values:\n",
    " ['satisfactory' 'good' 'excellent']\n",
    "The lowest similarity score is 86\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e5cdd",
   "metadata": {},
   "source": [
    "Now that we have our data extraction and cleaning operations in place, let's conclude the first two assignments by wrapping everything up to this point inside a single function `data_extraction`. It loads the price and house info datasets, merges them into a single DataFrame and polishes the DataFrame.\n",
    "\n",
    "Run the cell below to define the data extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(path: Path, correct_condition_values: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The entire data extraction/cleaning pipeline wrapped inside a single function.\n",
    "    Args:\n",
    "        path (Path): Path to the folder where the files exist.\n",
    "        correct_condition_values (List): A list of correct strings that we hope the condition column to include.\n",
    "    Returns:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the (cleaned) housing data.\n",
    "    \"\"\"\n",
    "    threshold = 80\n",
    "    prices, house_data = file_reader(path)\n",
    "    df = dataframe_merger(prices, house_data)\n",
    "    drop_futile_columns(df)\n",
    "    correct_distance_unit(df)\n",
    "    string_transformer(df)\n",
    "    typo_fixer(df, threshold, correct_condition_values)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab736373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training DataFrame that goes through the data extraction and cleaning processes. This DataFrame will be used in the following assignments\n",
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "train_df = data_extraction(train_path, ['excellent', 'good', 'satisfactory'])\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9f947",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"./images/extracted_train_df.png\" width=1300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0ad46",
   "metadata": {},
   "source": [
    "## Assignment 3: Data validation with Great Expectations (2 points)\n",
    "\n",
    "In this assignment, we practice the use of Great Expectations as a tool for data validation. We provide you with some code to help you instantiate and get started with Great Expectations. Running the code cell below creates a folder `gx` in your working directory with all things related to working with Great Expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_datasource_name = \"housing_datasource\"\n",
    "housing_expectation_suite_name = \"housing_expectation_suite\"\n",
    "\n",
    "# Instantiate a Data Context and save it in filesystem.\n",
    "context = gx.get_context(project_root_dir=str(WORKING_DIR))\n",
    "\n",
    "# Connect to data in our housing DataFrame.\n",
    "try:\n",
    "    # Create a new Data Source in the Data Context\n",
    "    datasource = context.sources.add_pandas(name=housing_datasource_name)\n",
    "except DataContextError:\n",
    "    # The Data Source already exists in the Data Context\n",
    "    datasource = context.get_datasource(housing_datasource_name)\n",
    "\n",
    "try:\n",
    "    # Create a new DataFrame Data Asset\n",
    "    training_data_asset = datasource.add_dataframe_asset(name=\"training_data\")\n",
    "except ValueError:\n",
    "    # The Data Asset already exists\n",
    "    training_data_asset = datasource.get_asset(\"training_data\")\n",
    "\n",
    "# Request all data in the DataFrame as a single batch\n",
    "training_batch_request = training_data_asset.build_batch_request(dataframe=train_df)\n",
    "\n",
    "# Create an Expectation Suite \n",
    "context.add_or_update_expectation_suite(housing_expectation_suite_name)\n",
    "\n",
    "# Create a Validator\n",
    "housing_validator = context.get_validator(\n",
    "    batch_request=training_batch_request,\n",
    "    expectation_suite_name=housing_expectation_suite_name\n",
    ")\n",
    "display(housing_validator.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a447615",
   "metadata": {},
   "source": [
    "### 3a) Create Expectations\n",
    "Your task is to create the following Expectations using the `expect_*` methods provided by the Validator (`housing_validator`). Additionally, recall that the `expect_*` methods will also validate the data loaded into the Validator, so please also store the validation results in a list.\n",
    "1. DataFrame `df` should contain all the columns in the same order as `train_df` (the training DataFrame produced by the `data_extraction` function).\n",
    "1. Column `price` can not contain NaN values.\n",
    "1. Column `yr_built` shouldn't have values smaller than 1917.\n",
    "1. Column `bedrooms` should be an integer.\n",
    "1. Column `condition` should contain only values from the set {'satisfactory', 'good', 'excellent'}.\n",
    "\n",
    "Hints: The following validation functions may be helpful. You can find more information of their usage [here](https://greatexpectations.io/expectations/). \n",
    "- expect_table_columns_to_match_ordered_list\n",
    "- expect_column_values_to_not_be_null\n",
    "- expect_column_values_to_be_between\n",
    "- expect_column_values_to_be_of_type\n",
    "- expect_column_values_to_be_in_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "### START CODE HERE\n",
    "# e.g., test_results.append(...)\n",
    "### END CODE HERE\n",
    "\n",
    "# Print the validation results of the batch loaded into the Validator\n",
    "for i, result in enumerate(test_results):\n",
    "    if result['success'] is True:      \n",
    "        print(f\"Test {i} succeeded ({result['expectation_config']['expectation_type']})\")\n",
    "    else:\n",
    "        print(f\"Test {i} failed ({result['expectation_config']['expectation_type']})\")\n",
    "        for item in result['result'].items():\n",
    "            print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f71a2",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "Test 0 succeeded (expect_table_columns_to_match_ordered_list)\n",
    "Test 1 succeeded (expect_column_values_to_not_be_null)\n",
    "Test 2 failed (expect_column_values_to_be_between)\n",
    "\t ('element_count', 11046)\n",
    "\t ('unexpected_count', 437)\n",
    "\t ('unexpected_percent', 3.9561832337497735)\n",
    "\t ('partial_unexpected_list', [1900, 1910, 1913, 1910, 1906, 1915, 1906, 1910, 1906, 1912, 1905, 1909, 1909, 1906, 1909, 1910, 1903, 1906, 1910, 1916])\n",
    "\t ('missing_count', 0)\n",
    "\t ('missing_percent', 0.0)\n",
    "\t ('unexpected_percent_total', 3.9561832337497735)\n",
    "\t ('unexpected_percent_nonmissing', 3.9561832337497735)\n",
    "Test 3 succeeded (expect_column_values_to_be_of_type)\n",
    "Test 4 succeeded (expect_column_values_to_be_in_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c8c89",
   "metadata": {},
   "source": [
    "We see that the Expectation regarding the building year is not met. There are $\\frac{437}{11046}=3.96\\%$ of entries not conforming to our Expectation. There are (at least) three ways in which we can deal with this: \n",
    "1. We can manually check the min value of the column `yr_built` (in fact, we already have the info in Assignment 2b) and use that as the lower bound. \n",
    "1. We can use the keyword argument `mostly` to specify a fraction of the data that must conform with our expectations. This allows for some values to be outside of the defined interval. \n",
    "1. We can try to define the boundaries automatically (see below). \n",
    "\n",
    "Which ever approach we take, we can just redefine the Expectation and that will overwrite the old one.\n",
    "\n",
    "Some (but not all) Expectations allow for automatic inferring of boundaries for values using the batch loaded into the Validator. This is called auto-initializing. You can check if an Expectation can be auto-initialized as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_1 = Expectation.is_expectation_auto_initializing(name=\"expect_table_columns_to_match_ordered_list\")\n",
    "boolean_2 = Expectation.is_expectation_auto_initializing(name=\"expect_column_values_to_be_between\")\n",
    "print(f\"\\nboolean_1 = {boolean_1} and boolean_2 = {boolean_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406124c7",
   "metadata": {},
   "source": [
    "Using auto-initializing is convenient especially when working with a large number of covariates. However, it requires that we have access to some data, which has already been validated (luckily, we do). Also, auto-initialization might produce some rules that are too strict or too loose, so always be careful when using them and check the results.\n",
    "\n",
    "Update our `housing_validator` with the following Expectations: \n",
    "- The values of columns `yr_built`, `bedrooms`, and `date` should be between a minimum value and a maximum value. These values are automatically inferred by Great Expectations.\n",
    "- The values of columns `area` should be in a set. The set is also automatically inferred by Great Expectations.\n",
    "\n",
    "Similar to what you did previously, also append the resulted ExpectationValidationResults into a list. \n",
    "\n",
    "Hint: Check [here](https://docs.greatexpectations.io/docs/guides/expectations/how_to_use_auto_initializing_expectations/) on how to enable auto-initialization of an Expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "# Print the Expectation configuration that Great Expectations automatically inferred\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"Test {i} ({result['expectation_config']['expectation_type']}) resulted in config:\")\n",
    "    for item in result['expectation_config']['kwargs'].items():\n",
    "        print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de3885",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "Test 0 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('column', 'yr_built')\n",
    "\t ('min_value', 1900)\n",
    "\t ('max_value', 2015)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('strict_max', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 1 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('max_value', 5)\n",
    "\t ('column', 'bedrooms')\n",
    "\t ('min_value', 0)\n",
    "\t ('strict_max', False)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 2 (expect_column_values_to_be_between) resulted in config:\n",
    "\t ('max_value', '2015-12-24T00:00:00')\n",
    "\t ('column', 'date')\n",
    "\t ('min_value', '2007-07-02T00:00:00')\n",
    "\t ('strict_max', False)\n",
    "\t ('mostly', 1.0)\n",
    "\t ('strict_min', False)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "Test 3 (expect_column_values_to_be_in_set) resulted in config:\n",
    "\t ('column', 'area')\n",
    "\t ('value_set', ['Jamison Centre', 'Gundaroo', 'Wanniassa', 'Kaleen', ...])\n",
    "\t ('mostly', 1.0)\n",
    "\t ('auto', True)\n",
    "\t ('batch_id', 'housing_datasource-training_data')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0296a2",
   "metadata": {},
   "source": [
    "We could specify more Expectations, but the Expectations we've defined suffice for now. Recall that the Expectation Suite we have configured this far exists only in memory and has to be persisted for future use. Let's save the Expectation Suite into our Data Context.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the Expectation Suite\n",
    "housing_validator.save_expectation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a3ee8",
   "metadata": {},
   "source": [
    "### 3b) Validate new data against the defined Expectations\n",
    "\n",
    "Let's run our newly defined Expectation Suite against some test data. This is done by creating and configuring a [Checkpoint](https://docs.greatexpectations.io/docs/terms/checkpoint). Before we do that, we'll have to create a Batch Request for the new data. We have some unprocessed test data residing in a test data folder. \n",
    "\n",
    "Create the following functions:\n",
    "1. `batch_creator`, which \n",
    "    1. creates a new DataFrame DataAsset named \"test_data\" to a Data Source or gets the asset from the Data Source if it's already existing, then \n",
    "    1. creates a new Batch Request for the given DataFrame and returns the new Batch Request.\n",
    "1. `create_checkpoint`, which \n",
    "    1. creates a new Checkpoint with the following configurations:\n",
    "        - The name of the Checkpoint should be the given `checkpoint_name`,\n",
    "        - It should validate the data of the given `batch_request` using an Expectation Suite whose name is the given `expectation_suite_name`\n",
    "    1. uses the created Checkpoint to run a validation, the validation running name should be the given `run_name`,\n",
    "    1. finally, returns the validation results. \n",
    "\n",
    "Hints: \n",
    "- The following functions may be useful for implementing the `batch_creator` function: \n",
    "    - [add_dataframe_asset and build_batch_request](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/in_memory/connect_in_memory_data/#add-a-data-asset-to-the-data-source)\n",
    "    - [get_asset](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/batch_requests/how_to_request_data_from_a_data_asset#retrieve-your-data-asset)\n",
    "\n",
    "- The following functions may be useful for implementing the `create_checkpoint` function: \n",
    "    - [add_or_update_checkpoint](https://docs.greatexpectations.io/docs/reference/api/data_context/AbstractDataContext_class#great_expectations.data_context.AbstractDataContext.add_or_update_checkpoint)\n",
    "    - [(Checkpoint).run](https://docs.greatexpectations.io/docs/reference/api/checkpoint/Checkpoint_class#great_expectations.checkpoint.Checkpoint.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(df: pd.DataFrame, context: FileDataContext, data_source_name: str) -> BatchRequest:\n",
    "    \"\"\"\n",
    "    Creates a new Batch Request using the given DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the cleaned housing data. \n",
    "        context (GX FileDataContext): The current active GX Data Context \n",
    "        data_source_name (str): Name of the GX Data Source, to which the DataFrame is added\n",
    "    Returns:\n",
    "        new_batch_request (GX BatchRequest): The GX batch request created using df\n",
    "    \"\"\"\n",
    "    datasource = context.get_datasource(data_source_name)\n",
    "    test_asset_name = \"test_data\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "def create_checkpoint(context: FileDataContext, batch_request: BatchRequest, checkpoint_name: str, expectation_suite_name: str, run_name: str) -> CheckpointResult:\n",
    "    \"\"\"\n",
    "    Creates a new GX Checkpoint from the argument batch_request\n",
    "    Args:\n",
    "        context (GX FileDataContext): The current active context \n",
    "        new_batch_request (GX BatchRequest): A GX batch request used to create the Checkpoint\n",
    "        checkpoint_name (str): Name of the Checkpoint\n",
    "        expectation_suite_name (str): Name of the Expectation Suite used to validate the data\n",
    "        run_name (str): Name of the validation running\n",
    "    Returns:\n",
    "        checkpoint_result (GX CheckpointResult): \n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412ee29",
   "metadata": {},
   "source": [
    "Run the cell below to call both functions. This should create the Checkpoint and returns a Checkpoint result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data location\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "# Create the batch request and checkpoint\n",
    "context = gx.get_context()\n",
    "test_df = data_extraction(test_path, ['satisfactory', 'good', 'excellent'])\n",
    "test_batch_request = batch_creator(\n",
    "    df=test_df,\n",
    "    context=context,\n",
    "    data_source_name=housing_datasource_name)\n",
    "\n",
    "test_data_checkpoint_result = create_checkpoint(context=context,\n",
    "                                                batch_request=test_batch_request,\n",
    "                                                checkpoint_name=\"test_data_checkpoint\",\n",
    "                                                expectation_suite_name=housing_expectation_suite_name,\n",
    "                                                run_name=\"test_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea17531",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.view_validation_result(test_data_checkpoint_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910ef15",
   "metadata": {},
   "source": [
    "If everything went smoothly, a page showing the result should be opened in your browser. We see that one out of eight expectations has failed. By scrolling down, we see that the failed expectation is the one which specifies that the values of column `cond_typos_fixed` has to be one of {'satisfactory', 'good', 'excellent'}. The test data seems to have new values 'tolerable' and 'poor'. You can find the result in `gx/uncommitted/data_docs/local_site/index.html`\n",
    "\n",
    "NOTE: _If we hadn't specified a threshold when we did fuzzy string matching, the algorithm would have changed the values 'tolerable' and 'poor' to their nearest matches from {'satisfactory', 'good', 'excellent'}, which would have been very undesirable._\n",
    "\n",
    "Do the following:\n",
    "1. Update the Expectation for column `condition` to include values 'tolerable' and 'poor' as well.\n",
    "1. Save the updated Expectation Suite into our Data Context.\n",
    "1. Use the `create_checkpoint` function you just made to create a new Checkpoint with arguments (`test_batch_request`, `\"second_test_checkpoint\"`, `\"housing_expectation_suite\"`, `\"second_test_run\"`). Store the results into a variable named `second_test_checkpoint_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb417abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE\n",
    "\n",
    "second_test_data_checkpoint_result = None\n",
    "### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.view_validation_result(second_test_data_checkpoint_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f110da",
   "metadata": {},
   "source": [
    "Now we should see that all Expectations have been met.\n",
    "\n",
    "NOTE: _Creating Checkpoints and validating data against a set of Expectations doesn't do anything in itself (other than create the Data Docs). We can of course manually check the Data Docs after validating each new batch of data, but it would be nice if could automate at least some part of this process. This is what the [Great Expectations Actions](https://docs.greatexpectations.io/docs/guides/validation/validation_actions/actions_lp) are for. However, we won't utilize this functionality in this exercise._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c992956",
   "metadata": {},
   "source": [
    "### Screenshots to be submitted for Assignment 3\n",
    "1. A screenshot of the failed Checkpoint result (i.e., the Checkpoint run named \"test-run\").\n",
    "1. A screenshot of the succeeded Checkpoint result (i.e., the Checkpoint run named \"second-test-run\"). \n",
    "\n",
    "**Note**: It's enough to show the overview and the failed/corrected Validation. \n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <figure>\n",
    "        <img src=\"./images/test-run-overview.png\" width=600/>\n",
    "        <img src=\"./images/test-run-condition.png\" width=800/>\n",
    "        <figcaption>test-run</figcaption>\n",
    "    </figure>\n",
    "    <br />\n",
    "    <figure>\n",
    "        <img src=\"./images/second-test-run-overview.png\" width=700/>\n",
    "        <img src=\"./images/second-test-run-condition.png\" width=800/>\n",
    "        <figcaption>second-test-run</figcaption>\n",
    "    </figure>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a6efe",
   "metadata": {},
   "source": [
    "## Assignment 4: Feature engineering (2 points)\n",
    "\n",
    "In this part we will touch on the subject of feature engineering. Since this subject is dealt with in more detail in the course '[Introduction to Machine Learning](https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01)', we try to keep it light whilst also (perhaps) covering some corners not talked about on that course. Some of the methods presented in this exercise require us to divide our data into features and targets, so let's begin by building a function to achieve that (the function is ready so you don't need to implement anything). Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_X_and_y(df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Separates the features and targets.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the cleaned housing data\n",
    "        target (str): Name of the target column\n",
    "    Returns:\n",
    "        X (DataFrame): A Pandas DataFrame holding the cleaned housing data without the target column\n",
    "        y (Series): A pandas Series with the target values\n",
    "    \"\"\"\n",
    "    df = df.copy()  # By copying the DataFrame we protect the original DataFrame in case we make errors in the following assignments. This way, we can always restart from this step.\n",
    "    y = df[target].astype('float64')\n",
    "    X = df.loc[:, df.columns != target]    \n",
    "    return (X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5382b0",
   "metadata": {},
   "source": [
    "Then, we'll use this function to separate the target column from the rest for both the training and testing datasets. Let's create the entire datasets from the beginning. Again, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "train_df = data_extraction(train_path, ['satisfactory', 'good', 'excellent'])\n",
    "test_df = data_extraction(test_path, ['poor', 'tolerable', 'satisfactory', 'good', 'excellent'])\n",
    "\n",
    "X_train, y_train = separate_X_and_y(train_df, target='price')\n",
    "X_test, y_test = separate_X_and_y(test_df, target='price')\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcc50a",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "X_train shape: (11046, 18)\n",
    "y_train shape: (11046,)\n",
    "X_test shape: (2762, 18)\n",
    "y_test shape: (2762,)\n",
    "```\n",
    "<img src=\"./images/data-split-output.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff82507",
   "metadata": {},
   "source": [
    "### 4a) Imputing missing values\n",
    "\n",
    "As we saw in Assignment 2a), the columns `sqft_living15` and `sqft_lot15` have roughly 1.5% and 10% of values missing respectively in the training data so we could try to impute something in place of the missing values. We could use a simple imputation scheme, such as imputing either zero, mean, median or mode to both columns in place of missing values. However, we could also use a more sophisticated scheme, such as [MICE](https://www.machinelearningplus.com/machine-learning/mice-imputation/), where we use a regressor model to infer the missing values by comparing other features iteratively. We will use the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) in this exercise. \n",
    "\n",
    "Your task is to complete the `impute_missing` function that uses `IterativeImputer` to impute missing values of Dataframes `X_train` and `X_test`. We use parameter `add_indicator=True` to include two new columns that indicate which values were imputed. These indicator variables will have a value of 1 where the original data was missing and 0 where it was not. This information might be valuable in model training (and inference). \n",
    "\n",
    "You can use the `fit_transform` method to fit the imputer on a DataFrame X and return the transformed X. Note that the fit_transform method will return a Numpy array instead of a DataFrame, and the shape of the returned Numpy array will not match the shape of the DataFrame given as an argument, so you'll have to figure a way around this.\n",
    "\n",
    "Hint: You can use the method [get_feature_names_out](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer.get_feature_names_out) to list the column names of the `IterativeImputer` object once it has been fitted.\n",
    "\n",
    "NOTE: _The computational complexity increases very rapidly with regards to the number of features in the dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Imputes missing numerical values using MICE.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "    \"\"\"\n",
    "    imp = IterativeImputer(random_state=RANDOM_SEED, add_indicator=True) \n",
    "\n",
    "    # A list of column names where missing values need to be imputed.\n",
    "    # We'll not include categorical or datetime variables in the calculations\n",
    "    included_columns = [True if x not in ['postcode', 'area', 'date', 'condition'] else False for x in df.columns]\n",
    "\n",
    "    ### START CODE HERE\n",
    " \n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974bbfa",
   "metadata": {},
   "source": [
    "Let's again test the function we just created. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_missing(X_train)\n",
    "impute_missing(X_test)\n",
    "nan_train = X_train.isnull().sum().sum()\n",
    "nan_test = X_test.isnull().sum().sum()\n",
    "print(f\"The numbers of missing values are {nan_train} for the train set and {nan_test} for the test set. Both should be zero.\")\n",
    "print(f\"There should be {X_train.shape[1]} columns: {X_train.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1414e",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```text\n",
    "The numbers of missing values are 0 for the train set and 0 for the test set. Both should be zero.\n",
    "There should be 20 columns: Index(['yr_built', 'bedrooms', 'postcode', 'area', 'date', 'bathrooms',\n",
    "       'condition', 'grade', 'floors', 'sqft_living', 'sqft_lot',\n",
    "       'sqft_basement', 'sqft_living15', 'sqft_lot15', 'waterfront', 'view',\n",
    "       'distance', 'similarity_scores', 'missingindicator_sqft_living15',\n",
    "       'missingindicator_sqft_lot15'],\n",
    "      dtype='object')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02d325",
   "metadata": {},
   "source": [
    "### 4b) Decomposing datetime and categorical variables\n",
    "\n",
    "The column `date` is actually hoarding a bunch of potential features. Since we converted this column to `DateTime` format, we can use the attributes of this format to decompose the column into several features, which might benefit model training. \n",
    "Create a function, which takes a DataFrame and a column name for a DateTime column as arguments and\n",
    "1. uses `pandas.Series.dt.*` attributes to decompose the DateTime column into three new columns: `year`, `quarter`, and `weekday`. The column `quarter` should have values 1-4 and the column `weekday` should have values 0-6, where 0 is Monday, 1 is Tuesday, and so on. (You might take a look at [this blog](https://datagy.io/pandas-datetime/)).\n",
    "1. removes the original DateTime column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06129ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_decomposer(df: pd.DataFrame, dt_column_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Decomposes datetime values into year, quarter, and weekday.\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "        dt_column_name(str): The name of the datetime column\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a87a74",
   "metadata": {},
   "source": [
    "Apply this function for both `X_train` and `X_test` by running the cell below. There should be 22 columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e55cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_decomposer(X_train, 'date')\n",
    "datetime_decomposer(X_test, 'date')\n",
    "\n",
    "print(f\"There are a total of {len(X_train.columns)} columns remaining:\")\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "There are a total of 22 columns remaining:\n",
    "Index(['yr_built', 'bedrooms', 'postcode', 'area', 'bathrooms', 'condition',\n",
    "       'grade', 'floors', 'sqft_living', 'sqft_lot', 'sqft_basement',\n",
    "       'sqft_living15', 'sqft_lot15', 'waterfront', 'view', 'distance',\n",
    "       'similarity_scores', 'missingindicator_sqft_living15',\n",
    "       'missingindicator_sqft_lot15', 'year', 'quarter', 'weekday'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b716033",
   "metadata": {},
   "source": [
    "Some ML algorithms can be used with categorical variables, but others require the features to be in numerical format. It might make sense to convert them anyway, if for example, we plan to compare different algorithms against each other. One simple option is to just assign a random integer for each value. This is called label encoding or ordinal encoding. However, this approach induces an arbitrary ordering for the values, which might confuse some algorithms (although 1 < 2, it doesn't make sense to state that 'cat' < 'dog'). If a variable is ordinal (like the variable `condition` in our case) and we __control the order__, this might be OK. \n",
    "\n",
    "Complete the `condition_encoder` function that uses label encoding to convert the values in column `condition` in a given DataFrame to numerical ones. Use scaling poor=1, tolerable=2, satisfactory=3, good=4, and excellent=5. \n",
    "\n",
    "Hint: The [pandas.Series.map](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.map.html#pandas.Series.map) function may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_encoder(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Encodes conditions to numerical range 1-5\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dcd27",
   "metadata": {},
   "source": [
    "Run the cell below to apply the encoder for the column `condition` in both `X_train` and `X_test`. You should see the following statistics:\n",
    "\n",
    "| | X_train | X_test |\n",
    "| :-: | :-: | :-: |\n",
    "| mean | 3.362665 | 3.296886 |\n",
    "| std | 0.526798 | 0.622107 |\n",
    "| min | 3 | 1 |\n",
    "| max | 5 | 5| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b37c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_encoder(X_train)\n",
    "condition_encoder(X_test)\n",
    "print(\"Statistics for X_train:\\n\", X_train['condition'].describe())\n",
    "print(\"\\nStatistics for X_test:\\n\", X_test['condition'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb380a1",
   "metadata": {},
   "source": [
    "In many cases, using label encoding is not advisable, because it implies that the values are ordered. For instance, take a look at the variable `postcode`, which although in numerical format, is actually a categorical variable. If we used them as is, a linear regressor model would have to assume that either a bigger postcode implies a bigger price or a smaller price. If (and when) this is not the case, the (assumably) valuable information in this column would be rendered pretty much useless.\n",
    "\n",
    "Another option for converting categorical variables into numeric format is to use one-hot-encoding, where a binary feature is created for each unique value of the variable. This often leads to sparse feature matrices, when the cardinality of the categorical variable is high. For example, the `area` column has 131 unique values, so using one-hot-encoding would require us to present this variable using 130 additional features.  \n",
    "\n",
    "One popular option is to use [target encoding](https://scikit-learn.org/stable/modules/preprocessing.html#target-encoder). The simplest way to use target encoding in a regression setting is to calculate the mean of target values and use that to encode categorical features. This might make sense, since for example, it's highly likely that the price of a house correlates with the area on which it is built. To reduce overfitting, the calculated means of categories can be (Laplace) smoothed with the overall mean of the target. Thus, the encoding $e_c$ for category $c$ becomes\n",
    "$$\n",
    "e_c = \\lambda_c \\cdot \\bar{y}_c + (1-\\lambda_c) \\cdot \\bar{y}, \n",
    "$$\n",
    "where $\\bar{y}_c$ is the category mean and $\\bar{y}$ is the overall mean of the target and $\\lambda \\in [0,1]$ is a parameter controlling the smoothing amount. \n",
    "\n",
    "Complete the `target_encode` function which applies target encoding for a set of columns in a given DataFrame using the scikit-learn implementation. It should behave as follows: \n",
    "- If the target column is specified, the methods `fit` and `transform` should both be used (separately, don't use `fit_transform` to disable cross-fitting). \n",
    "- If the target column is not specified (for test data), only the method `transform` should be used with a pre-trained encoder passed as an argument.\n",
    "- It replaces the values in the original column with the encoded ones.\n",
    "\n",
    "NOTE: _The implementation of \"fit_transform\" in scikit-learn uses cross-fitting internally to avoid over-fitting. One thing needs to be considered at the point: target encoding may also need to be applied to production data (i.e., the real-world data the model used to generate predictions). However, targets are not available during inference, the encodings derived using cross-fitting can not be used. We skip cross-fitting entirely for simplicity in this exercise and rely solely on smoothing._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(df: pd.DataFrame, columns: List[str], target: Optional[pd.Series]=None, encoder: Optional[TargetEncoder]=None) -> TargetEncoder:\n",
    "    \"\"\"\n",
    "    Encodes postcode and area to numerical format using a target encoder\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding the features\n",
    "        columns (list of strings): Names of the categorical columns to be encoded\n",
    "        target (Series|None): A pandas Series with the target values. This is required only when fitting the encoder.\n",
    "        encoder(TargetEncoder|None): An already fitted encoder. This is required when we want to apply an encoder, which has already been fitted during training.\n",
    "    Returns:\n",
    "        encoder(TargetEncoder): The fitted TargetEncoder. Either a new fitted one or the one passed as an argument.\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = TargetEncoder(target_type='continuous', smooth='auto', random_state=RANDOM_SEED)\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    df.loc[:, columns].astype('float64')\n",
    "    return encoder\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823c4fc",
   "metadata": {},
   "source": [
    "Use the function to target-encode columns `postcode` and `area` in both `X_train` and `X_test` by running the cell below. You should see the following statistics:\n",
    "\n",
    "| | X_train postcode | X_test postcode | X_train area | X_test area |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| mean | 5.919559e+05 | 5.991515e+05 | 5.897071e+05 | 5.963635e+05 |\n",
    "| std | 1.519866e+05 | 1.602493e+05 | 1.458486e+05 | 1.534161e+05 |\n",
    "| min | 1.506176e+05 | 1.506176e+05 | 1.378392e+05 | 1.619446e+05 |\n",
    "| max | 1.275222e+06 | 1.275222e+06| 1.298436e+06 | 1.298436e+05 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aad376",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder = target_encode(X_train, ['postcode', 'area'], target=y_train)\n",
    "_ = target_encode(X_test, ['postcode', 'area'], encoder=t_encoder)\n",
    "\n",
    "print(\"Statistics for X_train postcode:\\n\", X_train['postcode'].describe())\n",
    "print(\"\\nStatistics for X_test postcode:\\n\", X_test['postcode'].describe())\n",
    "print(\"Statistics for X_train area:\\n\", X_train['area'].describe())\n",
    "print(\"\\nStatistics for X_test area:\\n\", X_test['area'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b739de",
   "metadata": {},
   "source": [
    "We have now constructed a somewhat feasible feature extraction pipeline and derived a set of features. In real life, this process would be much more iterative (and time consuming) with a lot of experiments on different ways on how to build the features. We bypassed a lot of topics, such as scaling and normalization of numerical data, which are important parts for the optimal use of many ML algorithms. As already mentioned, this topic is dealt with more thoroughly on the [I2ML]((https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01)) course.\n",
    "\n",
    "The Internet is also full of (opinionated) blog posts on feature engineering. These posts make a good source for new ideas on how to do things, but remember to always reserve a hint of scepticism and try to think things through yourself. As a final note, the scikit-learn library offers nice tools for building complex feature engineering pipelines, which we omitted in these exercises due to lack of time. Feel free to [check it out](https://scikit-learn.org/stable/modules/compose.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637f6c",
   "metadata": {},
   "source": [
    "### 4c) Persisting derived features and encoders\n",
    "\n",
    "To conclude this exercise, let's store the derived features and the fitted target encoder for future use. In a more demanding setup, we would use a feature store (like [Feast](https://feast.dev/)) for this, but in this exercise, we use just a dedicated folder in the local file system, namely the folder `feature_store`. \n",
    "\n",
    "Create the following functions:\n",
    "1. `store_features`, which takes a set of features and stores them in the `feature_store` folder with a user-specified name in `.parquet` format.\n",
    "1. `store_targets`, which takes a set of targets and stores them in the `feature_store` folder with a user-specified name in `.csv` format. To avoid potential issues in the remaining assignments, don't include index when saving targets into a CSV file. \n",
    "1. `store_encoder`, which takes a fitted `TargetEncoder` and stores it in the subfolder `encoders` inside the `feature_store` folder with a user specified name in `.pkl` (pickle) format.\n",
    "\n",
    "Hint: Pickle files are written in binary format. Use mode 'wb' when writing the pickle file.\n",
    "\n",
    "NOTE: _Pickle is the default format for persisting fitted models in scikit-learn. However, it is not secure. [Only unpickle data you trust](https://docs.python.org/3/library/pickle.html)._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_features(X: pd.DataFrame, feature_file_path: str) -> None:    \n",
    "    \"\"\"\n",
    "    Stores a set of features to a specified location\n",
    "    Args:\n",
    "        X (DataFrame): A pandas DataFrame holding the features\n",
    "        feature_file_path (Path): Path for the stored features, e.g., feature_store/housing_train_X.parquet\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "   \n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "def store_targets(y: pd.Series, target_file_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Stores a set of features to a specified location\n",
    "    Args:\n",
    "        y (Series): A pandas Series holding the target values\n",
    "        target_file_path (Path): Path for the stored targets, e.g., feature_store/housing_train_y.csv\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    " \n",
    "    ### END CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "def store_encoder(encoder: TargetEncoder, encoder_file_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Stores a targetEncoder to a specified location\n",
    "    Args:\n",
    "        encoder (TargetEncoder): A fitted scikit-learn TargetEncoder object\n",
    "        encoder_file_path (Path): Path of the stored target encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    " \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e5ca6",
   "metadata": {},
   "source": [
    "Let's again test the above functions. Run the cell below to see if a set of new files appears in your `feature_store` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_path = WORKING_DIR / \"feature_store\"\n",
    "housing_train_x_path = feature_store_path / \"housing_train_X.parquet\"\n",
    "housing_test_x_path = feature_store_path / \"housing_test_X.parquet\"\n",
    "housing_train_y_path = feature_store_path / \"housing_train_y.csv\"\n",
    "housing_test_y_path =  feature_store_path / \"housing_test_y.csv\"\n",
    "housing_target_encoder_path = feature_store_path / \"encoders\" / \"housing_target_encoder.pkl\"\n",
    "\n",
    "store_features(X_train, housing_train_x_path)\n",
    "store_features(X_test, housing_test_x_path)\n",
    "store_targets(y_train, housing_train_y_path)\n",
    "store_targets(y_test, housing_test_y_path)\n",
    "store_encoder(t_encoder, housing_target_encoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5bf8a9",
   "metadata": {},
   "source": [
    "## Assignment 5: Feature importance and feature selection (2 points)\n",
    "Feature importance indicates how useful a feature is for a model to predict the target. Feature selection refers to the process of reducing the number of input variables in model training to save computational cost and even boost model performance. These topics are dealt in more detail in the [I2ML]((https://studies.helsinki.fi/courses/course-unit/hy-CU-118207827-2021-08-01)) course, so we'll just gently touch on them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34106256",
   "metadata": {},
   "source": [
    "### 5a) Basic trial with random forest regressor\n",
    "\n",
    "Let's start by doing a simple comparison to see if the features we have prepared through the feature engineering steps are any good. In this exercise, we'll use [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) from the scikit-learn library to train two models, each use different data. One model is trained with the \"fully engineered\" feature set we derived through the feature engineering steps in Assignment 4 and the other is trained with a \"minimally engineered\" dataset. We use [coefficient of determination $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) as our metric to evaluate the models (higher is better) and perform no hyperparameter optimization (which will be addressed in next week's exercises).\n",
    "\n",
    "Do the following:\n",
    "1. Prepare the minimally engineered data. \n",
    "    1. Split the training and testing DataFrames `train_df` and `test_df` provided below into features and targets. Specifically, `X_train_basic` contains the un-engineered features for the training dataset, `y_train_basic` contains the target values for the training dataset, `X_test_basic` contains the un-engineered features for the testing dataset, and `y_test_basic` contains the target values for the testing dataset. You can use the function `separate_X_and_y` implemented in Assignment 4.\n",
    "    1. Remove all non-numerical columns (including the column `date`) and all columns with missing values from both `X_train_basic` and `X_test_basic`, since these might cause problems for the regressor models.\n",
    "1. Read the fully engineered feature set and the target set for both training and test data from your `feature_store` directory. Similarly, `X_train` contains the ( fully engineered) features for the training dataset, `y_train` contains the corresponding target values for the training dataset, `X_test` contains the (fully engineered) features for the testing dataset, and `y_test` contains the corresponding target values for the testing dataset.\n",
    "1. Train a RandomForestRegressor model `housing_rfr_basic` with `X_train_basic` and `y_train_basic` and evaluate the performance with `X_test_basic` and `y_test_basic` using $R^2$ as score.\n",
    "1. Train another model `housing_rfr` using data `X_train` and `y_train` from Exercise 4. Evaluate the models performance with `X_test` and `y_test` using $R^2$ as score.\n",
    "\n",
    "(The expected values are $R^2_{\\text{basic}} = 0.89742$ and $R^2_{\\text{engineered}} = 0.93228$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83301708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_path = WORKING_DIR / \"data\" / \"reference\" / \"train\"\n",
    "test_path = WORKING_DIR / \"data\" / \"reference\" / \"test\"\n",
    "\n",
    "# DataFrames of training and test data (not yet feature-engineered)\n",
    "train_df = data_extraction(train_path, ['excellent', 'good', 'satisfactory'])\n",
    "test_df = data_extraction(test_path, ['poor', 'tolerable', 'satisfactory', 'good', 'excellent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regressor that will be trained using the minimally engineered data\n",
    "housing_rfr_basic = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Random forest regressor that will be trained using the feature-engineered data\n",
    "housing_rfr = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Names of columns that should not be included when using data without feature engineering to train a model\n",
    "# These columns are categorical or include missing values\n",
    "removed_columns = ['postcode', 'area', 'date', 'condition', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "# R2 score of the model trained using minimally engineered data\n",
    "r2_score_basic = 0\n",
    "\n",
    "# R2 score of the model trained using the feature-engineered data\n",
    "r2_score = 0\n",
    "\n",
    "### START CODE HERE\n",
    "X_train_basic, y_train_basic = None\n",
    "X_test_basic, y_test_basic = None\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "print(\"The R^2 score for the test set using only basic (minimally-engineered) features is\", r2_score_basic)    \n",
    "print(\"The R^2 score for the test set using the engineered features is\", r2_score)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02241955",
   "metadata": {},
   "source": [
    "### 5b) The most important features\n",
    "The random forest regressor implementation in sklearn allows access to the attribute [feature_importances_](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity), which scores the features used in training using the (normalized) total reduction of error brought by that feature (higher values imply more important feature).\n",
    "\n",
    "Your task is to draw a plot to show the importance of features used to train the `housing_rfr` model, the plot should be similar to the one in the [sklearn documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42987826",
   "metadata": {},
   "source": [
    "You should see the feature `sqft_living` is significantly more important than other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed1697",
   "metadata": {},
   "source": [
    "### 5c) Select only the most important features\n",
    "In this exercise, you'll gain an intuition of how feature selection can affect model performance. \n",
    "\n",
    "Do the following:\n",
    "1. Create a loop where a new RandomForestRegressor model is fitted on each iteration beginning with just the most important feature from `X_train` (the fully engineered feature set). Then, increase the number of important features by one in each iteration so the model is trained with all the features at the end. In other words, you need to train a model using the $i+1$ most important features of `X_train` in $i^{th}$ iteration (suppose i starts with zero). You also need to evaluate the model performance using $R^2$ score on each iteration.\n",
    "1. Record the number of features and the $R^2$ score in each iteration. \n",
    "1. Plot the $R^2$ scores from each iteration as a function of the number of features.\n",
    "\n",
    "Hint: You might find [numpy.argsort](https://numpy.org/doc/1.23/reference/generated/numpy.argsort.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00209929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of important features that yield the best R2 score\n",
    "optimal_number_of_features = 0\n",
    "\n",
    "# The best r2 score\n",
    "best_r2_score = 0\n",
    "\n",
    "# The list of features that produce the best r2 score\n",
    "best_features = []\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "print(f\"Using the {optimal_number_of_features} most important features resulted in the best R^2 score {best_r2_score}.\")\n",
    "unimportant_features = [x for x in X_train.columns if x not in best_features]\n",
    "print(f\"These important features are: {best_features}, while the unimportant features are {unimportant_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44acf79",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "Using the 16 most important features resulted in the best R^2 score 0.9344191835567803.\n",
    "These important features are: Index(['sqft_living', 'postcode', 'area', 'waterfront', 'sqft_basement',\n",
    "       'sqft_living15', 'yr_built', 'view', 'condition', 'bathrooms',\n",
    "       'sqft_lot15', 'sqft_lot', 'distance', 'grade', 'year', 'bedrooms'],\n",
    "      dtype='object'), while the unimportant features are ['floors', 'similarity_scores', 'missingindicator_sqft_living15', 'missingindicator_sqft_lot15', 'quarter', 'weekday']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd649eb",
   "metadata": {},
   "source": [
    "We now know which features can result in the best $R^2$ score, let's implement the `drop_unimportant_features` that drop those unimportant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unimportant_features(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df (DataFrame): DataFrame from which the unimportant features should be dropped.\n",
    "    \"\"\"\n",
    "    unimportant_features = ['floors', 'similarity_scores', 'missingindicator_sqft_living15', 'missingindicator_sqft_lot15', 'quarter', 'weekday']\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unimportant features\n",
    "X_train = pd.read_parquet(housing_train_x_path)\n",
    "X_test = pd.read_parquet(housing_test_x_path)\n",
    "drop_unimportant_features(X_train)\n",
    "drop_unimportant_features(X_test)\n",
    "\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdfb8d",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "Index(['yr_built', 'bedrooms', 'postcode', 'area', 'bathrooms', 'condition',\n",
    "       'grade', 'sqft_living', 'sqft_lot', 'sqft_basement', 'sqft_living15',\n",
    "       'sqft_lot15', 'waterfront', 'view', 'distance', 'year'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2381c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only important features\n",
    "store_features(X_train, feature_store_path / \"housing_train_X_important.parquet\")\n",
    "store_features(X_test, feature_store_path / \"housing_test_X_important.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494234d9",
   "metadata": {},
   "source": [
    "To conclude Assignments 4 and 5, let's again wrap everything inside one function that will be responsible for feature engineering. The function takes the data that has been preprocessed and validated, extracts all of the features, and stores them into the feature store. It's worth noting that this function will not only be applied to training data but also to test data and production data. As a result, the following three scenarios need to be considered:\n",
    "|dataset type|target included|fit a new target encoder|load a fitted target encoder|\n",
    "|------------|---------------|------------------------|----------------------------|\n",
    "|training|x|x||\n",
    "|test|x||x|\n",
    "|production|||x|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(df: pd.DataFrame, \n",
    "                                 feature_store_path: Path,\n",
    "                                 feature_file_name: str, \n",
    "                                 encoder_file_name: str, \n",
    "                                 target_file_name: Optional[str]=None, \n",
    "                                 fit_encoder: bool=False, \n",
    "                                 targets_included: bool=True) -> None:\n",
    "    \"\"\"\n",
    "    Converts a given (merged) housing data DataFrame into features and targets, performs feature engineering, and \n",
    "    stores the features along with possible targets and a fitted encoder\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame holding all of the housing data, or just the features (see targets_included)\n",
    "        feature_store_path (Path): Path of the feature store\n",
    "        feature_file_name (str): Filename for the stored features.\n",
    "        encoder_file_name (str): Filename for the stored encoder.\n",
    "        target_file_name (str|None): Filename for the stored targets.\n",
    "        fit_encoder (bool): Whether a new target encoder should be fitted. If False, uses a previously stored encoder\n",
    "        targets_included (bool):  If True, df has all of the housing data including targets. If False, df has only the features.\n",
    "    \"\"\"\n",
    "    if targets_included:\n",
    "        X, y = separate_X_and_y(df, target='price')   \n",
    "    else:\n",
    "        if fit_encoder:\n",
    "            raise ValueError(\"Target encoder can not be trained without targets.\")\n",
    "        X = df.copy()\n",
    "\n",
    "    impute_missing(X)\n",
    "    datetime_decomposer(X, dt_column_name='date')\n",
    "    condition_encoder(X)\n",
    "    drop_unimportant_features(X)\n",
    "\n",
    "    feature_file_path = feature_store_path / feature_file_name\n",
    "    target_file_path = feature_store_path / target_file_name\n",
    "    encoder_file_path = feature_store_path / \"encoders\" / encoder_file_name\n",
    "\n",
    "    if fit_encoder:\n",
    "        t_encoder = target_encode(X, columns=['postcode', 'area'], target=y)\n",
    "        store_features(X, feature_file_path)\n",
    "        store_targets(y, target_file_path)\n",
    "        store_encoder(t_encoder, encoder_file_path)\n",
    "        \n",
    "    else:\n",
    "        with open(encoder_file_path, 'rb') as enc_f:\n",
    "            t_encoder = pickle.load(enc_f)\n",
    "        target_encode(X, columns=['postcode', 'area'], encoder=t_encoder)\n",
    "        store_features(X, feature_file_path)    \n",
    "        if targets_included:\n",
    "            store_targets(y, target_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8469fa",
   "metadata": {},
   "source": [
    "## Assignment 6: One script to rule them all (2 points)\n",
    "\n",
    "The jupyter notebook we have played with this far is a good tool for exploratory and iterative development, but contains all sort of redundant code, which is no longer needed once we want to push the finalized version into production. As a final wrap-up exercise, we'll collect all of the functionalities created in this assignment to a single streamlined `etl.py` module. The file `etl.py` already exists as a template with all the necessary imports and global variables. \n",
    "\n",
    "### 6a) Let's begin with copying the functions...\n",
    "\n",
    "Copy the following functions to `etl.py`: file_reader, dataframe_merger, drop_futile_columns, correct_distance_unit, string_transformer, typo_fixer, data_extraction, batch_creator, create_checkpoint, separate_X_and_y, impute_missing, datetime_decomposer, condition_encoder, target_encode, store_features, store_targets, store_encoder, drop_unimportant_features, feature_engineering_pipeline. \n",
    "\n",
    "There are many ways you can achieve this. Here are three:\n",
    "1. Just copy-paste each function (carefully).\n",
    "1. Use jupyter cell magic. You can add the line `%%writefile -a etl.py` at the beginning of each cell that contains a function. Run each of these cells exactly once. This will append the cell content inside the `etl.py` file. The cells should not contain code outside the function definitions. Once you've done, remove the cell magic commands.\n",
    "1. Use `jupyter nbconvert` function. For example, running the following command will copy all your Python code (while ignoring Markdown cells) to a Python script that has the same name as the notebook. You can then clean the exported Python script and copy needed code such as the code skeleton of the `etl` function from `etl.py`. (Remember to rename your script to `etl.py` once you're done, otherwise you'll encounter issues when running some of the following code cells.)\n",
    "```bash\n",
    "jupyter nbconvert --to python --PythonExporter.exclude_markdown=True --no-prompt <jupyter-botebook-name>.ipynb \n",
    "```\n",
    "### 6b) ...and create a wrapper\n",
    "\n",
    "Create a wrapper function `etl` that loads, merges, cleans, and validates the specified data, extract features, and save the features (and possibly targets) in the feature store. Specifically, the function needs to perform as follows:\n",
    "- It first runs the `data_extraction` function, which will result in a merged, cleaned DataFrame. you can use the hard-coded `correct_condition_values=['poor', 'tolerable', 'satisfactory', 'good', 'excellent']` in the `data_extraction` function.\n",
    "- Then it uses Great Expectations Checkpoint to validate the DataFrame produced by the `data_extraction` function (The `batch_creator` and `create_checkpoint` functions are useful here). You can assume that the Great Expectations Context and the Expectation Suite are already configured. The GX Context is available via `context = gx.get_context(context_root_dir=gx_context_root_dir)` where `gx_context_root_dir` is the directory that contains all your Great Expectations config, which is the `gx` directory in our case. \n",
    "- If some validations fails, a warning should be printed. The warning can be as simple as a sentence like \"Some GX validations failed\". Hint: Please refer to [here](https://docs.greatexpectations.io/docs/reference/api/checkpoint/types/checkpoint_result/checkpointresult_class/) on how to check whether a validation succeeds or not. \n",
    "- Finally, no matter whether or not the DataFrame passes all Expectations, the `etl` function should feature-engineer the DataFrame using the `feature_engineering_pipeline` function. \n",
    "\n",
    "You can see a detailed description of the parameters of the `etl` function in [etl.py](./etl.py). \n",
    "\n",
    "Remember to include the completed `etl.py` in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27917cd9",
   "metadata": {},
   "source": [
    "Now, let's import the `etl` function and use it to process data from 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b233388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import etl\n",
    "\n",
    "params = {\n",
    "        \"path\": WORKING_DIR / \"data\" / \"2016\",\n",
    "        \"gx_context_root_dir\": WORKING_DIR / \"gx\",\n",
    "        \"gx_datasource_name\": \"housing_datasource\", \n",
    "        \"gx_checkpoint_name\": \"checkpoint_2016\",\n",
    "        \"gx_expectation_suite_name\": \"housing_expectation_suite\",\n",
    "        \"gx_run_name\": \"run_2016\",\n",
    "        \"feature_store_path\": WORKING_DIR / \"feature_store\",\n",
    "        \"feature_file_name\": \"2016_test_X.parquet\",\n",
    "        \"encoder_file_name\": \"housing_target_encoder.pkl\", \n",
    "        \"target_file_name\": \"2016_test_y.csv\", \n",
    "        \"fit_encoder\": False,\n",
    "        \"targets_included\": True\n",
    "    }\n",
    "etl(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85873e88",
   "metadata": {},
   "source": [
    "You should see a warning telling that some Expectations failed. \n",
    "\n",
    "**Please attach the failed Expectations to your PDF file**, e.g., \n",
    "\n",
    "<img src=\"./images/failed-expectation-example.png\" />\n",
    "\n",
    "(Please note that the provided example doesn't show the failed validations for data from 2016. It just illustrates which parts of the Checkpoint result file should be included in the submitted PDF.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579ffd4",
   "metadata": {},
   "source": [
    "Let's train a model using only the important features of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb868880",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_important = pd.read_parquet(feature_store_path / \"housing_train_X_important.parquet\")\n",
    "train_y = pd.read_csv(feature_store_path / \"housing_train_y.csv\").values.ravel()\n",
    "housing_rfr2 = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)\n",
    "housing_rfr2.fit(train_X_important, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c4c95",
   "metadata": {},
   "source": [
    "Then, evaluate the `housing_rfr2` model using data from 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec527cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2016 = pd.read_parquet(WORKING_DIR/\"feature_store\"/\"2016_test_X.parquet\")\n",
    "y_test_2016 = pd.read_csv(WORKING_DIR/\"feature_store\"/\"2016_test_y.csv\")\n",
    "r2_score = housing_rfr2.score(X_test_2016, y_test_2016)\n",
    "print(f\"Model's R2 score using data from 2016 as test data: {r2_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1ec34",
   "metadata": {},
   "source": [
    "Model's R2 score using data from 2016 as test data should be 0.92359378. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
